# 		 集群环境搭建

（<u>本次集群服务器采用最小化安装，部分功能需要手动安装，使用中出现的问题会记录，然后安装</u>）所用软件安装使用root用户

## 1. VM软件安装

略......

## 2. Centos7.5 镜像安装

略......

## 3. 服务器使用中遇到的问题

### 3.1 net-tools

最小化安装centos后，会发现很多基础的命令都没有，开启虚拟机后如果使用ifconfig会报错，![image-20210705203511471](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210705203511471.png)

需要手动安装

执行命令： **yum install net-tools**

### 3.2 安装rsync

服务器同步一些数据,报以下错误，因为服务器端没有安装`rsync`

![image-20210706102211204](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706102211204.png)

在服务器端安装`rsync`即可：

### 3.3 安装lrzsz

rz、sz是用来在windows和Linux上互转文件的一个命令。

yum安装：

**yum -y install lrzsz**

用法：

```sh
[root@hadoop102 module]# rz  
```

```sh 
[root@hadoop102 applog]# sz 
```



#### 虚拟机修改主机名称



#### 修改hosts映射文件



#### centos7关闭防火墙

 查看防火墙服务的状态

```sh
systemctl status firewalld
```

停止防火墙服务

```sh
systemctl stop firewalld
```

启动防火墙服务

```sh
systemctl start firewalld
```

重启防火墙服务

```sh
systemctl restart firewalld
```

#### 设置后台服务的自启配置

**开启/关闭iptables(防火墙)服务的自动启动**

```sh
systemctl enable firewalld.service
systemctl disable firewalld.service
```

### 4 服务器间SSH免密配置

hadoop102,hadoop103,hadoop104三台服务器之间无密码登录

1. 在hadoop102上执行 

```sh
ssh-keygen -t rsa 
```

![image-20210706093738665](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706093738665.png)

三次回车，然后分别执行

```sh
ssh-copy-id hadoop102
ssh-copy-id hadoop103
ssh-copy-id hadoop104
```

2. 在hadoop103上执行

   ```sh
   ssh-keygen -t rsa 
   ```

   三次回车，然后分别执行

   ```sh
   ssh-copy-id hadoop102
   ssh-copy-id hadoop103
   ssh-copy-id hadoop104
   ```

3. 在hadoop104上执行

   ```sh
   ssh-keygen -t rsa 
   ```

   三次回车，然后分别执行

   ```sh
   ssh-copy-id hadoop102
   ssh-copy-id hadoop103
   ssh-copy-id hadoop104
   ```

4. 三台服务器测试免密登录

   在hadoop102上登陆其他两台服务器

   ![image-20210706095159603](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706095159603.png)

如上图所示，同样在其他服务器登陆测试。



## 







配置集群间内容分发脚本

hadoop102服务器 /root/bin目录下创建脚本xsync

```sh
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
  echo ====================  $host  ====================
  #3. 遍历所有目录，挨个发送
  for file in $@
  do
    #4 判断文件是否存在
    if [ -e $file ]
    then
      #5. 获取父目录
      pdir=$(cd -P $(dirname $file); pwd)
      #6. 获取当前文件的名称
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done

```

修改执行权限

```sh
chmod +x xsync


```

配置查看进程脚本

hadoop102服务器 /root/bin目录下创建脚本xcall

写入内容

```sh
#! /bin/bash
for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "$*"
done

```

修改执行权限

```sh
chmod +x xcall
```



## 4. 集群上软件安装

### 4.1 安装jdk

**最小化安装没有预装jdk,因此不用卸载预装的jdk**

1. 解压缩jdk

```sh
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
```

2. 配置环境变量

   新建my_env.sh

```sh
 vim /etc/profile.d/my_env.sh 
```

配置如下内容

```sh
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin

```

3 .  然后重新加载环境变量,使之生效。

```sh
source /etc/profile.d/my_env.sh
```

4. 查看是否成功

   ```sh
   java -version
   ```

   ![image-20210706104250129](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706104250129.png)

5. 将jdk和my_env.sh分发到其他两台服务器(hadoop103,hadoop104)

   ```sh
   xsync /opt/module/jdk1.8.0_212/
   xsync /etc/profile.d/my_env.sh
   ```

6. **分别在hadoop103、hadoop104上执行source**

```sh
source /etc/profile.d/my_env.sh
```

### 4.2 Hadoop安装

问题：启动集群，报错

```
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [hadoop104]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
```

![image-20210706143138734](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706143138734.png)

hadoop-env.sh中添加如下配置

```sh
export JAVA_HOME=/opt/module/jdk1.8.0_212
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

![image-20210706143920776](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706143920776.png)

再次启动，成功！

### 4.3 Zookeeper安装

​	hadoop102服务器

1. 解压,然后修改名称

   ```sh
   tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
   
   mv apache-zookeeper-3.5.7-bin zookeeper-3.5.7
   
   ```

2. 同步/opt/module/zookeeper-3.5.7目录内容到hadoop103、hadoop104

   ```sh
   xsync zookeeper-3.5.7/
   ```

3. **配置服务器编号**

   1. 在/opt/module/zookeeper-3.5.7/这个目录下创建zkData

      ```sh
      mkdir zkData
      ```

   2. 在/opt/module/zookeeper-3.5.7/zkData目录下创建一个myid的文件

      ```sh
      [root@hadoop102 zkData]# vim myid
      
      ```

      在文件中添加与server对应的编号：

      ```sh
      2
      ```

   3. 拷贝配置好的zookeeper到其他机器上

      ```sh
      [root@hadoop102 zkData]# xsync myid 
      
      ```

      并分别在hadoop103、hadoop104上修改myid文件中内容为3、4

   4. **配置zoo.cfg文件**

      1. 重命名/opt/module/zookeeper-3.5.7/conf这个目录下的zoo_sample.cfg为zoo.cfg

         ```sh
         [root@hadoop102 conf]# mv zoo_sample.cfg zoo.cfg
         ```

      2. 修改数据存储路径配置

         ```sh
         dataDir=/opt/module/zookeeper-3.5.7/zkData
         #######################cluster##########################
         server.2=hadoop102:2888:3888
         server.3=hadoop103:2888:3888
         server.4=hadoop104:2888:3888
         ```

         ![image-20210706151609225](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210706151609225.png)

      3. 同步zoo.cfg配置文件

         ```sh
         [root@hadoop102 conf]# xsync zoo.cfg 
         ```

   5. 启动zookeeper

      三台服务器上分别执行启动脚本

      ```sh
      bin/zkServer.sh start
      ```

   6. 群起脚本

      1. 在hadoop102的/home/atguigu/bin目录下创建脚本zk.sh

      ```sh
      #!/bin/bash
      
      case $1 in
      "start"){
      	for i in hadoop102 hadoop103 hadoop104
      	do
              echo ---------- zookeeper $i 启动 ------------
      		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
      	done
      };;
      "stop"){
      	for i in hadoop102 hadoop103 hadoop104
      	do
              echo ---------- zookeeper $i 停止 ------------    
      		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
      	done
      };;
      "status"){
      	for i in hadoop102 hadoop103 hadoop104
      	do
              echo ---------- zookeeper $i 状态 ------------    
      		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
      	done
      };;
      esac
      
      ```

      2. 增加脚本执行权限

         ```sh
         [root@hadoop102 bin]# chmod +x zk.sh 
         ```

      3. 测试Zookeeper集群启停脚本

         ```sh
         zk.sh start
         zk.sh stop
         zk.sh status
         ```

         

### 4.4 Kafka安装

1. **解压**,修改名称

   ```sh
   [root@hadoop102 software]# tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/
   
   [root@hadoop102 module]# mv kafka_2.11-2.4.1/ kafka
   
   ```

2. **在/opt/module/kafka目录下创建logs文件夹**

   ```shell
   [root@hadoop102 kafka]# mkdir logs
   
   ```

3. 修改配置文件

   ```
   [root@hadoop102 kafka]# cd config/
   [root@hadoop102 config]# vi server.properties
   修改或者增加以下内容：
   #broker的全局唯一编号，不能重复
   broker.id=0
   #删除topic功能使能
   delete.topic.enable=true
   #kafka运行日志存放的路径
   log.dirs=/opt/module/kafka/data
   #配置连接Zookeeper集群地址
   zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka
   
   ```

4. **配置环境变量**

   ```sh
   [root@hadoop102 config]# vim /etc/profile.d/my_env.sh
   
   添加如下内容
   #KAFKA_HOME
   export KAFKA_HOME=/opt/module/kafka
   export PATH=$PATH:$KAFKA_HOME/bin
   
   使环境变量生效
   [root@hadoop102 config]# source /etc/profile.d/my_env.sh 
   ```

5. **分发安装包**

   ```sh
   [root@hadoop102 module]# xsync kafka/
   
   ```

6. **分别在hadoop103和hadoop104上修改配置文件**

   **/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2**

7. 集群启停脚本

   在/root/bin目录下创建脚本kf.sh

   ```sh
   #! /bin/bash
   
   case $1 in
   "start"){
       for i in hadoop102 hadoop103 hadoop104
       do
           echo " --------启动 $i Kafka-------"
           ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
       done
   };;
   "stop"){
       for i in hadoop102 hadoop103 hadoop104
       do
           echo " --------停止 $i Kafka-------"
           ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
       done
   };;
   esac
   
   ```

   修改脚本执行权限

   ```sh
   [root@hadoop102 bin]# chmod +x kf.sh
   ```

8. 测试脚本启停

   ```sh
   [root@hadoop102 bin]# kf.sh start
   [root@hadoop102 bin]# kf.sh stop
   
   ```

9. 基本操作命令

   **查看当前服务器中的所有topic**

   ```shell
   [atguigu@hadoop102 kafka]$ kafka-topics.sh --list --bootstrap-server hadoop102:9092
   ```

   **创建topic**

   ```shell
   [atguigu@hadoop102 kafka]$ kafka-topics.sh --create --bootstrap-server hadoop102:9092 --topic first --partitions 2 --replication-factor 2
   ```

   **删除topic**

   ```shell
   [atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first
   ```

   **发送消息**

   ```shell
   [atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
   ```

   **消费消息**

   ```shell
   [atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh \
   --bootstrap-server hadoop102:9092 --from-beginning --topic first
   
   [atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh \
   --bootstrap-server hadoop102:9092 --from-beginning --topic first
   --from-beginning：会把主题中以往所有的数据都读取出来。
   ```

   **查看某个topic的详情**

   ```shell
   [atguigu@hadoop102 kafka]$ kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first
   ```

   **修改分区数**

   ```shell
   [atguigu@hadoop102 kafka]$bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 6
   ```

   

### 4.5 Flume安装

1. 解压,修改文件夹名

   ```sh
   [root@hadoop102 software]# tar -zxvf apache-flume-1.9.0-bin.tar.gz -C /opt/module/
   
   [root@hadoop102 module]# mv apache-flume-1.9.0-bin/ flume
   
   ```

2. 将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop 3.1.3

   ```sh
   [root@hadoop102 flume]# rm lib/guava-11.0.2.jar 
   
   ```

   注意：删除guava-11.0.2.jar的服务器节点，一定要配置hadoop环境变量。否则会报如下异常。

   ```java
   Caused by: java.lang.ClassNotFoundException: com.google.common.collect.Lists
           at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
           at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
           at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
           at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
           ... 1 more
   
   ```

   

3. 将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件

   ```sh
   [root@hadoop102 conf]# mv flume-env.sh.template flume-env.sh
   
   [root@hadoop102 conf]# vim flume-env.sh 
   添加：
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   ```

### 4.6 Mysql安装

1. 卸载自带的Mysql-libs（如果之前安装过mysql，要全都卸载掉）

   ```sh
   [root@hadoop102 software]$ rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps
   ```

2. **安装mysql依赖**

   ```sh
   [root@hadoop102 software]$ sudo rpm -ivh 01_mysql-community-common-5.7.16-1.el7.x86_64.rpm
   [root@hadoop102 software]$ sudo rpm -ivh 02_mysql-community-libs-5.7.16-1.el7.x86_64.rpm
   [root@hadoop102 software]$ sudo rpm -ivh 03_mysql-community-libs-compat-5.7.16-1.el7.x86_64.rpm
   
   ```

3. **安装mysql-client**

   ```sh
   [root@hadoop102 software]$ sudo rpm -ivh 04_mysql-community-client-5.7.16-1.el7.x86_64.rpm
   ```

4. **安装mysql-server**

   ```sh
   [root@hadoop102 software]$ sudo rpm -ivh 05_mysql-community-server-5.7.16-1.el7.x86_64.rpm
   ```

   这一步报如下错误：

   ```java
   警告：05_mysql-community-server-5.7.16-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY
   错误：依赖检测失败：
   	libaio.so.1()(64bit) 被 mysql-community-server-5.7.16-1.el7.x86_64 需要
   	libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.16-1.el7.x86_64 需要
   	libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.16-1.el7.x86_64 需要
   
   ```

   ![image-20210707095133623](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210707095133623.png)

   缺少libiao，安装即可。

   ```sh
   [root@hadoop102 mysql]# yum install libaio
   
   ```

5. 启动mysql

   ```sh
   [root@hadoop102 mysql]# systemctl start mysqld
   
   ```

6. 查看mysql密码

   ```sh
   [root@hadoop102 mysql]# cat /var/log/mysqld.log | grep password
   
   ```

7. 配置Mysql

   1. **用刚刚查到的密码进入mysql**（如果报错，给密码加单引号）

      ```sh
      [rooot@hadoop102 software]$ mysql -uroot -p 'password'
      ```

   2. **设置复杂密码(****由于mysql****密码策略，此密码必须足够复杂)**

      ```sh
      mysql> set password=password("Qs23=zs32");
      ```

   3. **更改mysql密码策略**

      ```sh
      set global validate_password_length=4;
      set global validate_password_policy=0;
      ```

   4. **设置简单好记的密码**

      ```sh
      set password=password("123456");
      ```

   5. **修改user表，把Host表内容修改为%**

      ```sh
      mysql> use mysql;
      mysql> update user set host="%" where user="root";
      ```

   6. 刷新

      ```sh
      mysql> flush privileges;
      
      mysql> quit;
      ```

### 4.7 sqoop 安装

1. 解压,

   ```sh
   [root@hadoop102 software]# tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/
   
   [root@hadoop102 module]# mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop
   
   ```

2. 修改配置文件

   ```sh
   [root@hadoop102 module]# cd sqoop/conf
   
   [root@hadoop102 conf]# mv sqoop-env-template.sh sqoop-env.sh
   
   # 编辑sqoop-env.sh 增加以下内容：
   export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
   export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
   export HIVE_HOME=/opt/module/hive
   export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7
   export ZOOCFGDIR=/opt/module/zookeeper-3.5.7/conf
   ```

3. 拷贝JDBC驱动

   ```sh
   [root@hadoop102 conf]# cp /opt/software/mysql/mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/
   ```

4. ### 验证Sqoop

   ```sh
   [root@hadoop102 sqoop]# bin/sqoop help
   
   ```

   出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：

   ```
   Available commands:
     codegen            Generate code to interact with database records
     create-hive-table  Import a table definition into Hive
     eval               Evaluate a SQL statement and display the results
     export             Export an HDFS directory to a database table
     help               List available commands
     import             Import a table from a database to HDFS
     import-all-tables  Import tables from a database to HDFS
     import-mainframe   Import datasets from a mainframe server to HDFS
     job                Work with saved jobs
     list-databases     List available databases on a server
     list-tables        List available tables in a database
     merge              Merge results of incremental imports
     metastore          Run a standalone Sqoop metastore
     version            Display version information
   
   ```

5. 测试Sqoop是否能够成功连接数据库

   ```sh
   [root@hadoop102 sqoop]# bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 123456
   ```

   出现如下输出：

   ```
   information_schema
   gmall
   mysql
   performance_schema
   sys
   
   ```

### 4.8 Hive安装

1. 解压

   ```bash
   [root@hadoop102 software]# tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/
   
   ```

2. 修改名称

   ```bash
   [root@hadoop102 module]# mv apache-hive-3.1.2-bin/ hive
   ```

3. **修改/etc/profile.d/my_env.sh**，**添加环境变量**

   ```sh
   [root@hadoop102 module]# vim /etc/profile.d/my_env.sh 
   #HIVE_HOME
   export HIVE_HOME=/opt/module/hive
   export PATH=$PATH:$HIVE_HOME/bin
   
   ```

4. source一下 ，使环境变量生效

   ```sh
   [root@hadoop102 module]# source /etc/profile.d/my_env.sh 
   
   ```

5. **解决日志Jar包冲突，进入/opt/module/hive/lib目录**

   ```sh
   [root@hadoop102 lib]# mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
   
   ```

6.  Hive元数据配置到MySQL

   1. 拷贝驱动

      将MySQL的JDBC驱动拷贝到Hive的lib目录下

   ```sh
   [root@hadoop102 lib]# cp /opt/software/mysql/mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/
   
   ```

   2. 配置Metastore到MySQL

      在$HIVE_HOME/conf目录下新建hive-site.xml文件,添加如下内容

      ```xml
      <?xml version="1.0"?>
      <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
      <configuration>
          <property>
              <name>javax.jdo.option.ConnectionURL</name>
              <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
          </property>
      
          <property>
              <name>javax.jdo.option.ConnectionDriverName</name>
              <value>com.mysql.jdbc.Driver</value>
          </property>
      
          <property>
              <name>javax.jdo.option.ConnectionUserName</name>
              <value>root</value>
          </property>
      
          <property>
              <name>javax.jdo.option.ConnectionPassword</name>
              <value>123456</value>
          </property>
      
          <property>
              <name>hive.metastore.warehouse.dir</name>
              <value>/user/hive/warehouse</value>
          </property>
      
          <property>
              <name>hive.metastore.schema.verification</name>
              <value>false</value>
          </property>
      
          <property>
          <name>hive.server2.thrift.port</name>
          <value>10000</value>
          </property>
      
          <property>
              <name>hive.server2.thrift.bind.host</name>
              <value>hadoop102</value>
          </property>
      
          <property>
              <name>hive.metastore.event.db.notification.api.auth</name>
              <value>false</value>
          </property>
          
          <property>
              <name>hive.cli.print.header</name>
              <value>true</value>
          </property>
      
          <property>
              <name>hive.cli.print.current.db</name>
              <value>true</value>
          </property>
      </configuration>
      
      ```

7. 启动Hive

   1. 初始化元数据库

      ```sh
      登陆MySQL
      [root@hadoop102 conf]# mysql -uroot -p123456
      
      新建Hive元数据库
      mysql> create database metastore;
      mysql> quit;
      
      初始化Hive元数据库
      [root@hadoop102 conf]# schematool -initSchema -dbType mysql -verbose
      
      ```

   2. 启动hive客户端

      ```sh
      [root@hadoop102 hive]# bin/hive
      
      hive (default)> show databases;
      OK
      database_name
      default
      Time taken: 1.686 seconds, Fetched: 1 row(s)
      
      ```

### 4.9 Hive On Spark 

1. 解压,修改名称

   ```sh
   [root@hadoop102 software]# tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/
   
   [root@hadoop102 module]# mv spark-3.0.0-bin-hadoop3.2/ spark
   
   ```

2. 配置SPARK_HOME环境变量

   ```sh
   [root@hadoop102 module]# vim /etc/profile.d/my_env.sh
   
   # source 使其生效
   [root@hadoop102 module]# source /etc/profile.d/my_env.sh
   ```

3. 在hive中创建spark配置文件

   ```sh
   [root@hadoop102 hive]# vim conf/spark-defaults.conf
   # 添加如下内容（在执行任务时，会根据如下参数执行）
   spark.master                               yarn
   spark.eventLog.enabled                   true
   spark.eventLog.dir                        hdfs://hadoop102:8020/spark-history
   spark.executor.memory                    1g
   spark.driver.memory					   1g
   ```

   在HDFS创建如下路径，用于存储历史日志

   ```sh
   [root@hadoop102 hive]# hadoop fs -mkdir /spark-history
   
   ```

4. 向HDFS上传Spark纯净版jar包

   说明1：由于Spark3.0.0非纯净版默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。

   说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。

   1. 解压

      ```sh
      [root@hadoop102 software]# tar -zxvf spark-3.0.0-bin-without-hadoop.tgz 
      ```

   2. 上传Spark纯净版jar包到HDFS

      ```sh
      [root@hadoop102 software]# hadoop fs -mkdir /spark-jars
      
      [root@hadoop102 software]# hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars
      
      ```

   3. 修改hive-site.xml文件

      ```sh
      [root@hadoop102 software]# vim /opt/module/hive/conf/hive-site.xml
      ```

      添加如下内容

      ```xml
      <!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）-->
      <property>
          <name>spark.yarn.jars</name>
          <value>hdfs://hadoop102:8020/spark-jars/*</value>
      </property>
        
      <!--Hive执行引擎-->
      <property>
          <name>hive.execution.engine</name>
          <value>spark</value>
      </property>
      
      <!--Hive和Spark连接超时时间-->
      <property>
          <name>hive.spark.client.connect.timeout</name>
          <value>10000ms</value>
      </property>
      ```

      注意：hive.spark.client.connect.timeout的默认值是1000ms，如果执行hive的insert语句时，抛如下异常，可以调大该参数到10000ms

      ```java
      FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create Spark client for Spark session d9e0224c-3d14-4bf4-95bc-ee3ec56df48e
      ```

5. 测试

   1. 启动Hive客户端

      ```sh
      [root@hadoop102 hive]# bin/hive
      ```

   2. 创建一张测试表

      ```sql
      hive (default)>  create table student(id int, name string);
      ```

   3. 通过insert测试效果

      ```sql
      hive (default)> insert into table student values(1,'abc');
      
      ```

      测试过程中报错，

      ```java
      In order to change the average load for a reducer (in bytes):
        set hive.exec.reducers.bytes.per.reducer=<number>
      In order to limit the maximum number of reducers:
        set hive.exec.reducers.max=<number>
      In order to set a constant number of reducers:
        set mapreduce.job.reduces=<number>
      Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create Spark client for Spark session 9cbae7f9-365f-4782-acc4-ca2bebe3ff08)'
      FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session 9cbae7f9-365f-4782-acc4-ca2bebe3ff08
      
      ```

      解决办法：

      **可在spark 的spark-env.sh 里 添加：**

      ```java
      export SPARK_DIST_CLASSPATH=$(hadoop classpath)
      ```

      ![image-20210707173112870](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210707173112870.png)

      再次启动hive,重新测试插入语句：

      ![image-20210707173533440](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210707173533440.png)

      说明配置成功！

   







