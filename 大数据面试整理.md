# 			大数据相关整理

### 基础知识

#### 一、Linux

##### 常用操作



#### 二、 Java

#####  基础

###### 数据类型

- 基本数据类型
  - byte，short,int,long    整型
  - char    字符型
  - float,double    浮点型
  - boolean    布尔型
- 引用数据类型
  - String
  - Stringbulider    线程不安全，建议字符串相加操作比较多的情况下使用。
  - StringBuffer    线程安全的,多线程情况下使用

##### 集合

![image-20211015112519122](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211015112519122.png)

 **Collection接口常用方法**

- add:添加单个元素
- remove:删除指定元素
- contains:查找元素是否存在
- size:获取元素个数
- isEmpty:判断是否为空
- clear:清空
- addAll:添加多个元素
- containsAll:查找多个元素是否都存在
- removeAll:删除多个元素



###### ArrayList

- ArrayList中维护了一个Object类型的数组elementData

  transient Object[] elementData;

- 当创建ArrayList对象时，如果使用的是无参构造器，则初始elementData容量为0，第1次添加，则扩容elementData为10，如需要再次扩容，则扩容elementData为1.5倍。

- 如果使用的是指定大小的构造器，则初始elementData容量为指定大小，如果需要扩容,则直接扩容elementData为1.5倍。

###### LinkedList

- LinkedList底层维护了一个双向链表
-  可以添加任意元素(元素可以重复)，包括null
- LinkedList中维护了两个属性first和last分别指向首节点和尾节点
- 每个节点(Node对象)，里面又维护了prev、next、item三个属性，其中通过prev指向前一个，通过next指向后一个节点。最终实现双向链表.
- LinkedList的元素的添加和删除，不是通过数组完成的，相对来说效率较高。
- 线程不安全,没有实现同步

###### HashSet

- 不能包含重复的对象，并且最多只允许包含一个 null 元素

- 无序，即对象不按特定的方式排序，只是简单地把对象加入集合

- HashSet底层是HashMap

- 第一次添加元素时，数组扩容到16，临界值(threshold)是 16 * 加载因子(loadFactor) 0.75 = 12，如果table数组使用到了临界值12，就会扩容到 16*2 = 32，新的临界值就是 32 * 0.75 = 24 ，依次类推。

- 添加—一个元素时，先得到hash值-会转成->索引值

- 找到存储数据表table，看这个索引位置是否已经存放的有元素

- 如果没有，直接加入

- 如果有，调用equals比较，如果相同，就放弃添加。如果不相同，则添加到最后

- 在Java8中,如果一条链表的元素个数到达TREEIFY_THRESHOLD(默认是8)，并且table的大小>=MIN_TREEIFY_CAPACITY(默认64).就会进行树化(红黑树)

  

![image-20211016094423414](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211016094423414.png)

**HashSet底层源码解读**

```java
// HashSet源码解读
/*
1. 执行 HashSet()
    public HashSet() {
        map = new HashMap<>();
    }
    
 //--------------------------------------------//   
2. 执行add()
    public boolean add(E e) {
        return map.put(e, PRESENT)==null;
    }
 //--------------------------------------------//   
3. 执行put()
    public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);
    }
 //--------------------------------------------//
    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    } 
 //--------------------------------------------//
4. 执行putVal()
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i; //定义的辅助变量
        // table 就是HashMap的数组，类型是Node[]
        // if 语句表示如果当前table是null或者大小=0就是第一次扩容，到16个空间
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;//resize()方法：扩容
        //根据key，得到hash 去计算该key应该存放到table表的哪个索引位置，并把这个位置的对象，赋给p
        //判断p，是否为null
        //如果p为null，表示没有存放元素，就创建一个Node
        //就放在该位置 tab[i] = newNode(hash, key, value, null);
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
        	// Tips:在需要局部变量(辅助变量)的时候，再创建
            Node<K,V> e; K k;//定义辅助变量
            //如果当前索引位置对应的链表的第一个元素和准备添加元素的key的hash值相同，并且满足下面两个条件之一：
            // 1. 加入的key和p指向的Node节点的key是同一个对象
            // 2. p指向的Node节点的key的equals()和准备加入的key比较后相同
            // 就不能加入
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            // 再判断p 是不是一颗红黑树，
            //如果是一颗红黑树，就调用putTreeVal()
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            // 如果table对应索引位置，已经是一个链表，就是用for循环比较
            // 1.依次和该链表的每一个元素比较后，都不相同，则加入到该链表的最后
            // 在把元素添加到链表后，立即判断，该链表是否已经达到8个节点，就调用treeifyBin()，（该方法会检查该table数组的大小是否 >= 64，若小于则会对该数组扩容，不会树化，大于64的话则会树化）, 对当前这个链表进行树化
            // 2. 依次和该链表的每一个元素比较过程中，如果有相同情况，就直接break
            else {
                for (int binCount = 0; ; ++binCount) {
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        if (++size > threshold)
            resize();//扩容
        afterNodeInsertion(evict);
        return null;
    }
    
*/
```

###### LinkedHashSet

- 是HashSet的子类
- 底层是一个LinkedHashMap，底层维护了一个数组+双向链表
- LinkedHashSet根据元素的hashcode值来决定元素的存储位置，同时使用链表维护元素的次序，这使得元素看起来是以插入顺序保存的
- LinkedHashSet不允许添加重复元素
- 添加第一次时，直接将数组table扩容到16，存放的结点类型是 LinkedHashMap$Entry，数组是 HashNap$Node[]，存放的元素/数据是LinkedHashMap$Entry类型

###### TreeSet

```java
// TreeSet底层源码解读

/*
1.构造器把传入的比较器对象，赋给了TreeSet的底层的 TreeMap的属性this . comparator

pubLic TreeMap( Comparator<? super K> comparator) {
	this. comparator = comparator ;
}

2.在调用treeSet.add(), 在底层会执行到
if (cpr != null) {
	do {
		parent = t;
		//动态绑定到我们的匿名内部类(对象)compare()
		cmp = cpr . compare(key, t ,key);
		if (cmp < 0)
		t = t.Left;
		else if (cmp > 0)
			t = t.right;
		else
		return t。setValue(value); 
	} while (t != null);
*/
```



###### **HashMap**

- HashMap是 Map接口使用频率最高的实现类。
- HashMap是以 key-val对的方式来存储数据
- key 不能重复，但是是值可以重复,允许使用null键和null值。
- 如果添加相同的key，则会覆盖原来的key-val ,等同于修改(key不会替换，val会替换)
- 与HashSet一样，不保证映射的顺序，因为底层是以hash表的方式来存储的.
- HashMap没有实现同步，因此是线程不安全的

遍历方式

- 1 先取出所有的Key ,通过Key取出对应的Value

  ```java
  Set keyset = map.keySet();
  
  // 增强for循环获取
  for (object key : keyset) {
  system.out.println(key + "-" +map.get(key));
  }
  // 或者迭代器获取
  Iterator iterator = keyset.iterator();
  while (iterator.hasNext()) {
  0bject key = iterator.next();
  system.out.printin(key + "-" +map.get(key));
  }
  
  ```

- 2  把所有的value取出

  ```java
  Collection values = map.values();
  Iterator iterator = values.iterator();
  
  while (iterator.hasNext()) {
  0bject value = iterator.next();
      system.out.printin(value);
  }
  ```

- 3 通过EntrySet 获取

  ```java
  //通过EntrySet来获取k-v
  Set entrySet = map.entrySet();
  
  // 增强for循环获取
  for (object entry : entrySet) {
  Map.Entry m = (Map.Entry) entry;//将entry转成 Map. Entry
  system.out.println(m.getKey() +"-" + m.getValue());
  }
  
  // 或者迭代器获取
  Iterator iterator3 = entrySet.iterator();
  while (iterator3.hasNext()) {
  Object entry = iterator3.next();
  Map.Entry m = (Map.Entry) entry;//HashMap$Node//向下转型 Map.Entry
  system.out.println(m.getKey() +"-" +m.getValue());
  }
  
  ```

  

Map存放数据底层结构

![image-20211016144932546](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211016144932546.png)

**HashMap底层机制及源码剖析**

- HashMap底层维护了Node类型的数组table,默认为null
- 当创建对象时，将加载因子(loadfactor)初始化为0.75
- 当添加key-val时，通过key的哈希值得到在table的索引。然后判断该索引处是否有元素，
- 如果没有元素直接添加。如果该索引处有元素，继续判断该元素的key是否和准备加入的key相等，如果相等，则直接替换val;如果不相等需要判断是树结构还是链表结构，做出相应处理。如果添加时发现容量不够，则需要扩容。
- 第1次添加，则需要扩容table容量为16,临界值(threshold)为12.
- 以后再扩容，则需要扩容table容量为原来的2倍，临界值为原来的2倍,即24,依次类推
- 在Java8中，如果一条链表的元素个数超过TREEIFY THRESHOLD(默认是8),并且
  table的大小>= MIN TREEIFY CAPACITY(默认64),就会进行树化(红黑树)

```java
// HashMap源码解读

/*
 1. 执行构造器new HashMap()
    初始化加载因子Loadfactor = 0.75
    HashMap$Node[] table = null
    
 2. 执行put
public V put(K key, V value) {
	return putVaL(hash(key), key, value, false, true);
}
 调用hash()
 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    } 
    
 3.执行putVal()
 
 final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i; //辅助变量
        
        //如果底层的table数组为null 或者length = 0，就会扩容到16
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
        // 取出hash的对应的table的索引位置的Node，如果为null，就直接把加入的K-V 创建成一个Node，加入该位置即可。
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            // 如果table的索引位置的hash值和新的key的hash值相同，并且满足(table现有的节点的key和准备添加的key是同一个对象 || equals 相等)
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            // 如果当前table的已有的node是红黑树，就按照红黑树的方式处理
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            // 如果找到的节点，后面是链表，就循环比较
            else {
                for (int binCount = 0; ; ++binCount) {
                	// 如果整个链表，没有和它相同，就加到该链表的最后
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                        // 加入后判断当前链表的个数，是否已经到8个，若达到8个后，调用treeifyBin()进行红黑树的的转换
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    // 若比较的过程中，发现有相同，直接break，跳出循环
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount; // 每增加一个Node，就size++
        if (++size > threshold) // 判断是否扩容
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 4.关于树化(转成红黑树)
//如果table为null，或者 大小还没有到64, 暂时不树化，而是进行扩容。
final void treeifyBin (Node<K,V>[] tab, int hash) {
	int n, index; NodecK,V> e;
	if (tab == null 11 (n = tab,Length) < MIN_ TREEIFY CAPACITY)
	resize();

}

*/
```

TreeMap

```java
// TreeMap底层源码解读

/*
1.构造器。把传入的实现了Comparator接口的匿名内部类(对象)，传给给TreeMap的comparator 
pubLic TreeMap(Comparator<? super K> comparator) {
	this.comparator = comparator;
}

2. 调用put()方法,
第一次添加，把k-v封装到Entry对象，放入root
Entry<K,V> t = root;
if (t == null) {
	compare(key, key); // type (and possibly null) check
	root = new Entry<>(key, vaLue, nuLl);
	size=1;
	modCount++ ;
	return null ;
}

以后再次添加：
Comparator<? super K> cpr = comparator;
if(cpr !=null) {
	do { //遍历所有的key ,
		parent = t;
		cmp = cpr. comprre(key, t ,key);
		if (cmp < 0)
			t = t.Left;
		else if (cmp > 0)
			t = t.right;
		else
			return t. setValue (value);
	} while (t != nuLl);
*/
```



###### HashTable

- 存放的元素是键值对:即K-V
-  hashtable的键和值都不能为null,否则会抛出NullPointerException
-  hashTable使用方法基本上和HashMap- -样
- hashTable是线程安全的，hashMap 是线程不安全的

HashTable底层

```java

// 简单说明一下HashtabLe的底层
/*
1. 底层有数组Hashtable$Entry[] 初始化大小为11
2. 临界值threshold 8 = 11 * 0.75

4. 执行方法addEntry(hash, key, value, index) 添加到; 
5. 当if(count >= threshold)满足时，就进行扩容
6. 按照自己的扩容机制来进行.
	rehash(){
	int newCapacity = (oldCapacity << 1) + 1;
	}
*/
```

|           | 版本 | 线程安全(同步) | 效率 | 允许null键null值 |
| :-------: | ---- | :------------: | :--: | :--------------: |
|  HashMap  | 1.2  |     不安全     |  高  |       可以       |
| HashTable | 1.0  |      安全      | 较低 |      不可以      |





###### Properties

- Properties类继承自Hashtable类并且实现了Map接口，也是使用一种键值对的形式来保存数据。
- 他的使用特点和Hashtable类似
- Properties 还可以用于从 xxx.properties文件中，加载数据到Properties类对象,并进行读取和修改



###### ArrayList、LinkedList、Vetor

- ArrayList 

   内部是通过数组实现的，它允许对元素进行快速随机访问.

  优点：查询快，修改快

  缺点：增删慢

  - 数组的缺点是每个元素之间不能有间隔，当数组大小不能满足时需要增加存储能力，会在原始大小上扩容1.5倍，将已有数据复制到新的存储空间中。
  - 当从ArrayList的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。

  适合随机查找和遍历，不适合插入和删除。

- Vetor

  与ArrayList一样，也是通过数组实现的，但是它支持线程的同步，即某一时刻只有一个线程能够写Vector,避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问效率比ArrayList慢。

- LinkedList

  它是用链表结构存储数据的，优缺点和数组正好相反。

  优点：增删快

  每次增加或删除，不会影响其他大量元素，只会影响链表中相关联的前后关系。

  缺点：查询慢，修改慢

  

  每次查询元素，都需要根据链接关系逐个进行匹配
  
  适合做数据的动态插入和删除，随机访问和遍历速度比较慢。

<font color=red>问题拓展</font>

数组和链表的区别

- 数组

  必须事先定义固定的长度，不能适应数据动态地增减的情况。从栈中分配空间, 对于程序方便快速,但是自由度小。

  优点：利用下标定位，随机访问性强，查找速度快。

  缺点：插入和删除的效率低，内存利用率低，内存空间要求高，必须有足够的连续的内存空间。

- 链表

  链表动态地进行存储分配，可以适应数据动态地增减的情况。从堆中分配空间, 自由度大但是申请管理比较麻烦。

  优点：插入和删除的效率高。

  缺点：定位查询速度慢，修改慢。内存利用率高，不会浪费内存。

- 总结

  如果需要快速访问数据，很少或不插入和删除元素，就应该用数组;相反， 如
  果需要经常插入和删除元素就需要用链表数据结构了。

- 结合项目中使用

  - 对于需要快速插入，删除元素，应该使用 LinkedList

  - 对于需要快速随机访问元素，应该使用 ArrayList

  - 对于“单线程环境操作 List” 或者 “多线程环境，但 List 仅仅只会被单个线
    程操作”的情况，此时应该使用非同步的类(如 ArrayList)

    对于“多线程环境，且 List 可能同时被多个线程操作”，此时，应该使用同步的类
    Vector，或 Collections.synchronizedList(new ArrayList())

###### 总结-开发中如何选择集舍实现类(记住)

在开发中，选择什么集合实现类，主要取决于业务操作特点，然后根据集合实现类特性进行选择，分析如下:

1. 先判断存储的类型(一组对象[单列]或一组键值对[双列)

2. 一组对象[单列]:Collection接口
   允许重复: List
       增删多:LinkedList[底层维护了一个双向链表]
       改查多:ArrayList[底层维护Object类型的可变数组]
   不允许重复: Set
       无序:HashSet[底层是HashMap，维护了一个哈希表即(数组+链表+红黑树)J

​    排序: TreeSet
​    插入和取出顺序一致:LinkedHashSet，维护数组+双向链表

3. 一组键值对: Map
   键无序: HashMap [底层是:哈希表 jdk7:数组+链表，jdk8:数组+链表+红黑树]

   键排序: TreeMap
   键插入和取出顺序一致:LinkedHashMap

   读取文件Properties

##### 反射

java reflection

反射机制允许程序在执行期借助于reflection api 取得任何类的内部信息，比如成员变量、构造器、成员方法等。并能操作对象的属性及方法。反射在设计模式和框架底层都会用到。

加载完类之后，在堆中就产生了一个Class类型的对象（一个类只有一个Class对象），这个对象包含了类的完整结构信息。通过这个对象得到类的结构。这个对象就像一面镜子，透过这个镜子看到类的结构，所以，形象的称之为：反射。

![image-20211013152823813](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211013152823813.png)

**Java反射机制可以完成：**

1. 在运行时判断任意一个对象所属的类
2. 在运行时构造任意一个类的对象
3. 在运行时得到任意一个类所具有的成员变量和方法
4. 在运行时调用任意一个对象的成员变量和方法
5. 生成动态代理

反射相关的主要类：

- java.lang.Class
- java.lang.reflect.Method
- java.lang.reflect.Field
- java.lang.reflect.Constructor

反射机制的优缺点：

优点：可以动态的创建和使用对象(也是框架的地城核心)，使用灵活，没有反射机制，框架技术就失去底层支撑；

缺点：使用反射基本是解释执行，对执行速度有影响；

**反射初体验**

```java
public class Cat {

    public String name = "毛蛋";

    public Cat(String name) {
        this.name = name;
    }

    public Cat() {
    }

    public void hi(){
        System.out.println("毛蛋：" + " hi");
    }
}

public class ReflectionQuestion {
    public static void main(String[] args) throws Exception {

        Properties pro = new Properties();
        pro.load(new FileInputStream("src\\re.properties"));
        String classfullpath = pro.get("classfullpath").toString();
        String method = pro.get("method").toString();
        //System.out.println("类：" + classfullpath);
        //System.out.println("方法：" + method);

        Class<?> cls = Class.forName(classfullpath);
        Object o = cls.newInstance();
        System.out.println("获取实例的运行类型：" + o.getClass());

        Method method1 = cls.getMethod(method);
        method1.invoke(o);

        Field nameField = cls.getField("name");//获取公有的成员变量
        System.out.println(nameField.get(o));

        Constructor<?> constructor = cls.getConstructor();//获取无参构造器
        System.out.println("无参构造器" + constructor);

        Constructor<?> constructor1 = cls.getConstructor(String.class);//获取有参构造器
        System.out.println("有参构造器" + constructor1);

    }
}

```

反射调用优化——关闭访问检查

Method和Field、Constructor对象都有setAccessible()方法，该方法作用是启动和禁用访问安全检查的开关，当参数设置为true，表示放射的对象在使用时取消访问检查，提高反射的效率。参数为false，表示反射的对象执行访问检查

**Class类**

![image-20211013163247321](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211013163247321.png)

![image-20211013164757892](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211013164757892.png)

- Class也是类，因此也继承Object类
- Class类对象不是new出来的，而是系统创建的
- 对于某个类的Class类对象，在内存中只有一份，因为类只加载一次
- 每个类的实例都会记得自己有哪个Class实例所生成
- 通过Class可以完整地得到一个类的完整结构，通过一系列API
- Class对象是存放在堆的
- 类的字节码二进制数据，是放在方法区的，有的地方称之为类的元数据

**获取Class类对象方式**

1. Class.forName("***")

   已知一个类的全类名，且该类在类路径下，可通过Class类的静态方法forName()获取，可能抛出ClassNotFoundException，例：

   Class cls = Class.forName("com.zqh.Cat");

   多用于配置文件，读取类全路径，加载类

2. xx.class

   若已知具体的类，通过类的class获取，该方式最为安全可靠，程序性能最高；

   例：String.class   // xxx.class

   多用于参数传递；

3. 对象.getClass()

   已经有对象实例，

   例：car.getClass();

4. 通过类加载器获取到类的Class对象

   ```java
   ClassLoader classLoader = car.getClass().getClassLoader();
   Class cls = classLoader.loadClass(classAllPath);
   ```

5. 基本数据类型.class

   基本数据(int,char,boolean,float,double,byte,long,short)可以通过如下方式得到Class类对象

   ```java
   Class cls = 基本数据类型.class;
   ```

6. 包装类.TYPE

   基本数据类型对应的包装类，可通过.TYPE得到Class类对象

   ```java
   Class cls = 包装类.TYPE;
   ```

**类加载**

反射机制是java实现动态语言的关键，也就是通过反射实现类动态加载。

- 静态加载：编译时加载相关的类，如果没有则报错，依赖性太强。
- 动态加载：运行时加载需要的类，如果运行时不用该类，则不报错，降低了依赖性。

类加载时机

1. 当创建对象时（new）
2. 当子类被加载时
3. 调用类中的静态成员时
4. 通过反射

**类加载过程**

![image-20211014000242852](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211014000242852.png)

![image-20211014000640081](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211014000640081.png)

- 加载阶段

  JVM在该阶段的主要目的是将字节码从不同的数据源(可能是class文件、也可能是jar包，甚至网络)转化为二进制字节流加载到内存中，并生成一个代表该类的java.lang.Class对象。

- 连接阶段-验证

  1. 目的是为了确保Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。
  2. 包括:文件格式验证(是否以魔数oxcafebabe开头)、元数据验证、字节码验证和符号引用验证。
  3. 可以考虑使用-Xverify:none 参数来关闭大部分的类验证措施，缩短虚拟机类加载的时间。

- 链接阶段-准备

  JVM会在该阶段对静态变量，分配内存并初始化(对应数据类型的默认初始值，如0、OL、null、false 等)。这些变量所使用的内存都将在方法区中进行分配

  ```java
  class A {
  //属性-成员变量-字段
  //老韩分析类加载的链接阶段-准备属性是如何处理
  //1. n1是实例属性，不是静态变量，因此在准备阶段，是不会分配内存
  //2. n2是静态变量，分配内存n2是默认初始化0 ,而不是20
  //3. n3是static final 是常量，他和静态变量不一样，因为一旦赋值就不变n3 = 30
  pubLic int n1 = 10;
  pubLic static nt n2 = 20;
  pubLic static final int n3 = 30;
  }
  ```

- 链接阶段-解析

  虚拟机将常量池内的符号引用替换为直接引用的过程。

- 初始化阶段

  1. 到初始化阶段，才真正开始执行类中定义的Java程序代码，此阶段是执行< clinit> ()方法的过程。
  2.  <clinit> ()方法是由编译器按语句在源文件中出现的顺序，依次自动收集类中的所有静态变量的赋值动作和静态代码块中的语句，并进行合并。
  3. 虚拟机会保证一个类的 <clinit> ()方法在多线程环境中被正确地加锁、同步，如果
     多个线程同时去初始化一个类， 那么只会有一个线程去执行这个类的<clinit>() 方
     法，其他线程都需要阻塞等待，直到活动线程执行< clinit> ()方法完毕[debug源码]

**通过反射获取类的结构信息**

1. java.lang.Class类

   - getName:获取全类名
   2. getSimpleName:获取简单类名
   3. getFields:获取所有public修饰的属性， 包含本类以及父类的
   4. getDeclaredFields:获取本类中所有属性
   5. getMethods:获取所有public修饰的方法， 包含本类以及父类的
   6. getDeclaredMethods:获取本类中所有方法
   7. getConstructors:获取本类所有public修饰的构造器
   8. getDeclaredConstructors:获取本类中所有构造器
   - getPackage:以Package形式返回包信息
   - getSuperClass:以Class形式返回父类信息
   - getInterfaces:以Class[]形式返回接口信息
   - getAnnotations:以Annotation[]形式返回注解信息

2. java.lang.reflect.Field类

   - getModifiers:以int形式返回修饰符

     [说明:默认修饰符是0，public 是1 , private 是2，protected 是4 ,static是8，final 是16]

   - getType:以Class形式返回类型

   - getName:返回属性名

3. Java.lang.reflect.Method类

   - getModifiers:以int形式返回修饰符
     [说明:默认修饰符是0 , public 是1，private是2 , protected是4,
     static是8，final 是16]
   2. getReturnType:以Class形式获取返回类型
   3. getName:返回方法名
   4. getParameterTypes:以Class[]返回参数类型数组

4. java.lang.reflect.Constructor类

   - getModifiers:以int形式返回修饰符
   2. getName:返回构造器名(全类名)
   3. getParameterTypes:以Class[]返回参数类型数组

###### 案例操作

1. 通过反射访问类中的成员-属性

   1. 根据属性名获取Field对象    Field f = clazz对象.getDeclaredField(属性名)；
   2. 爆破：f.setAccessible(true);
   3. 访问：f.set(o,值)； sout(f.get(o)); //o表示对象
   4. 如果是静态属性，则set和get中的参数o，可以写成null。

   ```java
   Class<?> C = Class.forName("com.hspedu.reflection.User");
   //1.根据属性名获取属性对象
   //Field field = c.getField(" name")://只能访问某个特定的public修饰的属性
   Field field = c.getDeclaredField( name )://可以访问非public 
   //暴破字段
   field.setAccessible(true);
   //2.创建该类的对象，给他的某个属性赋值
   Object object = c.newlnstance();
   Field field2 = c.getDeclaredField("age");
   field2.set(object, 33);
   //3.为该对象的属性赋值(静态属性name)
   field.set(null, "john"); 
   //4.访问该对象的属性
   System.out.println(field.get(nul);
   System.out.println(object);
   
   ```

2. 通过反射访问类中的成员-方法

   1. 根据方法名和参数列表获取Method方法对象: Method m = clazz.getDeclaredMethod(方法名，XX.class); //得到本类的所有方法
   2. 获取对象: Object o=clazz.newlnstance();
   3. 暴破: m.setAccessible(true);
   4. 访问: Object returnValue = m.invoke(o,实参列表);
   5. 注意:如果是静态方法，则invoke的参数o,可以写成null!

   ```java
   Class<?> C = Class.forName(" com.hspedu.reflection.User");
   //1.根据方法名和参数列表获取方法Method对象private static String say(int a,String b,char c){
   //Method method = c.getMethod(name, parameterTypes);
   Method method = c.getDeclaredMethod(" say", int.class,String.class,char.class);
   Method hiMethod = C.getDeclaredMethod("hi", String.class);
   //2.创建对象
   Object object = c.newlnstance();
   //暴破
   method.setAccessible(true);
   //3.调用方法
   System.out.println(method.invoke(null, 1,"yellow",'A')); 
   hiMethod.invoke(object,”韩顺平教育");
   ```

   

##### Java多线程

###### 线程基础

**程序**(Program): 是为了完成特定任务、用某种语言编写的一组指令的集合。简单说就是我们写的代码。

**进程**(process)

是操作系统对于一个正在运行的程序的抽象，进程是操作系统资源分配的基本单位，我们编写的源程序在被编译之后只不过是位于磁盘的静态的二进制数据，他们需要被加载到内存，运行在进程的上下文中。

**线程**(thread)
	由进程创建是进程的一个实体，进程可进一步细化为线程，一个进程可以拥有多个线程，线程是   一个程序内部的一条执行路径。若一个进程同一时间并行执行多个线程，就是支持多线程的
线程作为调度和执行的单位，每个线程拥有独立的运行栈和程序计数器（pc），线程切换的开销小
一个进程中的多个线程共享相同的内存单元／内存地址空间-它们从同一堆中分配对象，可以访问相同的变量和对象。这就使得线程间通信更简便、高效；但多个线程燥作共享的系統资源可能就会带来安全的隐患。

- 用户线程：也叫工作线程，当线程的任务执行完或通知方式结束
- 守护线程：一般是为工作线程服务的，当所有的用户线程结束，守护线程自动结束（常见的守护线程：垃圾回收机制）

**并发**

同一个时刻，多个任务交替执行，造成一种“貌似同时”的错觉，简单的说,单核cpu实现的多任务就是并发。

**并行**

同一时刻，多个任务同时进行。多核CPU可以实现并行。

**线程的生命周期**

1. NEW
2. Runnable
3. Blocked
4. Waiting
5. Timed_Waiting
6. Terminated

![image-20211014192830934](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211014192830934.png)

###### 线程的基本使用

**创建线程的两种方式**

1. 继承Thread类，重写run方法

   ```java
   /**
    * @Description TODO
    * @Author Hua
    * @Date 2021/10/14
    * @Version 1.0
    **/
   public class ThreadDemo1 {
       public static void main(String[] args) {
   
           Cat cat = new Cat();
           cat.start();//启动线程
           
           /*
           start()底层源码
           (1)
           public synchronized void start() {
               start0();
           }
           (2)
           start0()是本地方法，是JVM调用，底层是c/c++实现
           真正实现多线程的效果，是start0()，而不是 run
           private native void start0();
           */        
       }
   }
   
   /**
    * 一个类继承了Thread类，该类就可以当做线程类使用
    * 重写run()，写自己的业务代码
    */
   class Cat extends Thread{
       int times = 0;
       @Override
       public void run() {
           while(true){
               times++;
               System.out.println("小猫：喵喵喵~~~" + times);
               try {
                   Thread.sleep(1000);
               } catch (InterruptedException e) {
                   e.printStackTrace();
               }
           }
       }
   }
   ```

2. 实现Runnable接口，重写run方法（建议使用）

   java是单继承的，在某些情况下一个类可能已经继承了某个父类,这时在用继承 Thread类方法来创建线程显然不可能了。

   java设计者们提供了另外一个方式创建线程，就是通过实现Runnable接口来创建线程

   ```java
   /**
    * @Description TODO
    * @Author Hua
    * @Date 2021/10/14
    * @Version 1.0
    **/
   public class ThreadRunnableDemo {
       public static void main(String[] args) {
   
           Dog dog = new Dog();
   
           Thread thread = new Thread(dog);
           thread.start();
   
       }
   }
   
   class Dog implements Runnable{
   
       int count = 0;
       @Override
       public void run() {
           while (true){
               System.out.println("线程：" + Thread.currentThread().getName() + " 小狗： 汪汪汪~~~" + (++count));
               try {
                   Thread.sleep(1000);
               } catch (InterruptedException e) {
                   e.printStackTrace();
               }
           }
       }
   }
   ```

使用JConsole监控线程执行情况

![image-20211014153458195](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211014153458195.png)



**使用多线程，模拟3个窗口同时售票**

```java
/**
 * @Description 使用多线程，模拟3个窗口同时售票
 * @Author Hua
 * @Date 2021/10/14
 * @Version 1.0
 **/
public class ThreadTicketDemo {
    public static void main(String[] args) {

        SellTicket01 sellTicket01 = new SellTicket01();
        SellTicket01 sellTicket02 = new SellTicket01();
        SellTicket01 sellTicket03 = new SellTicket01();
        sellTicket01.start();
        sellTicket02.start();
        sellTicket03.start();
    }
}

/**
 * 此种方式会产生超卖问题
 * 窗口 Thread-2售出一张票，剩余票数：4
 * 窗口 Thread-0售出一张票，剩余票数：2
 * 窗口 Thread-1售出一张票，剩余票数：1
 * 窗口 Thread-2售出一张票，剩余票数：2
 * 窗口 Thread-1售出一张票，剩余票数：-1
 * 窗口 Thread-0售出一张票，剩余票数：-2
 * 票已售完。。。
 * 窗口 Thread-2售出一张票，剩余票数：0
 * 票已售完。。。
 * 票已售完。。。
 */
class SellTicket01 extends Thread{

    private static int ticketNum = 100;
    @Override
    public void run() {
        while (true){
            if(ticketNum <= 0) {
                System.out.println("票已售完。。。");
                break;
            }
            try {
                Thread.sleep(50);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            System.out.println("窗口 " + Thread.currentThread().getName() + " 售出一张票，" + "剩余票数：" + (--ticketNum));
        }
    }
}

// 解决超卖问题
// 方式一：synchronized 同步方法

/**
 * synchronized实现线程同步
 */
class SellTicket02 extends Thread{

    private static int ticketNum = 100;

    private static boolean flag = true;

    public static synchronized void sell(){//同步 方法，在同一时刻，只能有一个线程执行该方法
        if(ticketNum <= 0) {
            System.out.println("票已售完。。。");
            flag = false;
            return;
        }
        try {
            Thread.sleep(50);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("窗口 " + Thread.currentThread().getName() + " 售出一张票，" + "剩余票数：" + (--ticketNum));
    }
    @Override
    public void run() {
        while (flag){
            sell();

        }
    }
}    

/**
 * 方式二:
 * synchronized 同步代码块
 */
class SellTicket03 extends Thread{

    private static int ticketNum = 100;

    private boolean flag = true;


    public void sell(){//同步方法，在同一时刻，只能有一个线程执行该方法
        synchronized (Object.class){
            if(ticketNum <= 0) {
                System.out.println("票已售完。。。");
                flag = false;
                return;
            }
            try {
                Thread.sleep(50);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            System.out.println("窗口 " + Thread.currentThread().getName() + " 售出一张票，" + "剩余票数：" + (--ticketNum));
        }

    }
    @Override
    public void run() {
        while (flag){
            sell();

        }
    }
}

```

**线程常用方法**

1. setName //设置线程名称，使之与参数name相同

2. getName //返回该线程的名称

3. start //使该线程开始执行;Java虚拟机底层调用该线程的start0方法

4. run //调用线程对象 run方法;

5. setPriority //更改线程的优先级

6. getPriority //获取线程的优先级

7. sleep //在指定的毫秒数内让当前正在执行的线程休眠（暂停执行)

8. interrupt //中断线程,但并没有正真的结束线程。所以一般用于中断正在休眠的线程。

9. yield :线程的礼让。让出cpu，让其他线程执行，但礼让的时间不确定，所以也不一定礼让成功

10. join :线程的插队。插队的线程一旦插队成功，则肯定先执行完插入的线程所有的任务
    案例:创建一个子线程，每隔1s输出hello,输出20次,主线程每隔1秒，输出hi,输出20次.要求:两个线程同时执行，当主线程输出5次后，就让子线程运行完毕，主线程再继续。

  ```java
  /**
   * @Description TODO
   * @Author Hua
   * @Date 2021/10/14
   * @Version 1.0
   **/
  public class ThreadJoinDemo {
  
      public static void main(String[] args) throws InterruptedException {
          MyThread thread = new MyThread();
          thread.start();
  
          for (int i = 1; i <= 20; i++) {
              System.out.println("hi~~~" + i);
              Thread.sleep(1000);
              if(i == 5){
                  thread.join();
              }
          }
      }
  }
  
  class MyThread extends Thread{
      private int count = 0;
      @Override
      public void run() {
          while (true){
              System.out.println("hello~~~" + (++count));
              try {
                  Thread.sleep(1000);
              } catch (InterruptedException e) {
                  e.printStackTrace();
              }
              if(count >=20){
                  break;
              }
          }
  
      }
  }
  
  ```

###### 线程同步机制

​		在多线程编程，一些敏感数据不允许被多个线程同时访问，此时就使用同步访问技
术，保证数据在任何同一时刻，最多有一个线程访问，以保证数据的完整性。
也可以这里理解: 即当有一个线程在对内存进行操作时，其他线程都不可以对这个内存地址进行操作,直到该线程完成操作,其他线程才能对该内存地址进行操作。

**互斥锁**

1.Java语言中，引入了对象互斥锁的概念，来保证共享数据操作的完整性

2.每个对象都对应于一个可称为“互斥锁”的标记，这个标记用来保证在任一时刻，只能有一个线程访问该对象。
3.关键字synchronized来与对象的互斥锁联系。当某个对象用synchronized修饰时,表明该对象在任一时刻只能由一个线程访问
4.同步的局限性:导致程序的执行效率要降低
5.同步方法(非静态的)的锁可以是this,也可以是其他对象(要求是同一个对象)

6.同步方法(静态的)的锁为当前类本身。

<font color=red>注意事项和细节：</font>

- 同步方法如果没有使用static修饰：默认锁对象为this
- 如果方法使用static修饰，默认锁对象：当前类.class
- 实现的步骤
  - 需要先分析上锁的代码
  - 选择同步代码块或同步方法
  - 要求多个线程的锁对象为同一个即可

**线程死锁**

多个线程都占用了对方的锁资源，且不肯相让，导致了死锁，在编程中是一定要避免死锁的发生。

```java
public class ThreadDeadLock {
    public static void main(String[] args) {

        new DeadLock(false).start();
        new DeadLock(true).start();

    }
}

class DeadLock extends Thread{

     static Object o1 = new Object();
     static Object o2 = new Object();
    boolean flag;

    public DeadLock(boolean flag){
        this.flag = flag;
    }

    @Override
    public void run() {

        if(flag){
            synchronized (o1){
                System.out.println(Thread.currentThread().getName() + "进入1");
                synchronized (o2){
                    System.out.println(Thread.currentThread().getName() + "进入2");
                }
            }
        }
        else{
            synchronized (o2){
                System.out.println(Thread.currentThread().getName() + "进入3");
                synchronized (o1){
                    System.out.println(Thread.currentThread().getName() + "进入4");
                }
            }
        }
    }
}
```

**释放锁**

1. 当前线程的同步方法、同步代码块执行结束
2. 当前线程在同步代码块、同步方法中遇到break、return
3. 当前线程在同步代码块、同步方法中出现了未处理的Error或Exception，导致异常结束
4. 当前线程在同步代码块、同步方法中执行了线程对象的wait()方法，当前线程暂停，并释放锁

**下面这些情况不会释放锁：**

- 线程执行同步代码块或同步方法时，程序调用Thread.sleep()、Thread.yield()方法暂停当前线程的执行,不会释放锁
- 线程执行同步代码块时，其他线程调用了该线程的suspend()方法将该线程挂起，该线程不会释放锁。
  提示;应尽量避免使用suspend()和resume()来控制线程,方法不再推荐



##### jvm

##### 类加载机制

##### 垃圾回收



##### 设计模式

###### 单例模式

顾名思义就是只有一个实例，并且它自己负责创建自己的对象，这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。

应用场景：

window桌面的回收站、网站计数器、多线程的线程池等。

- 需要频繁的进行创建和销毁的对象；
- 创建对象时耗时过多或耗费资源过多，但又经常用到的对象；
- 工具类对象；
- 频繁访问数据库或文件的对象。

单例的实现主要是通过以下两个步骤：

1. 将该类的构造方法cxz义为私有方法，这样其他处的代码就无法通过调用该类的构造方法来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例；
2. 在该类内提供一个静态方法，当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用。

实现方式：

1. 懒汉式  (线程不安全，不可用)

   ```java
   // 1、线程不安全（不可用）
   public class Singleton {
    
       // 指向自己实例的私有静态引用
       private static Singleton instance;
    
       // 私有的构造方法
       private Singleton(){}
    
       // 以自己实例为返回值的静态的公有方法，静态工厂方法
       public static Singleton getInstance(){
           // 被动创建，在真正需要使用时才去创建
           if (instance == null) {
               instance = new Singleton();
           }
           return instance;
       }
   }
   // 这种写法起到了Lazy Loading的效果，但是只能在单线程下使用。如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式。
   
   // 2、同步方法，线程安全（不推荐使用）
   public class Singleton {
   
       private static Singleton singleton;
   
       private Singleton() {}
   
       public static synchronized Singleton getInstance() {
           if (singleton == null) {
               singleton = new Singleton();
           }
           return singleton;
       }
   }
   
   //解决上面第一种实现方式的线程不安全问题，做个线程同步就可以了，于是就对getInstance()方法进行了线程同步。
   // 缺点：效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return就行了。方法进行同步效率太低要改进。
   
   // 3、同步代码块，线程安全（不可用）
   public class Singleton {
   
       private static Singleton singleton;
   
       private Singleton() {}
   
       public static Singleton getInstance() {
           if (singleton == null) {
               synchronized (Singleton.class) {
                   singleton = new Singleton();
               }
           }
           return singleton;
       }
   }
   // 由于第二种实现方式同步效率太低，所以摒弃同步方法，改为同步产生实例化的的代码块。但是这种同步并不能起到线程同步的作用。跟第一种实现方式遇到的情形一致，假如一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例
   
   ```

   

2. 饿汉式

   ```java
   // 1、静态常量 （可用）
   public class Singleton {
   
       private final static Singleton INSTANCE = new Singleton();
   
       private Singleton(){}
   
       public static Singleton getInstance(){
           return INSTANCE;
       }
   }
   
   //优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。
   
   //缺点：在类装载的时候就完成实例化，没有达到Lazy Loading的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费。
   
   // 2、静态代码块 （可用）
   public class Singleton {
   
       private static Singleton instance;
   
       static {
           instance = new Singleton();
       }
   
       private Singleton() {}
   
       public static Singleton getInstance() {
           return instance;
       }
   }
   
   "这种方式和上面的方式其实类似，只不过将类实例化的过程放在了静态代码块中，也是在类装载的时候，就执行静态代码块中的代码，初始化类的实例。优缺点和上面是一样的。"
   
   ```

   

3. 双检锁 （推荐用）

   ```java
   public class Singleton {
   
       private static volatile Singleton singleton;
   
       private Singleton() {}
   
       public static Singleton getInstance() {
           if (singleton == null) {
               synchronized (Singleton.class) {
                   if (singleton == null) {
                       singleton = new Singleton();
                   }
               }
           }
           return singleton;
       }
   }
   // Double-Check概念对于多线程开发者来说不会陌生，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。
   // 优点：线程安全；延迟加载；效率较高
   ```

   

4. 静态内部类  (推荐用)

   ```java
   public class Singleton {
   
       private Singleton() {}
   
       private static class SingletonInstance {
           private static final Singleton INSTANCE = new Singleton();
       }
   
       public static Singleton getInstance() {
           return SingletonInstance.INSTANCE;
       }
   }
   // 这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。
   //类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。
   //优点：避免了线程不安全，延迟加载，效率高。
   ```

   

5. 枚举 (推荐用)

   ```java
   public enum Singleton {
       INSTANCE;
       public void whateverMethod() {
   		//
       }
   }
   // 直接通过Singleton.INSTANCE.doSomething()的方式调用即可。方便、简洁又安全。
   ```

###### 工厂模式



##### 常用算法



#### 三、Mysql

#### 四、Redis

###### 数据类型

1. string  

   Redis最基本的数据类型，一个Redis中字符串value最多可以是512M 

   - 赋值：set key value
   - 取值：get key/getset key value
   - 删除：del key
   - 数值自增和 自减 ：incr key / decr key
   - 为key增加一个指定数值：incrby key increment
   - 为key减少一个指定数值：decrby key decrement
   - 拼凑字符串：append key value 

2. hash  

   是一个string类型的field和value的映射表，hash特别适合用于存储对象

3. list

   简单的字符串列表，按照插入顺序排序

4. set

   是string类型的无序集合，自动排重

5. zset

   是一个没有重复元素的字符串有序集合

###### Redis的持久化

- RDB

  在指定的时间间隔内将内存中的数据集快照写入磁盘， 也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里

  优势：

  - 适合大规模的数据恢复
  - 对数据完整性和一致性要求不高更适合使用
  - 节省磁盘空间
  - 恢复速度快

  劣势

  - Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑
  - 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。

  ![image-20210723110536032](C:\Users\Hua\AppData\Roaming\Typora\typora-user-images\image-20210723110536032.png)

- AOF

  以日志的形式来记录每个写操作（增量保存），将Redis执行过的所有写指令记录下来(读操作不记录)， 只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作

  默认不开启，可以在redis.conf中配置文件名称

  AOF和RDB同时开启，系统默认取AOF的数据

  **AOF同步频率设置**

  - appendfsync always

    始终同步，每次Redis的写入都会立刻记入日志；性能较差但数据完整性比较好

  - appendfsync everysec

    每秒同步，每秒记入日志一次，如果宕机，本秒的数据可能丢失

  - appendfsync no

    redis不主动进行同步，把同步时机交给操作系统

  优势：

  - 备份机制更稳健，丢失数据概率更低。
  - 可读的日志文本，通过操作AOF文件，可以处理误操作。

  劣势：

  - 比起RDB占用更多的磁盘空间。
  - 恢复备份速度要慢。
  - 每次读写都同步的话，有一定的性能压力。

  总结：

  ![image-20210723111256919](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210723111256919.png)



###### 缓存穿透

参考：https://www.imooc.com/article/283986?from=timeline

**缓存流程**

前台请求，后台先从缓存中取数据，取到直接返回结果，取不到时从数据库中取，数据库取到更新缓存，并返回结果，数据库也没取到，那直接返回空结果。

![img](https://gitee.com/zhengqianhua0314/image-store/raw/master/20180919143214712)



缓存穿透是指查询一个一定不存在的数据，由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查，给数据库造成了很大的压力，失去了缓存的意义。

**这里需要注意和缓存击穿的区别，缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞**

**如何解决缓存穿透：**

- 布隆过滤器

  将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。

- 缓存空对象

  （简单粗暴）如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。

  但是这种方法会存在两个问题：

  - 如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多的空值的键；

  - 即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。

###### 缓存击穿

key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。

**如何解决缓存击穿**

- 互斥锁的方式来实现
- edis、tair、zookeeper 等提供的分布式锁来实现

###### 缓存雪崩

当缓存服务器重启、挂掉或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力

**雪崩场景**

- Redis挂掉了，请求全部走数据库.
- 对缓存数据设置相同的过期时间，导致某段时间缓存失效，请求全部走数据库.

这种情况下，很可能就把我们的数据库搞垮，导致整个服务器瘫痪。

**如何解决缓存雪崩**

- Redis挂掉了，请求全部走数据库的情况

  - 事发前：实现Redis的高可用（主从架构+Sentinel(哨兵) 或者Redis Cluster (集群)），尽量避免Redis挂掉
  - 事发中：万一Redis真的挂了，我们可以设置本地缓存（ehcache）+ 限流（hystrix），尽量避免我们的数据库被干掉
  - 事发后：Redis持久化，重启后自动从磁盘上加载数据，快速回复缓存数据

- 缓存数据设置相同的过期时间，导致某段时间缓存失效，请求全部走数据库

  在缓存的时候给过期时间加上一个随机值，这样就会大幅减少缓存在同一时间过期。

#### 五、Nginx

##### 负载均衡

**负载均衡策略**

- 轮询

  ```properties
  upstream zqhtest{
  	server 127.0.0.1:8080;
  	server 127.0.0.1:8082;
  }
  ```

- 权重

  ```properties
  upstream weightTest{
  	server 127.0.0.1:8080 weight=1;
  	server 127.0.0.1:8082 weight=2;
  }
  ```

- ip_hash

  ```properties
  upstream weightTest{
  	ip_hash;
  	server 127.0.0.1:8080;
  	server 127.0.0.1:8082;
  }
  ```

  

**负载均衡配置：**

- http{}内配置upstream(server{}同级别)

  ```xml
  upstream zqhtest{
  	server 127.0.0.1:8080;
  	server 127.0.0.1:8082;
  }
  ```

- server{}内配置location{}的proxy_pass

  ```xml
  server{
  	...
  
  	location /abc {
  		proxy_pass http://zqhtest/;
  	}
  
  }
  ```

##### 动静分离

#### 六、Zookeeper

######  选举机制

选举流程:
zookeeper集群有5台服务器，那么根据半数机制，zookeeper集群正常启动至少需要3台，当票数相同时会比较服务器的myid的值，myid的值是唯—的。
(1)当server1启动时，发起第一次选举。此时，server1会投给自己一票，此时server1的票数为1<3，不满足半数机制，leader选举未完成，集群无法正常启动，server1保持为looking 状态;
(2)当server2启动时，发起第二次选举。server1和server2会先投自己一票并相互交换选票信息，交互过程: server1和server2的投票票数相同，会比较myid，server1 发现 server2的myid 比自己投票推举的(server1)大，改投server2。此时,server1的票数为0, server2的票数为2，不满足半数机制，选举无法完成，集群无法正常启动，server1和server2保持为looking 状态;
3 server3启动时，发起第三次选举。先给自己投一票后交换选票信息，然后比较myid，server3的myid最大，server1和server2都会更改选票信息，改投server3。此时, server1的票数为0,server2的票数为0， server3的票数为3。当前已经满足半数机制, server3当选leader。server1和server2更改状态为following,server3的状态为leading;
(4) server4启动时，发起第四次选举。由于server1、server2、server3已经不是looking状态，不会更改选票信息。此时，server1的票数为0，server2的票数为O, server3的票数为3, server4的票数为0。server4服从多数，更改选票信息为server3并更改状态为following,，server3的票数为4。(也可以理解为:集群中老大已经诞生了，其他都会甘愿做小弟，把票投给老大)
(5) server5启动，跟server4的情况─样，最终状态为following。

### 1 大数据组件

##### 大数据组件端口号

| 组件   | HDFS                | MR   | Hadoop历史服务器 | 客户端    |      |      |      |      |      |
| ------ | ------------------- | ---- | ---------------- | --------- | ---- | ---- | ---- | ---- | ---- |
| 端口号 | 2.x 50070  3.x 9870 | 8088 | 19888            | 9000/8020 |      |      |      |      |      |

![image-20210904101935953](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210904101935953.png)

#### 1.1 Hadoop

- 广义上说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。
- 狭义上说，Hadoop指Apache这款开源框架，是分布式系统基础架构。它的核心组件有：
  - HDFS(分布式文件系统)：解决海量数据存储
  - YARN（作业调度和集群资源管理）：解决资源任务调度
  - MapReduce（分布式运算编程框架）：解决海量数据计算

**Hadoop特性优点**

- 扩展能力：Hadoop是在可用的计算机集群间分配数据并完成计算任务的，这些集群可以方便的扩展到数以千计的节点中。
- 成本低：Hadoop通过普通廉价的机器组成服务器集群来分发以及处理数据，以至于成本很低。
- 高效率：通过并发数据，Hadoop可以在节点之间动态并行的移动数据，使得速度非常快。
- 可靠性：能自动维护数据的多份复制，并且在任务失败后能自动地重新部署计算任务。所以Hadoop的按位存储和处理数据的能力值得人们信赖。

Hadoop集群中hadoop都需要启动哪些进程，他们的作用分别是什么

- namenode

  HDFS的守护进程，负责维护整个文件系统，存储着整个文件系统的元素据信息如文件名，文件目录结构，文件属性以及每个文件的块列表和块所在的额datanode等，image+editlog

- datanode

  是文件系统的工作节点，存储文件块数据，以及块数据的校验和。

- secondarynamenode

  守护进程，相当于一个namenode的元数据的备份机制，定期的更新，和namenode进行通信，将namenode上的image和edits进行合并，可以作为namenode的备份使用

- resourcemanager

  是YARN的守护进程，

  - 负责所有资源的分配与调度
  - client的请求由它负责
  - 监控nodemanager
  - 启动或监控ApplicationMaster

- nodemanager

  - 单个节点的资源管理，
  - 执行来自resourcemanager的具体任务和命令
  - 处ApplicationMaster的命令

- DFSZKFailoverController

  高可用是他负责监控NN的状态，并及时的把状态信息写入ZK。它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为active NN的权利，因为最多只有2个节点，目前选择策略还比较简单（先到先得，轮换）。

- JournalNode

  高可用情况下存放namenode的editlog文件

##### 1、 HDFS  

<font color = red>**端口：(9870)**</font>

分布式文件管理系统，适合一次写入，多次读出的场景，且不支持文件的修改。

###### HDFS的架构

![image-20210721142755430](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721142755430.png)

- NameNode（nn）：也就是Master
  - 管理HDFS的名称空间
  - 配置副本策略
  - 管理数据块（Block）映射信息
  - 处理客户端读写请求
- DataNode（dn）：也就是slave
  - 存储实际的数据块
  - 执行数据块的读写操作
- Client
  - 文件切分，文件上传HDFS的时候，Client将文件切分成一个个的Block，然后进行上传
  - 与NameNode交互，获取文件的位置信息
  - 与DataNode交互，读取或写入数据
  - Client提供一些命令来管理HDFS，比如NameNode格式化
  - Client可以通过一些命令来访问HFDS，比如对HDFS增删改查操作
- SecondaryNameNode
  - 辅助NameNode，分担其工作量，比如定期合并Fsiimage和Edits，并推送给NameNode
  - 在紧急情况下，可辅助恢复NameNode

###### HDFS读写流程

**写流程**

![image-20210721151358219](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721151358219.png)

1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查该用户是否有上传权限，以及目标文件是否已存在，父目录是否存在，如果这两者有任意一个不满足，则直接报错，如果两者都满足，
   则返回给客户端一个可以上传的信息。
2. client 根据文件的大小进行切分，默认 128M 一块，切分完成之后给
   namenode 发送请求第一个 block 块上传到哪些服务器上
3. namenode 收到请求之后，根据网络拓扑和机架感知以及副本机制进行文
   件分配，返回可用的 DataNode 的地址
4. 客户端收到地址之后与服务器地址列表中的一个节点如 A 进行通信，本质
   上就是 RPC 调用，建立 pipeline，A 收到请求后会继续调用 B，B 在调用 C，将整个 pipeline 建立完成，逐级返回 client
5. client 开始向 A 上发送第一个 block（先从磁盘读取数据然后放到本地内存
   缓存），以 packet（数据包，64kb）为单位，A 收到一个 packet 就会发送给B，然后 B 发送给 C，A 每传完一个 packet 就会放入一个应答队列等待应答
6. 数据被分割成一个个的 packet 数据包在 pipeline 上依次传输，在
   pipeline 反向传输中，逐个发送 ack（命令正确应答），最终由 pipeline 中第一个 DataNode 节点 A 将 pipeline ack 发送给 Client
7. 当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个
   block ，namenode 重新选择三台 DataNode 给 client

**读流程**

![image-20210721152415020](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721152415020.png)

1. 客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。



###### HDFS文件块大小

HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）设置，默认大小Hadoop2.x版本中是128M，老版本中是64M。

为什么块的大小不能设置太小，太大

- HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置
- 如果块设置太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间，导致程序在处理这块数据时，非常慢
- 总结：HDFS块的大小设置主要取决于磁盘传输速率

###### 熟悉HDFS的配置



###### NN和DN的作用

DataNode工作机制

![image-20210721154141383](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721154141383.png)

- 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。
- DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。
- 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。
- 集群运行中可以安全加入和退出一些机器。

DataNode掉线时限参数设置

![image-20210721154451072](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721154451072.png)

需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。

```xml
<property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>300000</value>
</property>
<property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
</property>
```



###### NameNode和SecondaryNameNode

NameNode工作机制

![image-20210721153014978](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721153014978.png)

1. NameNode启动
   - 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
   - 客户端对元数据进行增删改的请求。
   - NameNode记录操作日志，更新滚动日志。
   - NameNode在内存中对元数据进行增删改。
2. Secondary NameNode工作
   - SecondaryNameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。
   - Secondary NameNode请求执行CheckPoint。
   - NameNode滚动正在写的Edits日志。
   - 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。
   - Secondary NameNode加载编辑日志和镜像文件到内存，并合并。
   - 生成新的镜像文件fsimage.chkpoint。
   - 拷贝fsimage.chkpoint到NameNode。
   - NameNode将fsimage.chkpoint重新命名成fsimage。

**NN和2NN工作机制详解：**

```

Fsimage：NameNode内存中元数据序列化后形成的文件。
Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。
NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。
由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。
SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。
```

**CheckPoint时间设置**

- 通常情况下，SecondaryNameNode每隔一小时执行一次。

  ​	hdfs-default.xml

  ```xml
  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>3600s</value>
  </property>
  ```

- 一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次

  ```xml
  <property>
    <name>dfs.namenode.checkpoint.txns</name>
    <value>1000000</value>
  <description>操作动作次数</description>
  </property>
  
  <property>
    <name>dfs.namenode.checkpoint.check.period</name>
    <value>60s</value>
  <description> 1分钟检查一次操作次数</description>
  </property >
  ```

  

###### 机架感知 （副本存储节点选择）

![image-20210721152259789](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721152259789.png)

###### HDFS安全模式

安全模式是HDFS所处的一种特俗状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求，是一种保护机制。用于保证集群中的数据块的安全性。

在NameNode主节点启动时，HDFS首先进入安全模式，集群会开始检查数据块的完整性，DataNode在启动的时候会向NameNode汇报可用的Block信息，当整个系统达到安全标准时就，HDFS自动离开安全模式。

###### HDFS操作文件的常用命令

1. mkdir创建目录

   ```shell
   # hadoop fs -mkdir <paths>
   
   hadoop fs -mkdir /bigdata/dir1
   
   hadooop fs -mkdir -p /bigdata/dir2/dir3
   ```

2. put上传文件

   ```shell
   # hadoop fs -put <localsrc> ... <dst>
   
   hadoop fs -put localfile /bigdata/dir1
   ```

3. ls列出文件

   ```shell
   # hadoop fs -ls <args>
   
   hadoop fs -ls /bigdata/dir1
   ```

4. lsr    递归列出子目录中的文件及目录信息

   ```shell
   # hadoop fs -lsr <args>
   
   hadoop fs -lsr /bigdata
   ```

5. cat    将路径指定文件的内容输出到stdout

   ```shell
   # hadoop fs -cat URI [URI...]
   
   hadoop fs -cat /bigdata/*
   ```

6. get    复制文件到本地文件系统

   ```shell
   # hadoop fs -get [-ignorecrc] [-crc] <src> <localdst>
   
   hadoop fs -get /bigdata/test.txt localfile
   ```

7. rm    删除指定文件,只删除非空目录和文件

   rmr 递归删除

   ```shell
   # hadoop fs -rm URI [URI...]
   
   hadoop fs -rm /bigdata/test.txt
   
   # hadoop fs -rm URI [URI...]
   
   hadoop fs -rmr /bigdata
   ```

8. chgrp   改变文件所属的组,命令的使用者必须是文件的所有者或超级用户

   ```shell
   # hadoop fs -chgrp [-R] GROUP URI [URI...]
   
   hadoop fs -chgrp root /bigdata/test.txt
   ```

9. chmod    改变文件的权限，命令的使用者必须是文件的所有者或超级用户

   ```shell
   hadoop fs -chmod 744 /bigdata/test.txt
   ```

10. copyFromLocal    上传文件，除了限定资源路径是一个本地文件外，与put命令相似

    ```shell
    # hadoop fs -copyFromLocal <localsrc> URI
    
    hadoop fs -copyFromLocal word.txt /bigdata/dir1
    # 再上传一次就会报错，如果想要覆盖文件要加-f
    
    hadoop fs -copyFromLocal -f word.txt /bigdata/dir1
    ```

11. copyToLocal    复制文件到本地文件系统 ,与get命令相似

    ```shell
    # hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>
    
    hadoop fs -copyToLocal /bigdata/dir1/word.txt 
    ```

12. cp    复制文件

    ```shell
    # hadoop fs -cp URI[URI...] <dest>
    
    hadoop fs -cp /bigdata/test.txt /bigdata/dir1
    ```

13. du    显示目录中所有文件的大小

    ```shell
    # hadoop fs -du URI [URI...]
    
    hadoop fs -du /bigdata
    ```

14. expunge    清空回收站

    ```shell
    hadoop fs -expunge
    ```

15. mv    移动文件

    ```shell
    # hadoop fs -mv URI [URI...] <dest>
    
    hadoop fs -mv /bigdata/dir1/word.txt /bigdata
    ```

16. setrep    改变一个文件的副本系数

    ```shell
    # hadoop fs -setrep [-R] <path>
    # -R 用于递归改变目录下所有文件的副本系数
    
    hadoop fs -setrep -w 3 -R /bigdata
    ```

17. stat    返回指定路径的统计信息

    ```shell
    # hadoop fs -stat URI [URI...]
    
    hadoop fs -stat /bigdata
    ```

18. tail    将文件尾部的内容输出到stdout

    ```shell
    # hadoop fs -tail [-f] URI
    
    hadoop fs -tail /bigdata/test.txt
    ```

19. touchz    创建一个空文件

    ```shell
    # hadoop fs -touchz URI [URI...]
    
    hadoop fs -touchz /bigdata/new.txt
    ```

###### hadoop系统管理命令

1. 查看hadoop 版本

   ```shell
   hadoop version
   ```

   

2. 启动hadoop所有进程

   ```shell
   sbin/start-all.sh
   ```

   

3. 停止hadoop所有进程

   ```shell
   sbin/stop-all.sh
   ```

   

4. 格式化一个新的分布式文件系统

   ```shell
   bin/hadoop namenode -format
   ```

   

5. 启动HDFS

   ```shell
   bin/start-dfs.sh
   ```

   

6. 停止HDFS

   ```shell
   bin/stop-dfs.sh
   ```

   

7. 启动Yarn

   ```shell
   bin/start-yarn.sh
   ```

   

8. 停止Yarn

   ```shell
   bin/stop-yarn.sh
   ```

9. HDFS 安全模式

   - 进入安全模式

     ```shell
     bin/hadoop dfsadmin -safemode enter
     ```

     

   - 推出安全模式

     ```shell
     bin/hadoop dfsadmin -safemode leave
     ```

     

   - 查看集群是否处于安全模式

     ```shell
     bin/hadoop dfsadmin -safemode get
     ```

##### 2、YARN

<font color=red>**端口：（8088）**</font>

###### YARN架构图

YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。

![image-20210721135231860](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721135231860.png)

###### yarn工作机制

![image-20210722100512097](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722100512097.png)

1. MR程序提交到客户端所在的节点。
2. YarnRunner向ResourceManager申请一个Application。
3. RM将该应用程序的资源路径返回给YarnRunner。
4. 该程序将运行所需资源提交到HDFS上。
5. 程序资源提交完毕后，申请运行mrAppMaster。
6. RM将用户的请求初始化成一个Task。
7. 其中一个NodeManager领取到Task任务。
8. 该NodeManager创建容器Container，并产生MRAppmaster。
9. Container从HDFS上拷贝资源到本地。
10. MRAppmaster向RM 申请运行MapTask资源。
11. RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。
12. MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。
13. MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。
14. ReduceTask向MapTask获取相应分区的数据。
15. 程序运行完毕后，MR会向RM申请注销自己。

###### yarn资源调度策略

Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。

具体设置详见：yarn-default.xml文件

```xml
<property>
    <description>The class to use as the resource scheduler.</description>
    <name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>
```

- 先进先出调度器（FIFO）

  ![image-20210722101146158](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722101146158.png)

  即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。

- 容量调度器（Capacity Scheduler）

  ![image-20210722101432248](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722101432248.png)

  Capacity Scheduler 主要有以下几个特点：

  - 容量保证。管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。
  - 灵活性，如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。
  - 多重租赁。支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。
  - 安全保证。每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。
  - 动态更新配置文件。管理员可根据需要动态修改各种配置参数，以实现在线集群管理。

- 公平调度器（Fair Scheduler）

  ![image-20210722101741550](C:\Users\Hua\AppData\Roaming\Typora\typora-user-images\image-20210722101741550.png)

  公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源。当有作业提交上来，系统会将空闲的资源分配给新的作业，每个任务大致上会获取平等数量的资源。和传统的调度策略不同的是它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的任务挨饿！

###### yarn的配置

###### yarn任务调度的整体过程

##### 3、MapReduce

MapReduce是一个分布式运算程序的编程框架，将计算过程分为两个阶段：Map和Reduce

- Map阶段并行处理输入数据

- Reduce阶段对Map结果进行汇总

**优点**

- MapReduce易于编程
- 良好的扩展性
- 高容错性
- 适合PB级以上海量数据的离线处理

**缺点**

- 不擅长实时计算
- 不擅长流式计算
- 不擅长DAG(有向无环图)计算

###### MR的工作原理

![image-20210721161706253](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721161706253.png)

MapReduce执行流程

![image-20210722095400570](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722095400570.png)

![image-20210722095425768](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722095425768.png)

上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：

1. MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中
2. 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序
5. ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据
6. ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）
7. 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）

**注意：**

- Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。
- 缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。



###### MR手写代码实现简单的wc或topN

- wordcount

  1. 编写Mapper类

     ```java
     package com.atguigu.mapreduce;
     import java.io.IOException;
     import org.apache.hadoop.io.IntWritable;
     import org.apache.hadoop.io.LongWritable;
     import org.apache.hadoop.io.Text;
     import org.apache.hadoop.mapreduce.Mapper;
     
     public class WordcountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
     	
     	Text k = new Text();
     	IntWritable v = new IntWritable(1);
     	
     	@Override
     	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {
     		
     		// 1 获取一行
     		String line = value.toString();
     		
     		// 2 切割
     		String[] words = line.split(" ");
     		
     		// 3 输出
     		for (String word : words) {
     			
     			k.set(word);
     			context.write(k, v);
     		}
     	}
     }
     ```

     

  2. 编写Reducer类

     ```java
     package com.atguigu.mapreduce.wordcount;
     import java.io.IOException;
     import org.apache.hadoop.io.IntWritable;
     import org.apache.hadoop.io.Text;
     import org.apache.hadoop.mapreduce.Reducer;
     
     public class WordcountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
     
     int sum;
     IntWritable v = new IntWritable();
     
     	@Override
     	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
     		
     		// 1 累加求和
     		sum = 0;
     		for (IntWritable count : values) {
     			sum += count.get();
     		}
     		
     		// 2 输出
              v.set(sum);
     		context.write(key,v);
     	}
     }
     ```

     

  3. 编写Driver驱动类

     ```java
     package com.atguigu.mapreduce.wordcount;
     import java.io.IOException;
     import org.apache.hadoop.conf.Configuration;
     import org.apache.hadoop.fs.Path;
     import org.apache.hadoop.io.IntWritable;
     import org.apache.hadoop.io.Text;
     import org.apache.hadoop.mapreduce.Job;
     import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
     import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
     
     public class WordcountDriver {
     
     	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
     
     		// 1 获取配置信息以及获取job对象
     		Configuration configuration = new Configuration();
     		Job job = Job.getInstance(configuration);
     
     		// 2 关联本Driver程序的jar
     		job.setJarByClass(WordcountDriver.class);
     
     		// 3 关联Mapper和Reducer的jar
     		job.setMapperClass(WordcountMapper.class);
     		job.setReducerClass(WordcountReducer.class);
     
     		// 4 设置Mapper输出的kv类型
     		job.setMapOutputKeyClass(Text.class);
     		job.setMapOutputValueClass(IntWritable.class);
     
     		// 5 设置最终输出kv类型
     		job.setOutputKeyClass(Text.class);
     		job.setOutputValueClass(IntWritable.class);
     		
     		// 6 设置输入和输出路径
     		FileInputFormat.setInputPaths(job, new Path(args[0]));
     		FileOutputFormat.setOutputPath(job, new Path(args[1]));
     
     		// 7 提交job
     		boolean result = job.waitForCompletion(true);
     		System.exit(result ? 0 : 1);
     	}
     }
     ```

     

###### combinner

![image-20210721165534051](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721165534051.png)

自定义Combiner步骤

1. 自定义一个Combiner继承Reducer，重写Reduce方法

   ```java
   public class WordcountCombiner extends Reducer<Text, IntWritable, Text,IntWritable>{
   
   	@Override
   	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
   
           // 1 汇总操作
   		int count = 0;
   		for(IntWritable v :values){
   			count += v.get();
   		}
   
           // 2 写出
   		context.write(key, new IntWritable(count));
   	}
   }
   ```

2. 在Job驱动类中设置：  

   ```java
   job.setCombinerClass(WordcountCombiner.class);
   ```

   

###### partitioner

如果要求将统计结果按照条件输出到不同文件中（分区）。

默认Partitioner分区

![image-20210721164643608](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721164643608.png)

**自定义Partitioner步骤**

1. 自定义类继承Partitioner，重写个体Partition()方法

   ```java
   public class ProvincePartitioner extends Partitioner<Text, FlowBean> {
   
   	@Override
   	public int getPartition(Text key, FlowBean value, int numPartitions) {
   
   		// 1 获取电话号码的前三位
   		String preNum = key.toString().substring(0, 3);
   		
   		int partition = 4;
   		
   		// 2 判断是哪个省
   		if ("136".equals(preNum)) {
   			partition = 0;
   		}else if ("137".equals(preNum)) {
   			partition = 1;
   		}else if ("138".equals(preNum)) {
   			partition = 2;
   		}else if ("139".equals(preNum)) {
   			partition = 3;
   		}
   
   		return partition;
   	}
   }
   ```

2. 在job驱动类中设置自定义Partitioner

   ```java
   job.setPartitionerClass(ProvincePartitioner.class);
   ```

   

3. 自定义Partition后，要根据自定义Partition的逻辑设置相应数量的ReduceTask

   ```
   job.setNumReduceTasks(5);
   ```

![image-20210721165241640](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721165241640.png)

###### MR数据倾斜问题

###### shuffle的原理，减少shuffle的方法

Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。

map:首先执行getpartition()标记分区，然后进入环形缓冲区，默认100M，阈值80%，达到后溢写，排序-快排，对key的索引排序，按照字典顺序排序。然后分区内部进行归并排序，写入磁盘。

reduce：拉取数据，放到内存中，然后对数据进行归并排序，按照相同的key分组。

![image-20210722100105428](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722100105428.png)

###### Mapreduce优化

执行Mapreduce常见的问题

1. client对集群中HDSF的操作没有权限

2. 提交集群运行，运行失败

   ```java
   job.setJar("");
   ```

   

##### 4、其他

###### hadoop集群搭建过程及常见的bug

- DataNode和NameNode进程同时只能工作一个

  ![image-20210721140326154](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721140326154.png)

###### hadoop集群的扩容

###### 

##### 





#### 1.2 Spark





###### Shuffle



#### 1.3 Flink

#### 1.4 Kafka

###### kafka介绍

Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域，使用scala语言编写。

kafka相比其他消息队列的优势

- 可靠性：分布式，分区，复制和容错
- 可扩展性：kafka消息传递系统轻松缩放，无需停机
- 耐用性：kafka使用分布式提交日志，这意味着消息会尽可能快速的保存在磁盘上，因此它是持久的
- 性能：kafka对于发布和订阅消息都具有高吞吐量，即使存储了许多TB的消息，他也保持稳定的性能
- 速度快：保证零停机和零丢失

###### 为什么使用Kafka

- 缓冲和削峰，上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka 在中间可以起到一个缓冲的作用，把消息暂存在 kafka 中，下游服务就可以按照自己的节奏进行慢慢处理。
  
- 解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。
  
- 冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅 topic的服务消费到，供多个毫无关联的业务使用。
  
- 健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。
  
- 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。
  
  

###### kafka架构

![image-20210723144904892](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210723144904892.png)

- **Producer**

  消息生产者，就是向kafka broker发消息的客户端

- **Consumer**

  消息消费者，向kafka broker取消息的客户端

- **Consumer Group**

  消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。

- **Broker**

  一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic

- **Topic**

  可以理解为一个队列，生产者和消费者面向的都是一个topic

- **Partition**

  为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列

- **Replica**

  副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower

- **Leader**

  每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader

- **Follower**

  每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader

###### kafka消费的有序性

单分区内有序，多分区，分区间无序。

如果想保证消费者有序，可以从业务上把需要有序的打到同⼀个 partition。

###### 分区与消费者组间的关系

每个分区只能由同一个消费组内的一个消费者(consumer)来消费，可以由不同的消费组的消费者来消费，同组的消费者则起到并发的效果。

###### 生产者分区策略

分区的原因：

- 方便在集群中扩展
- 可以提高并发

分区的原则：

- 指明 partition 的情况下，直接将指明的值直接作为 partiton 值
- 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值
- 既没有 partition 值又没有 key 值的情况下，所有发往指定话题的records，会积攒成一个batch（达到一定大小或者两条消息间隔过长）一起发送到一个分区 。当形成新的batch，我们会随机选择一个新的分区发送

###### 消费者分区分配策略

参考：https://zhuanlan.zhihu.com/p/127349064

一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。

Kafka有三种分配策略，roundrobin，range，Sticky(0.11.0才被提出来的)。

- range（默认）

  对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。

- roundRobin

  策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。

- sticky

  Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的：

  1. 分区的分配要尽可能的均匀，分配给消费者者的主题分区数最多相差一个；
  2. 分区的分配尽可能的与上次分配的保持相同。

**触发分区重平衡的时机**

- 消费者的数量增加或减少
- 分区数量增加，目前只能增加，不能减少
- 订阅的主题发生变化

```properties
应尽量减少分区重平衡的时机，因为重平衡过程中，消费者无法从kafka消费消息，这对kafka的TPS影响极大，而如果kafka集内节点较多，比如数百个，那重平衡可能会耗时极多。数分钟到数小时都有可能，而这段时间kafka基本处于不可用状态
```

**如何减少分区重平衡的次数**

- session.timout.ms :  控制心跳超时时间
- heartbeat.interval.ms :控制心跳发送频率
- max.poll.interval.ms  :控制消费者poll的间隔

推荐设置

```properties
session.timout.ms：设置为6s
heartbeat.interval.ms：设置2s
max.poll.interval.ms：推荐为消费者处理消息最长耗时再加1分钟
```



###### 数据不丢失

可以从三方面保证：producer,consumer,broker

- **生产者**

  **ack 机制**：在 kafka 发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到，其中状态有 0，1，-1。
  
  - 同步模式
  
    ack 设置为 0，风险很大，一般不建议设置为 0。即使设置为 1，也会随着 leader宕机丢失数据。所以如果要严格保证生产端数据不丢失，可设置为-1。
  
  - 异步模式
  
    也会考虑 ack 的状态，除此之外，异步模式下的有个 buffer，通过 buffer 来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果 buffer满了数据还没有发送出去，有个选项是配置是否立即清空 buffer。可以设置为-1，永久阻塞，也就数据不再生产。异步模式下，即使设置为-1。也可能因为程序员的不科学操作，操作数据丢失，比如 kill -9，但这是特别的例外情况。
  
  ```properties
  注：
  ack=0：producer 不等待 broker 同步完成的确认，继续发送下一条(批)信息。
  ack=1（默认）：producer 要等待leader成功收到数据并得到确认，才发送下一条message。
  ack=-1：producer 得到 follwer 确认，才发送下一条数据。
  ```
  
- **消费者**

  通过 offset commit 来保证数据的不丢失，kafka 自己记录了每次消费的 offset 数值，下次继续消费的时候，会接着上次的 offset 进行消费。
  
  而 offset 的信息在 kafka0.8 版本之前保存在 zookeeper 中，在 0.8 版本之后保存到 topic 中，即使消费者在运行过程中挂掉了，再次启动的时候会找到 offset 的值，找到之前消费消息的位置，接着消费，由于 offset 的信息写入的时候并不是每条消息消费完成后都写入的，所以这种情况有可能会造成重复消费，但是不会丢失消息。
  
  唯一例外的情况是，我们在程序中给原本做不同功能的两个 consumer 组设置KafkaSpoutConfig.bulider.setGroupid 的时候设置成了一样的 groupid，这种情况会导致这两个组共享同一份数据，就会产生组 A 消费 partition1，partition2 中的消息，组 B 消费 partition3 的消息，这样每个组消费的消息都会丢失，都是不完整的。 为了保证每个组都独享一份消息数据，groupid 一定不要重复才行。
  
- **集群的Broker**

  每个 broker 中的 partition 我们一般都会设置有 replication（副本）的个数，生产者写入的时候首先根据分发策略（有 partition 按partition，有 key 按 key，都没有轮询）写入到 leader 中，follower（副本）再跟 leader 同步数据，这样有了备份，也可以保证消息数据的不丢失。

数据重复

###### ISR机制

Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。

如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由**replica.lag.time.max.ms**参数设定。Leader发生故障之后，就会从ISR中选举新的leader。

###### 故障处理细节（HW、LEO）

```properties
AR：  Assigned Replicas的缩写，是每个partition下所有副本（replicas）的统称；
ISR： In-Sync Replicas的缩写，是指副本同步队列，ISR是AR中的一个子集；
LEO：LogEndOffset的缩写，表示每个partition的log最后一条Message的位置。
HW： HighWatermark的缩写，是指consumer能够看到的此partition的位置。 取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。
```

![image-20210724092841819](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210724092841819.png)

- follower故障

  follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了

- leader故障

  leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。

  注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。

###### kafka Exactly Once 语义

- At Least Once

  将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，但是不能保证数据不重复。

- At Most Once

  将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次

- Exactly Once

  数据既不重复也不丢失。0.11版本的Kafka，引入了一项重大特性：

  幂等性。所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：

  At Least Once + 幂等性 = Exactly Once

  要启用幂等性，只需要设置Producer的参数 enable.idompotence=true 即可。

###### kafka auto.offset.reset值详解

- earliest

  当各分区下有已提交的offest时，从提交的offest开始消费，无提交的offest时，从头消费。

- latest

  当各分区下有已提交的offest时，从提交的offest开始消费；无提交的offest时，消费新产生的该分区下的数据。

- none

  当各分区下有已提交的offest时，从提交的offest开始消费；只要有一个分区不存在已提交的offest，则抛出异常。

#### 1.5 Hive

Hive是基于Hadoop的一个数据仓库工具，可以将一个结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。

###### 架构

###### Hivesql执行原理

###### 与Mysql区别

|        | mysql | Hive  |
| ------ | ----- | ----- |
| 数据量 | 小    | 大    |
| 速度   | 小 快 | 大 快 |



###### Hive底层如何存储null的

###### Hive支持的几种排序

###### Hive的动态分区

###### Hive内部表、外部表

删除  内部表：元素据、原始数据都删除

​			外部表：只删除元素据

企业中通常都是外部表

只有自己使用的临时表

###### 系统函数

date_add

date_sub

next_day

last_day

date_format

get_json_object

###### 条件函数

###### 排序函数

order by  ：全局排序

sort by：排序

distributed by:  分区

clusted by:  分区字段和排序字段相同时

###### 日期函数

###### 爆炸函数

###### 窗口函数

rank over

topn

行转列

列转行

###### 自定义函数

UDF

定义类   继承UDF   重写evaluate

1、编写自定义函数
2、打包上传到集群机器中
3、进入hive客户端，添加jar包：hive> add jar /root/hive_udf.jar
4、创建临时函数：hive> create temporary function getLen as 'com.raphael.len.GetLength';
5、销毁临时函数：hive> DROP TEMPORARY FUNCTION getLen;

UDTF

定义类   继承G...UDTF 重写3个方法   初始化

UDAF

解析json

系统函数就可以解决为啥还要自定义：方便定位问题，可以处理一些解析

mysql内元数据的备份

如果数据损坏，可能整个集群无法运行，至少要保证每日零点之后备份到其他服务器两个副本，keepalived或者mycat

###### Hive数据倾斜问题

1. 不同数据类型的表字段关联查询

2. 空值引起

3. 开启负载均衡

4. 设置reduce个数

   

```sql
-- 1 先从表中取出数据量过大的key,先不关联,给它的key做个加工,这里的key 是 channel 字段
select *,
md5( concat( channel,cast( round(rand()*10,0) as string) ) ) as new_channel -- 加工打散 随机1-10
from dws_dmx.adhoc_20210325_hwl_dr1456_mp_temp_mccafe_result 
where channel='MP_McCafe' and order_dt >= '20210301' and order_dt <'20210401'  ; -- A表

select md5( concat( channel,cast( round(rand()*10,0) as string) ) ) as new_channel  -- 加工打散 随机1-10
from dws_dmx.adhoc_20210325_hwl_dr1456_mp_exists_mccafe_user_order 
where channel='MP_McCafe' and order_dt >= '20210301' and order_dt <'20210401'; -- B表

-- 2 两个表关联 
select * from 
A inner join B on A.new_channel =  B.new_channel ;

--3 其它key关联的逻辑之后union all 上去
select * .... from t1 inne join t2 on  channel = channel -- 正常关联取数的逻辑
union all
select * from A inner join B on A.new_channel =  B.new_channel ; -- 单独处理key得逻辑
```



###### Hive优化

1. mapjoin默认打开  ,就不要关闭了

2. 提前进行行过滤，join where =》 where join

3. left semi join（左半连接）是 in/exsits 子查询的一种更高效的实现

   ```sql
   select A.key,A.value from A where A.key in (select B.key from B);
   
   #可以改写成：
   
   select A.key,A.value from A left semi join B on (A.key=B.key);
   
   "
   1、left semi join 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。
   
   2、left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。
   
   3、因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高。
   "
   ```

4. 分区（防止后续全表扫描）分桶 （数据量大    未知key  采样）

5. 小文件处理

   1. JVM重用
   2. combineHiveInputformat
   3. merge maponly 默认打开   mapreduce任务，需要手动打开

6. 采用压缩

   map（snappy） reduce

7. 列式存储

8. 合理设置map个数和reduce个数

   map 不能直接设置，可以通过设置split 间接控制。max(0,min(快大小，long最大值))

   redude可以设置

9. 提前进行combiner（不能影响业务逻辑）

10. 更改引擎   mr => spark  / tez

Fetch抓取



#### 1.6 HBase

<font color = red>**端口：(16010)**</font>

HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。

###### HBase的特点

1. 大：一个表可以有数十亿行，上百万列；
2. 无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态
   的增加，同一张表中不同的行可以有截然不同的列；
3. 面向列：面向列（族）的存储和权限控制，列（族）独立检索；
4. 稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；
5. 数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动
   分配，是单元格插入时的时间戳；
6. 数据类型单一：Hbase 中的数据都是字符串，没有类型。

###### 数据模型

- 逻辑结构

  ![image-20210722150208944](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722150208944.png)

- 物理存储结构

  ![image-20210722150239148](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722150239148.png)

1. Name Space

   命名空间，类似于关系型数据库的database概念，每个命名空间下有多个表。HBase两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间。

2. Table

   类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。

3. Row

   HBase表中的每行数据都由一个***\*RowKey\****和多个***\*Column\****（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。

4. Column

   HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。

5. Time Stamp

   用于标识数据的不同版本（version），每条数据写入时，系统会自动为其加上该字段，其值为写入HBase的时间。

6. Cell

   由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据全部是字节码形式存贮。



###### 架构原理

![image-20210722150407608](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722150407608.png)

架构角色：

- Region Server

  Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:

  - 对于数据的操作：get, put, delete
  - 对于Region的操作：splitRegion、compactRegion

- Master

  Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：

  - 对于表的操作：create, delete, alter
  - 对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。

- Zookeeper

  HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。

- HDFS

  HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。



###### RegionServer架构

![image-20210610102843088](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610102843088.png)

1. **StoreFile**

   保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。

2. **MemStore** 

   写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。

3. **WAL**

   由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。

4. **BlockCache**

   读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。

###### 写流程

![image-20210610105020064](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610105020064.png)

**流程**

- 1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。

- 2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。

- 3）与目标Region Server进行通讯；

- 4）将数据顺序写入（追加）到WAL；

- 5）将数据写入对应的MemStore，数据会在MemStore进行排序；

- 6）向客户端发送ack；

- 7）等达到MemStore的刷写时机后，将数据刷写到HFile

###### MemStore Flush

![image-20210610110258703](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610110258703.png)

**MemStore刷写时机：**

触发MemStore刷写的机制大概分为：人为手动触发、HBase定时触发、HLog数量限制触发，其他事件触发（Compact、Split、Truncate等）、内存限制触发。其中内存限制触发细分为：MemStore级别限制触发、Region级别限制触发、RegionServer级别限制触发。

1. **Region 中所有 MemStore 占用的内存超过相关阈值**

   当某个memstore的大小达到了**hbase.hregion.memstore.flush.size（默认值128M）**，其所在region的所有memstore都会刷写。

   但是如果我们的数据增加得很快，达到了 **hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier** 的大小，**hbase.hregion.memstore.block.multiplier 默认值为4**，也就是128*4=512MB的时候，那么除了触发 MemStore 刷写之外，HBase 还会在刷写的时候同时阻塞所有写入该 Store 的写请求！这时候如果你往对应的 Store 写数据，会出现 RegionTooBusyException 异常。

2. **整个 RegionServer 的 MemStore 占用内存总和大于相关阈值**

   当region server中memstore的总大小达到

   <font color=red>**java_heapsize  * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit(默认值0.95)**</font>,region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。

   如果此时数据写入吞吐量依然很大，导致该RegionServer种所有MemStore的大小加和超过该RegionServer全局水位阈值<font color=red>java_heapsize * hbase.regionserver.global.memstore.size </font>值大小，RegionServer会阻塞写请求，直到MemStore刷写大小将到低水位阈值。

3. **定期自动刷写**

   到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置<font color=red>**hbase.regionserver.optionalcacheflushinterval**（默认1小时）</font>。

4. **WAL数量大于相关阈值**

   WAL（Write-ahead log，预写日志）用来解决宕机之后的操作恢复问题的。数据到达 Region 的时候是先写入 WAL，然后再被写到 Memstore 的。如果 WAL 的数量越来越大，这就意味着 MemStore 中未持久化到磁盘的数据越来越多。当 RS 挂掉的时候，恢复时间将会变成，所以有必要在 WAL 到达一定的数量时进行一次刷写操作。

   当WAL文件的数量超过<font color=red>**hbase.regionserver.maxlogs**</font>，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到该参数值以下(<font color=red>该属性名已经废弃，现无需手动设置，最大值为32</font>.

5. **人为手动触发**

   通过 shell 命令`flush 'tablename'`或者`flush ‘regionname’`分别对整表所有region和具体一个Region进行flush。

6. **其他事件触发**

   在执行Region的合并、分裂、快照以及HFile的Compact等前会执行刷写

###### 读流程

**整体流程**

![image-20210610120054869](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610120054869.png)

**Merge细节**

![](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610120148944.png)

具体流程：

1. Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。
2. 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。
3. 与目标Region Server进行通讯.
4. 分别在MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。
5. 将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。
6. 将合并后的最终结果返回给客户端。

###### StoreFile Compaction

由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。

Compaction分为两种，分别是**Minor Compaction**和**Major Compaction**。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，并不清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且**会**清理掉所有过期和删除的数据。

![image-20210610151009698](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610151009698.png)



###### Region Split

默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。

**Region Split时机：**

当1个region中的某个Store下所有StoreFile的总大小超过

<font color=red>**Min(initialSize * R^3 ,hbase.hregion.max.filesize)**</font>，该Region就会进行拆分。其中initialSize的默认值为<font color=red>**2*hbase.hregion.memstore.flush.size**</font>，R为当前Region Server中属于该Table的Region个数（0.94版本之后）。

具体的切分策略为：

第一次split：1^3 * 256 = 256MB 

第二次split：2^3 * 256 = 2048MB 

第三次split：3^3 * 256 = 6912MB 

第四次split：4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB 

后面每次split的size都是10GB了。

**Hbase 2.0引入了新的split策略**：如果当前RegionServer上该表只有一个Region，按照2 * hbase.hregion.memstore.flush.size分裂，否则按照hbase.hregion.max.filesize分裂。

![image-20210610152309076](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610152309076.png)

###### HBase有没有并发问题，底层如何实现MVVC的

###### HBase预分区

- 预分区作用

  - 增加数据读写效率
  - 负载均衡，防止数据倾斜
  - 方便集群容灾调度
  - 优化map数量

- 如何如分区

  每一个redion维护着startRowkey和endRowKey，如果加入的数据符合某个region维护的rowkey范围，则该数据交给这个region维护

  - 手动设定预分区

    ```shell
    create 'staff','info','pattition1',SPLITS => ['1000','2000','3000','4000']
    ```

  - 使用16进制算法生成预分区

    ```shell
    create 'staff2','info',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}
    ```

###### HBase RowKey设计

在HBase中RowKey在数据检索和数据存储方面都有重要的作用，一个好的RowKey设计会影响到数据在HBase中的分布，还会影响我们查询效率，所以一个好的RowKey的设计方案是多么重要。

**RowKey设计原则**

- **长度原则**

  RowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，**一般设计成定长。建议越短越好，不要超过16个字节**，原因如下：

  - 数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。
  - MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。
  - 目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。

- **散列原则**

  设计的RowKey应均匀的分布在各个HBase节点上。

- **唯一原则**

  必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。

**总结**

在HBase的使用过程，设计RowKey是一个很重要的一个环节。我们在进行RowKey设计的时候可参照如下步骤：

1. 结合业务场景特点，选择合适的字段来做为RowKey，并且按照查询频次来放置字段顺序
2. 通过设计的RowKey能尽可能的将数据打散到整个集群中，均衡负载，避免热点问题
3. 设计的RowKey应尽量简短

###### HBase数据热点问题

HBase 中的行是以 rowkey 的字典序排序的，这种设计优化了scan 操作，可以将相关的 行 以及会被一起读取的行 存取在临近位置，便于 scan 。 然而，糟糕的 rowkey 设计是 热点 的源头。 热点发生在大量的客户端直接访问集群的一个或极少数节点。访问可以是读，写，或者其他操作。大量访问会使 热点region 所在的单个机器超出自身承受能力，引起性能下降甚至是 region 不可用。这也会影响同一个 regionserver 的其他 regions，由于主机无法服务其他region 的请求。设计良好的数据访问模式以使集群被充分，均衡的利用。

解决方案：

1. 加盐

   Salting（加盐）的原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之前的RowKey的开头不同。随机数能保障数据在所有Regions间的负载均衡。

2. 哈希

   哈希会使同一行永远用同一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完成的 rowkey，使用Get 操作获取正常的获取某一行数据。

3. 反转

   翻转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没意义的部分）放在前面。这样可以有效的随机 rowkey,但是牺牲了 rowkey 的有序性。

###### HBase列簇设计

原则：

在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享region 的，每个列簇数据相差太大导致查询效率低下。
最优：

将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查询效率最高，也能保持尽可能少的访问不同的磁盘文件。以用户信息为例，可以将必须的基本信息存放在一个列族，而一些附加的额外信息可以放在另一列族。

###### 场景题

- 每天百亿甚至千亿数据存入HBase,如何保证数据的存储正确，在规定时间里全部写入完毕、不存留数据

  需求分析：
   1）百亿数据：证明数据量非常大；
   2）存入 HBase：证明是跟 HBase 的写入数据有关；
   3）保证数据的正确：要设计正确的数据结构保证正确性；
   4）在规定时间内完成：对存入速度是有要求的。
  解决思路：
   1）数据量百亿条，什么概念呢？假设一整天 60x60x24 = 86400 秒都在写入数据，那么每秒的写入条数高达 100 万条，HBase 当然是支持不了每秒百万条数据的， 所以这百亿条数据可能不是通过实时地写入，而是批量地导入。
  
  批量导入推荐使用 BulkLoad 方式（推荐阅读：Spark 之读写 HBase），性能是普通写入方式几倍以上；
   2）存入 HBase：普通写入是用 JavaAPI put 来实现，批量导入推荐使用BulkLoad；
   3）保证数据的正确：这里需要考虑 RowKey 的设计、预建分区和列族设计等问题；
   4）在规定时间内完成也就是存入速度不能过慢，并且当然是越快越好，使用BulkLoad。
  
  
  
- HBase的regionserver发生故障之后的处理方法

  1）ZooKeeper 会监控 HRegionServer 的上下线情况，当 ZK 发现某个HRegionServer 宕机之后会通知 HMaster 进行失效备援；
  2）该 HRegionServer 会停止对外提供服务，就是它所负责的 region 暂时停止对外提供服务；
  3）HMaster 会将该 HRegionServer 所负责的 region 转移到其他HRegionServer 上，并且会对 HRegionServer 上存在 memstore 中还未持久化到磁盘中的数据进行恢复；
  4）这个恢复的工作是由 WAL 重播来完成，这个过程如下：
   ① wal 实际上就是一个文件，存在/hbase/WAL/对应 RegionServer 路径下。
  
   ② 宕机发生时，读取该 RegionServer 所对应的路径下的 wal 文件，然后根据不同的 region 切分成不同的临时文件 recover.edits。
   ③ 当 region 被分配到新的 RegionServer 中，RegionServer 读取 region时会进行是否存在 recover.edits，如果有则进行恢复。



#### 1.7 Sqoop

###### sqoop介绍

![image-20210722103356927](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722103356927.png)

Sqoop 是一款开源的工具，主要用于在 Hadoop(Hive)与传统的数据库(mysql、postgresql...)
间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres 等）中的
数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。

将导入或导出命令翻译成 mapreduce 程序来实现。
在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制。



###### 1、sqoop中文乱码问题

乱码问题由于两个平台数据编码不一致造成的。或者远程连接平台编码问题以及sqoop命令中编码问题

```shell
sqoop export --connect "jdbc:mysql://192.168.200.40:3306/otherdb?useUnicode=true&characterEncoding=utf-8" 
```

mysql建库建表时指定编码：

```mysql
 mysql> CREATE DATABASE `otherdb` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 

mysql> create table  use_02 (remark varchar(20),groupName varchar(225)) charset utf8 collate utf8_general_ci;
```



###### 2、sqoop导入到Hive表中文注释乱码问题

Hive在MySQL中的元数据出现乱码,用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：



1. 进入Hive数据库Metastore执行以下SQL

   ```sql
   # 修改表字段注解和表注解
   alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8；
   alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8；
   # 修改分区字段注解
   alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;
   alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;
   # 修改索引注解
   alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
   
   ```

2. 修改hive-site.xml配置文件

   ```xml
   <property>
       <name>javax.jdo.option.ConnectionURL</name>
       <value>jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;useUnicode=true&characterEncoding=UTF-8</value>
       <description>JDBC connect string for a JDBC metastore</description>
   </property>
   ```

###### 3、Sqoop导入导出Null存储一致性问题

Hive 中的 Null 在底层是以“\N”来存储，而 MySQL 中的 Null 在底层就是 Null，为了
保证数据两端的一致性。

在导出数据时采用

```properties
--input-null-string  '\\N' 和--input-null-non-string  '\\N' 
```

导入Hive数据时采用

```properties
--null-string '\\N' 和--null-non-string '\\N'
```

###### 4、Sqoop数据导出一致性问题

如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。
通过–staging-table选项指定登台表来解决此问题，该选项充当用于暂存导出数据的辅助表。

```sql
--staging-table app_cource_study_report_tmp
--clear-staging-table
```

###### 5、Sqoop 在导入数据的时候数据倾斜

sqoop 抽数的并行化主要涉及到两个参数：

- num-mappers：启动N个map来并行导入数据，默认4个；

- split-by：按照某一列来切分表的工作单元。

split-by根据不同的参数类型有不同的切分方法，如比较简单的int型，Sqoop会取最大和最小split-by字段值，然后根据传入的num-mappers来确定划分几个区域。

sqoop import 抽数查询愿数据的时候主要有两者方式：--table  方式：全量数据抽取  --query 方式：增加检索条件部分数据抽数，当然也可以where 1=1 进行全量操作，有很多人应该注意到 --query 语句后面必须要加一个 "and $CONDITIONS" 字符串，不加还报错。那$CONDITIONS 到底是干什么用的呢？其实它的作用是数据分割条件的占位符，也就是说最终数据查询语句中的$CONDITIONS 会被split-by>=501 and split-by<=1000 这样的分割条件替换。

###### 6、Sqoop 数据导出 Parquet（项目中遇到的问题）

Ads 层数据用 Sqoop 往 MySql 中导入数据的时候，如果用了 orc（Parquet）不能导入，需转化成 text 格式

- 创建临时表，把 Parquet 中表数据导入到临时表，把临时表导出到目标表用于可视化
- Sqoop 里面有参数，可以直接把 Parquet 转换为 text

总结：ads 层建表的时候就不要建 Parquet 表

#### 1.8 Canal

[参考文章](https://blog.csdn.net/yehongzhi1994/article/details/107880162)

[官网](https://github.com/alibaba/canal)

版本：1.1.4

canal，译意为水道/管道/沟渠，主要用途是基于 **MySQL 数据库增量日志解析**，提供**增量数据订阅和消费**。

我们可以简单地把canal理解为一个用来**同步增量数据的一个工具**。

![img](https://gitee.com/zhengqianhua0314/image-store/raw/master/aHR0cHM6Ly9zdGF0aWMubG92ZWJpbGliaWxpLmNvbS9waWMvY2FuYWxfc3l0LnBuZw)

工作原理就是把自己伪装成MySQL slave，模拟MySQL slave的交互协议向MySQL Mater发送 dump协议，MySQL mater收到canal发送过来的dump请求，开始推送binary log给canal，然后canal解析binary log，再发送到存储目的地，比如MySQL，Kafka，Elastic Search等等。

但是canal的数据同步**不是全量的，而是增量**。基于binary log增量订阅和消费，canal可以做：

- 数据库镜像
- 数据库实时备份
- 索引构建和实时维护
- 业务cache(缓存)刷新
- 带业务逻辑的增量数据处理

实际项目我们是**配置MQ模式，配合RocketMQ或者Kafka，canal会把数据发送到MQ的topic中，然后通过消息队列的消费者进行处理**。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMubG92ZWJpbGliaWxpLmNvbS9waWMvY2FuYWxfMTIucG5n?x-oss-process=image/format,png)











#### 1.9 Flume

Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。

![image-20210805085914371](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210805085914371.png)

##### 基础架构



![image-20210805090020008](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210805090020008.png)

- Agent

  Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。主要有3个部分组成，Source、Channel、Sink。

- Source

  Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。

- Sink

  Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。

  Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。

- Channel

  Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。

  Flume自带两种Channel：Memory Channel和File Channel。另外还有Kafkachannel。

  Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。

  File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。

- Event

  传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。Event由**Header**和**Body**两部分组成，Header用来存放该event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。

  ![img](https://gitee.com/zhengqianhua0314/image-store/raw/master/wps1.jpg) 









#### 1.10  Kylin

#### 1.11 Presto



#### 1.12 Azkaban

每天统计多少指标（100左右）

#### 1.13 ClickHouse(后边学习)



#### 1.14 DataX

##### 简介

DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle 等)、HDFS、Hive、ODPS、HBase、FTP 等各种异构数据源之间稳定高效的数据同步功能。

DataX 目前已经有了比较全面的插件体系，主流的 RDBMS 数据库、NOSQL、大数据计算系统都已经接入。

![image-20211123163328854](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211123163328854.png)

##### 框架设计

![datax_framework_new](https://gitee.com/zhengqianhua0314/image-store/raw/master/ec7e36f4-6927-11e6-8f5f-ffc43d6a468b.png)

DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。

- Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。
- Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。
- Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。

DataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。

![datax_arch](https://gitee.com/zhengqianhua0314/image-store/raw/master/aa6c95a8-6891-11e6-94b7-39f0ab5af3b4.png)

Job：单个作业的管理节点，负责数据清理、子任务划分、TaskGroup监控管理。
Task：由Job切分而来，是DataX作业的最小单元，每个Task负责一部分数据的同步工作。
Schedule：将Task组成TaskGroup，单个TaskGroup的并发数量为5。
TaskGroup：负责启动Task。

核心模块介绍：

1. DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。
2. DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。
3. 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。
4. 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—>Channel—>Writer的线程来完成任务同步工作。
5. DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0

与Sqoop的对比

![image-20211123165103687](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20211123165103687.png)

##### 快速上手

下载地址：http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz
源码地址：https://github.com/alibaba/DataX

- Linux
- JDK(1.8 以上，推荐 1.8)
- Python(推荐 Python2.6.X)

导入导出数据一致性

首先把数据写入到一个临时文件，若：

1. job成功，修改路径，文件名
2. 个别task失败、job失败，删除临时文件

###### DataX任务提交命令

DataX的使用十分简单，用户只需根据自己同步数据的数据源和目的地选择相应的Reader和Writer，并将Reader和Writer的信息配置在一个json文件中，然后执行如下命令提交数据同步任务即可。

```shell
[atguigu@hadoop102 datax]$ python bin/datax.py path/to/your/job.json
```

###### DataX配置文件格式

可以使用如下命名查看DataX配置文件模板。

```shell
[atguigu@hadoop102 datax]$ python bin/datax.py -r mysqlreader -w hdfswriter
```

配置文件模板如下，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。

![image-20230625102746500](C:%5CUsers%5CHua%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20230625102746500.png)

###### Mysql —> HDFS

**1）编写配置文件**

**（1）创建配置文件base_province_tm.json**

```shell
[atguigu@hadoop102 ~]$ vim /opt/module/datax/job/base_province_tm.json
```

**（2）配置文件内容如下**

```json
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "where": "id>=3",
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/edu2077"
                                ],
                                "table": [
                                    "base_province"
                                ]
                            }
                        ],
                        "password": "000000",
                        "splitPk": "",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}

```

**配置文件说明**

 ![image-20230625145718236](https://raw.githubusercontent.com/kexiyigeren/tuchuang/main/image-20230625145718236.png)

![image-20230625145829627](https://raw.githubusercontent.com/kexiyigeren/tuchuang/main/image-20230625145829627.png)                                                                                    

###### 使用优化

1.  job.setting.speed.channel : channel 并发数
2. job.setting.speed.record : 2 全局配置 channel 的 record 限速
3. job.setting.speed.byte：全局配置 channel 的 byte 限速
4. core.transport.channel.speed.record：单个 channel 的 record 限速
5. core.transport.channel.speed.byte：单个 channel 的 byte 限速

### 2 项目

数仓架构图

![image-20210719103018902](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719103018902.png)

- 数据采集层

  把数据从各种数据源中采集和存储到数据库或HDFS，期间有可能会做一些ETL(抽取 extra，转化 transfer，装载 load)操作。

  数据源种类可以有多种：

  日志，所占份额最大，存储在备份服务器上

  业务数据库：如mysql,oracle 

  HTTP/FTP接口的数据：合作伙伴提供的数据接口

  其他数据源：如Excel等需要手工录入的数据

- 数据存储与分析

  HDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。

  离线数据分析与计算，也就是对实时性要求不高的部分，Hive是很不错的选择

  presto操作Hive

- 数据共享

  前面使用Hive、Spark、SparkSQL、Presto分析和计算的结果，还是在HDFS上，但大多业务和应用不能直接从HDFS上获取数据，那么就需要一个数据共享的地方，使得各业务和产品能方便的获取数据。这里的数据共享，其实指的是前面数据分析计算后的结果存放的地方，也就是关系型数据库和NoSql数据库

- 数据应用

  报表：报表所使用的数据，一般也就是已经统计汇总好的，存放于数据共享层

  接口：接口的数据都是直接查询数据共享层即可得到的。

  即席查询：通常是现有的报表和数据共享层数据并不满足需求，需要从数据存储层直接查询。一般都是通过直接操作SQL得到。

数据仓库的要求

- 高效率

  数据仓库的分析数据一般分为日、周、月、季、年等，可以看出，以日为周期的数据要求效率最高，要求24小时甚至12小少内，客户能看到昨天的数据分析。由于有的企业每日的数据量很大，如果数据仓库设计的不好，需要延时1到2天才能显示数据，这显然是不能出现这总事情的。

- 高质量

  数据仓库所提供的各种信息，肯定要准确的数据。数据仓库通常要经过数据清洗，装载，查询，展现等多个流程得到。如果复杂的架构会有风多层次，那么由于数据源有脏数据或者代码不严谨，都可能导致数据不准确或错误，如果客户看到错误的信息就可能导致分析出错误的决策，造成经济的损失。

- 高拓展性

  之所以有的大型数据仓库系统架构设计复杂，是因为考虑到了未来3-5年的扩展性，如果在未来需要扩展一些新的功能了，就可以不用重建数据仓库系统，就能很稳定运行。

数据仓库分层的原因

- 用空间换时间，通过数据预处理提高效率，通过大量的预处理可以提升应用系统的用户体验（效率），但是数据仓库会存在大量冗余的数据。
- 增强可扩展性，方便以后业务的变更。如果不分层的话，当源业务系统的业务规则发生变化，整个数据仓库需要重建，会影响整个数据清洗过程，工作量巨大。
- 通过分层管理来实现分布完成工作，简化数据清洗过程，使每一层处理逻辑变得更简单。因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，每一层的处理逻辑都相对简单和容易理解，比较容易保证每一个步骤的正确性，但数据发生错误的时候，往往我们只需要局部调整某个步骤即可。

数据仓库的分层

标准的数据仓库分层分为：stg(数据缓冲层)，ODS(贴源层)，DW: DWD,DWS,DWT(数据仓库层)，ADS(数据集市层),APP(应用层)















#### 离线项目——数仓

##### 数仓概念

数据仓库（Data Warehouse）是为企业所有决策制定过程，提供所有系统数据支持的战略集合。通过数据仓库中数据的分析，可以帮助企业改进业务流程。控制成本、提高产品质量等。

是一个面向主题的、集成的、相对稳定的、反应历史变化的数据集合，用于支持管理、运营决策等。

![image-20210718101740349](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718101740349.png)

##### 数仓建模

- 确认业务流程
- 声明粒度
- 确认维度
- 确认事实

![微信图片_20210718102005](https://gitee.com/zhengqianhua0314/image-store/raw/master/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210718102005.png)

**事实表**

分为事务事实表、周期快照事实表、累积快照事实表

**事务事实表**

发生在某个时间点上的一个事件,记录发生在现实世界中的操作型事件。比如以订单为例：下单是一个事实、付款是一个事实、退款是一个事实，所有事实的累计就是事务事实表.

**周期快照事实表**

以具有规律性的、可预见的时间间隔来记录事实。它统计的是间隔周期内的度量统计，每个时间段一条记录，是在事务事实表之上建立的聚集表。

**累积快照事实表**

记录的不确定的周期的数据。代表的是完全覆盖一个事务或产品的生命周期的时间跨度，通常具有多个日期字段，用来记录整个生命周期中的关键时间点。

![img](https://gitee.com/zhengqianhua0314/image-store/raw/master/153316-20170717231029615-1212064980.png)

#### 1  数据源

```
数据源：业务系统的埋点日志，业务系统数据库、FTP文件服务器。
分别采取Flume,sqoop和datax的方式进行抽取。抽取的方式有全量、增量。
```

#### 2 ODS层

```

```

#### 3 DWD层

```

快照是一天一个的全量数据，一般适用于数据量比较小、对历史数据进行分析的场景。
拉链表：在增量数据量比较大的时候，但是数据量变化比较缓慢的场景可以使用拉链表。比如新增用户信息表。
```

- 

#### 4 Dim层



#### 项目逻辑梳理

- 拿到需求，指标如何建模

  ```
  根据指标确定计算指标所需的数据所在的业务流，自上而下地根据业务流程进行数据探查，找到计算指标所需数据的表，确定在哪一层建设模型。比如运营分析系统的项目在DWD层建了一个埋点日志行为拓展表的明细大宽表模型，可以支持运营分析系统所有的指标。以event事件表作为中心事实表，以用户维度表、产品信息、活动信息等作为关联维度。
  ```

#### 数据治理

数据治理是对数据资源全生命周事期的规划设计，过程控制和质量监督，通过规范化的数据治理，可实现救据资源的透明。可管，可控，厘清数据资产，完善数据标准落地、规范救据处理流程，提升数据质量，保障数据安全使用，促进数据流通与价值提炼。

- 元数据

- 数据血缘

  数据血缘是指在数据产生，加工融合、流转流通到最终消亡过程
  中形成的继承关系集台。通过对各类数据资源间和数据项间的继
  承关系进行描述和管理，反映数据资源在各个环节间的继承关系

  包括血缘关系管理，血缘关系分析，血缘关系查询

- 数据质量管理

  - 评估要求
    - 完整性
      - 记录完整性
      - 属性完整性
      - 关系完整性
    - 有效性
      - 格式有效性
      - 值域有效性
    - 正确性
      - 逻辑合理性
    - 唯一性
      - 主键唯一性
      - 数据唯一性
    - 及时性
      - 接入及时性
      - 更新及时性
    - 合理性
      - 业务合理性

  

1. 数据质量管理

   离线数据

   **线上准确性监控**：分为指标、表、APP层数据数据应用层级别的监控。

   指标准确性：

   ​	跨表对比：不同表相同指标之间等值判断

   ​	同一张表的逻辑性判断：相同表不同指标之间的逻辑判断（放贷人数<= 贷款申请人数）

   ​	自身判断：指标本身的规则判断，枚举值、唯一性、非空判断

   表维度：

   ​	行数级别的判断：全量表或分区表行数基于过去某个时间同比或环比的变化判断，也支持取值范围的判断

   ​	大小判断：全量表或分区表大小基于过去某个时间同比或环比的变化判断，也支持取值范围的判断

   数据应用层维度：

   ​	接口返回历史的数据不变判断：指过去某天、周、月，在指标定义不变的前提下。

   **线上及时性规则**

   ​	开始调度的时间：job任务从开始进入队列的时间，不是执行时间。

   ​	执行时长：作业开始执行到执行结束的时间，通常由作业的优先级、执行引擎、	SQL执行的效率。

   ​	deadline时间：从开始调度到最长的可执行时间

   ​	规则校验时间：针对表编写的检验规则（阈值、重试次数、邮件钉钉告警），数	据更新，达到阈值，重试次数超过多少次？触发，开始下一个调度执行。



### 3 优化

##### Spark

###### 内存溢出

Spark中的OOM问题主要有以下三种：

- map执行中内存溢出
- shuffle后内存溢出
- driver内存溢出

**Driver heap OOM**

场景一：用户在Driver端生成大对象，比如创建了一个大的集合数据结构

解决思路：

1. 考虑将该大对象转化成Executor端加载。例如调用sc.textFile、sc.hadoopFile等
2. 如果无法避免，相应增加driver-memory的值

场景二：从Executor端收集数据回Driver端

解决思路：

1. 本身不建议将大的数据从Executor端，collect回来，建议将Driver端对collect回来的数据所做的操作，转化成Executor端RDD操作
2. 若无法避免，相应增加driver-memory的值

**Map过程中OOM**

**Shuffle后OOM**

shuffle 后，单个文件过大导致的。在 Spark 中，join，reduceByKey 这一类型的过程，都会有 shuffle 的过程，在 shuffle 的使用，需要传入一个 partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是HashPatitioner，默认值是父 RDD 中最大的分区数，这个参数通过Spark.default.parallelism 控 制 ( 在 Spark-sql 中 用 Spark.sql.shuffle.partitions) ，Spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的Partitioner 或者自己实现的 Partitioner 就不能使用Spark.default.parallelism 这个参数来控制shuffle 的并发量了。如果是别的 partitioner 导致的 shuffle 内存溢出，就需要从partitioner 的代码增加 partitions 的数量。

##### Hive

###### Hive/spark小文件解决方案

程序运行的结果最终落地有很多的小文件，产生的原因：

- 读取的数据源就是大量的小文件
- 动态分区插入数据，会产生大量的小文件，从而导致map数量剧增
- Reduce/Task数量较多，最终落地的文件数量和Reduce/Task的个数是一样的

文件的数量决定了MapReduce/Spark中Mapper/Task数量，小文件越多，Mapper/Task的任务越多，每个Mapper/Task都会对应启动一个JVM/线程来运行，每个Mapper/Task执行数据很少、个数多，导致占用资源多，甚至这些任务的初始化可能比执行的时间还要多，严重影响性能

**解决方案**

1. distribute by

   少用动态分区，如果场景下必须使用时，那么记得在SQL语句最后添加上distribute by

   ```sql
   insert overwrite table temp.wlb_tmp_smallfile partition(dt)
   select * from process_table
   DISTRIBUTE BY rand();
   
   -- 设置参数
   -- 在 map only 的任务结束时合并小文件
   set hive.merge.mapfiles = true;
   -- 在 MapReduce 的任务结束时合并小文件
   set hive.merge.mapredfiles = true;
   -- 作业结束时合并文件的大小 
   set hive.merge.size.per.task = 256000000;
   -- 每个Map最大输入大小(这个值决定了合并后文件的数量) 
   set mapred.max.split.size=256000000;   
   -- 每个reducer的大小， 默认是1G，输入文件如果是10G，那么就会起10个reducer；
   set hive.exec.reducers.bytes.per.reducer=1073741824;
   
   set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
   ```

2. repartition/coalesce

3. 使用har归档文件

   对于已经产生小文件的hive表可以使用har归档

   ```sql
   set  hive.archive.enabled=  true ;
   set  hive.archive.har.parentdir.settable=  true ;
   set  har.partfile.size=256000000;
   
   ALTER   TABLE  ad_dev.wlb_tmp_smallfile_20210118 ARCHIVE PARTITION(pt='2020-12-01');
   ```

### 4 踩坑记录

#### 4.1 Hive

##### 1、[Hive分区表新增字段，查询为Null](https://www.cnblogs.com/wuning/p/11867733.html)

描述：在开发过程中，向hive分区表新增字段，发现查询新增字段的值为NULL。

```sql
-- 分区在增加字段前存在，会出现查询新增字段值为NULL的情况
-- 分区在增加字段前不存在，正常

-- 解决办法
-- 对于在增加字段前已经存在的分区，必须再执行
alter table student paritition(dt = '2019-11-14') add columns(sex string);
```

##### 2、hive 分区字段为空

在hive里面表可以创建成分区表，但是当分区字段的值是`''` 或者 `null`时，hive会自动将分区命名为默认分区名称。默认情况下，默认分区的名称为`_HIVE_DEFAULT_PARTITION_`，默认分区名称是可配置的

配置参数是

```sql
hive.exec.default.partition.name
```

### 5 面试问题

![image-20210719224635057](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719224635057.png)

![image-20210719232621163](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719232621163.png)

##### 1 Hive

1. 如下数据为蚂蚁森林中用户领取的减少碳排放量,找出连续 3 天及以上减少碳排放量在 100 以上的用户.

   ```sql
   id      dt     lowcarbon
   1001 2021-12-12 123
   1002 2021-12-12 45
   1001 2021-12-13 43
   1001 2021-12-13 45
   1001 2021-12-13 23
   1002 2021-12-14 45
   1001 2021-12-14 230
   1002 2021-12-15 45
   1001 2021-12-15 23
   … …
   ```

   解题思路：

   1. 按照用户ID及日期字段分组，计算每个用户单日减少的碳排放量

   2. 等差数列法：两个等差数列如果等差相同，则相同位置的数据相减得到的结果相同。

      按照用户分组，同时按照日期排序，计算每条数据的Rank值

   3. 将每行数据中的日期减去Rank值

   4. 按照用户及Flag分组，求每个组有多少条数据，并找出大于等于3条的用户

      ```sql
      select
          id,
          flag,
          count(*) ct
      from 
      (select
          id,
          dt,
          lowcarbon,
          date_sub(dt,rk) flag
      from 
      (select
          id,
          dt,
          lowcarbon,
          rank() over(partition by id order by dt) rk
      from 
      (select
          id,
          dt,
          sum(lowcarbon) lowcarbon
      from test1
      group by id,dt
      having lowcarbon>100)t1
      )t2
      )t3
      group by id,flag
      having ct>=3;
      ```

2. 分组问题

   如下为电商公司用户访问时间数据：

   ```sql
   id   ts(秒)
   1001 17523641234
   1001 17523641256
   1002 17523641278
   1001 17523641334
   1002 17523641434
   1001 17523641534
   1001 17523641544
   1002 17523641634
   1001 17523641638
   1001 17523641654
   ```

   某个用户连续的访问记录如果时间间隔小于 60 秒，则分为同一个组，结果为：

   ```sql
   id   ts(秒)      group
   1001 17523641234 1
   1001 17523641256 1
   1001 17523641334 2
   1001 17523641534 3
   1001 17523641544 3
   1001 17523641638 4
   1001 17523641654 4
   1002 17523641278 1
   1002 17523641434 2
   1002 17523641634 3
   ```

   解题思路：

   1. 将上一行时间数据下移
   2. 将当前行时间数据减去上一行时间数据
   3. 计算每个用户范围内从第一行到当前行tsdiff大于等于60的总个数(分组号)

   HQL:

   ```sql
   select
       id,
       ts,
       sum(if(tsdiff>=60,1,0)) over(partition by id order by ts) groupid
   from
       (select
       id,
       ts,
       ts-lagts tsdiff
   from
       (select
       id,
       ts,
       lag(ts,1,0) over(partition by id order by ts) lagts
   from
       test2)t1)t2;
   ```

3. 间隔连续问题

   某游戏公司记录的用户每日登录数据

   ```sql
   id   dt
   1001 2021-12-12
   1002 2021-12-12
   1001 2021-12-13
   1001 2021-12-14
   1001 2021-12-16
   1002 2021-12-16
   1001 2021-12-19
   1002 2021-12-17
   1001 2021-12-20
   ```

   计算每个用户最大的连续登录天数，可以间隔一天。解释：如果一个用户在 1,3,5,6 登录游戏，则视为连续 6 天登录。

   解题思路：

   1. 将上一行日期数据下移
   2. 将当前行日期减去上一行日期数据（datediff(t1,t2)）
   3. 按照用户分组,同时按照时间排序,计算从第一行到当前行大于2的数据的总条数(sum(if(flag>2,1,0)))
   4. 按照用户和flag分组,求最大时间减去最小时间并加上1
   5. 取连续登录天数的最大值

   HQL

   ```sql
   select
       id,
       max(days)+1
   from
       (select
       id,
       flag,
       datediff(max(dt),min(dt)) days
   from
       (select
       id,
       dt,
       sum(if(flag>2,1,0)) over(partition by id order by dt) flag
   from
       (select
       id,
       dt,
       datediff(dt,lagdt) flag
   from
       (select
       id,
       dt,
       lag(dt,1,'1970-01-01') over(partition by id order by dt) lagdt
   from
       test3)t1)t2)t3
   group by id,flag)t4
   group by id;
   ```

4. 打折日期交叉问题

   如下为平台商品促销数据：字段为品牌，打折开始日期，打折结束日期

   ```sql
   id     stt        edt
   oppo   2021-06-05 2021-06-09
   oppo   2021-06-11 2021-06-21
   vivo   2021-06-05 2021-06-15
   vivo   2021-06-09 2021-06-21
   redmi  2021-06-05 2021-06-21
   redmi  2021-06-09 2021-06-15
   redmi  2021-06-17 2021-06-26
   huawei 2021-06-05 2021-06-26
   huawei 2021-06-09 2021-06-15
   huawei 2021-06-17 2021-06-21
   ```

   计算每个品牌总的打折销售天数，注意其中的交叉日期，比如 vivo 品牌，第一次活动时
   间为 2021-06-05 到 2021-06-15，第二次活动时间为 2021-06-09 到 2021-06-21 其中 9 号到 15
   号为重复天数，只统计一次，即 vivo 总打折天数为 2021-06-05 到 2021-06-21 共计 17 天。

   解题思路

   1. 将edt列当前行以前的数据中最大的edt放置当前行
   2. 比较开始时间与移动下来的数据,如果开始时间大,则不需要操作,反之则需要将移动下来的数据加一替换当前行的开始时间，如果是第一行数据,maxEDT为null,则不需要操作
   3. 将每行数据中的结束日期减去开始日期
   4. 按照品牌分组,计算每条数据加一的总和

   ```sql
   select
       id,
       sum(if(days>=0,days+1,0)) days
   from
       (select
       id,
       datediff(edt,stt) days
   from
       (select
       id,
       if(maxEdt is null,stt,if(stt>maxEdt,stt,date_add(maxEdt,1))) stt,
       edt
   from 
       (select
       id,
       stt,
       edt,
       max(edt) over(partition by id order by stt rows between UNBOUNDED PRECEDING and 1 PRECEDING) maxEdt
   from test4)t1)t2)t3
   group by id;
   ```

5. 同时在线问题

   如下为某直播平台主播开播及关播时间，根据该数据计算出平台最高峰同时在线的主播人数

   ```sql
   id   stt                 edt
   1001 2021-06-14 12:12:12 2021-06-14 18:12:12
   1003 2021-06-14 13:12:12 2021-06-14 16:12:12
   1004 2021-06-14 13:15:12 2021-06-14 20:12:12
   1002 2021-06-14 15:12:12 2021-06-14 16:12:12
   1005 2021-06-14 15:18:12 2021-06-14 20:12:12
   1001 2021-06-14 20:12:12 2021-06-14 23:12:12
   1006 2021-06-14 21:12:12 2021-06-14 23:15:12
   1007 2021-06-14 22:12:12 2021-06-14 23:10:12
   
   ```

   解题思路

   1. 对数据分类,在开始数据后添加正1,表示有主播上线,同时在关播数据后添加-1,表示有主播下线
   2. 按照时间排序,计算累加人数
   3. 找出同时在线人数最大值

   HQL

   ```sql
   select
       max(sum_p)
   from
       (select
       id,
       dt,
       sum(p) over(order by dt) sum_p
   from
       (select id,stt dt,1 p from test5
   union
   select id,edt dt,-1 p from test5)t1)t2;
   ```


##### 2 Spark

1 在日常任务sparkstreaming任务出现堆积怎么办

出现原因分析：

1. 由于修改业务逻辑代码，模型任务停掉，kafka数据不断增加
2. 平时正常运行的模型，在业务高峰期，有高于正常情况几倍甚至几十倍的数据打入kafka，资源是固定的。

任务堆积的影响：

任务会有延迟，堆积，处理延迟变大。

优化：反压

修改参数：

spark.streaming.concurrentJobs=1

spark.streaming.backpressure.enabled=true 开启spark的反压机制，会减少一次从kafka拉取的数据量

spark.streaming.kafka.maxRatePerPartition=2000   设置每个分区每秒拉取的最大数据量，当模型重启的时候，限制拉取的数据量，防止一下拉取过多，OOM,卡死。

spark.streaming.backpressure.initialRate = 1000    设置任务启动后第一个任务处理的数据量，一般是跟maxRatePerPartition参数配合使用。



##### 3 Kafka

###### Kafka机器数量

kafka机器数量 = 2 * （峰值生产速度*副本数/100）+ 1

###### 副本数

一般设置成2个或3个，很多企业设置为2个。

副本优势：提高可靠性

副本劣势：增加了网络IO传输，增加了磁盘存储压力

###### Kafka压测

Kafka 官方自带压力测试脚本（kafka-consumer-perf-test.sh、kafka-producer-perf-test.sh）。Kafka 压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络 IO）。一般都是网络 IO达到瓶颈。

###### Kafka日志保存时间

生产环境建议3天,kafka默认保存7天

###### Kafka消息数据积压，Kafka消费能力不足怎么处理

- 如果是 Kafka 消费能力不足，则可以考虑增加 Topic 的分区数，并且同时提升消费
  组的消费者数量，消费者数=分区数。（两者缺一不可）
- 如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取
  数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

###### Kafka 参数优化

- Broker参数配置（server.properties）

  日志保留策略配置

  ```properties
  # 保留三天，也可以更短 （log.cleaner.delete.retention.ms）
  log.retention.hours=72
  ```

  副本相关配置

  ```properties
  default.replication.factor:2 默认副本 1 个
  ```

  网络通信延时

  ```properties
  replica.socket.timeout.ms:30000 #当集群之间网络不稳定时,调大该参数
  replica.lag.time.max.ms= 600000# 如果网络不好,或者 kafka 集群压力较大,会出现副本丢失,然后会频繁复制副本,导致集群压力更大,此时可以调大该参数
  ```
  
- Producer优化（producer.properties）

  ```properties
  compression.type:none
  #默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和
  Broker 的存储压力。
  ```

- Kafka内存调整（kafka-server-start.sh）

  默认内存 1 个 G，生产环境尽量不要超过 6 个 G。

  ```sh
  export KAFKA_HEAP_OPTS="-Xms4g -Xmx4g"
  ```

###### Kafka 单条日志传输大小

kafka 对于消息体的大小默认为单条最大值是 1M 但是在我们应用场景中, 常常会出现
一条消息大于 1M，如果不对 kafka 进行配置。则会出现生产者无法将消息推送到 kafka 或消费者无法去消费 kafka 里面的数据, 这时我们就要对 kafka 的server.properties进行以下配置：

```properties
replica.fetch.max.bytes: 1048576 broker 可复制的消息的最大字节数, 默认为
1M
message.max.bytes: 1000012 kafka 会接收单个消息 size 的最大限制， 默认为
1M 左右
```

注意：message.max.bytes 必须小于等于 replica.fetch.max.bytes，否则就会导致 replica 之间数据同步失败。



##### 9 其他应用场景题

###### 风控部署策略所需数据的埋点及获取

埋点意义就是在应用中特定的流程收集一些信息，应用使用的状况。

![image-20210718152337918](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718152337918.png)

步骤如下：

- 认识金融借贷产品环节
- 数据存表公共字段解析
- 用户基本信息的获取/价值
- 用户安装列表信息获取/价值
- 用户设备信息获取/价值
- 创建埋点的整体思路

2. 数据存表公共字段解析

   用户每次登录，每步操作都需要进行数据采集存储，数据的公共字段可以帮助我们尽快匹配特征数据与用户之间的联系以及特征与特征之间的联系。

   ![image-20210718152751417](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718152751417.png)

3. 用户基本信息的获取/价值

   ![image-20210718152957379](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718152957379.png)

   ![image-20210718153341494](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718153341494.png)

4. 用户安装列表信息获取/价值

   ![image-20210718153630640](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718153630640.png)

5. 用户设备信息获取/价值

   ![image-20210718154013957](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718154013957.png)

   ![image-20210718154038103](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718154038103.png)

6. 创建埋点的整体思路

   公共字段/详细字段

   满足要求的数据格式/数据再加工

   满足要求的获得率（90%）

   具体措施

   1. 明确埋点的目的，根据需求进行埋点

      埋点前要先想清楚需求是什么，目的是什么，要达到这个目的，需要统计哪些数据，要统计这些数据，需要在哪些页面埋点，埋在页面哪些位置，通过什么样的形式埋点，是统计按钮点击数，还是进入页面的数量。

   2. 与前端开发、数据产品经理沟通讨论

      梳理好要埋点的数据后，要多跟前端开发沟通，讨论埋点合理性与可行性，把埋点的目的跟开发描述清楚，一方面前端开发可以帮忙进行梳理，查漏补缺甚至提出更好的埋点思路；另一方面开发了解清楚后埋起点来更加胸有成竹，效率更快，防止出错。

   3. 开始进行埋点

      使用我司数据分析云平台，在APP/WX/H5里埋点后，还需要在云平台上传相应的事件ID与事件名称，一定要与代码中的ID与名称一致。ID与名称一般是产品这边整理命名，IOS与Android统一。

   4. 漏斗模型

      数据埋点完成后，如果要统计分析事件转化率，则需要提前添加漏斗模型，添加漏斗模型后第二天才会开始统计数据

###### 金融场景大数据云分析平台架构设计深度剖析

结合面试点来讲 ，数据仓库体系：

**如何判断一个模型的好坏**

数仓模型：

1 **模型的完整度**：

app、ads、dwt层直接引用ODS层的比例过高，跨层引用率过高

2 **复用率**

dwd dws层的产出的数量很少

3 **规范度**

数据可回滚，冲泡数据结果不变

###### 元素据管理是怎么做的

元素据的分类：技术元素据，业务元素据，管理元素据

过程：元数据范围探查，数据接入，定标准，维护，分析报告产出，冷热数据分析，关联分析，数据资产数据地图

###### 大宽表的优缺点

**什么是宽表：**

通常是指业务主题相关的指标、维度、属性关联在一起的一张数据库表

在数据仓库建设中，组织相关和相似数据，采用明细宽表，复用关联计算，减少数据扫描，提高明细数据表的易用性

在汇总数据层，加群指标的维度退化，采取更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工。

**优点**

- 提高查询性能
- 快速响应
- 方便使用，降低使用成本
- 提高用户满意度

**缺点**

- 由于把不同的内容都放在同一张表存储，宽表已经不符合三范式的模型设计规范，随之带来的主要坏处就是数据的大量冗余
-  另外就是灵活性差，就比如说线上业务表结构变更，宽表模式改造量也比较大
- 开发宽表为了避免宽表重复迭代，我们应该去了解业务全流程，得需要知道需扩展哪些维度，沉淀哪些指标，这样流程就会比较长，特别是有些业务快速迭代的话，就有点捉襟见肘

###### 互联网大厂实时数仓企业最佳实践

- 实时计算场景

  - 公司的核心数据大盘，大屏
    - 核心经营情况大盘
    - 实时核心日报表
    - 领导老板 运营App
  - 大型节假日 活动的核心数据
  - 运营数据体系
  - 实时特征 ：风险算法，推荐，广告

- 实时数仓架构以及容灾备份保障措施

  - 数据准确性  和离线对比 

  - 数据延迟  核心大屏数据<2min

  - 数据稳定性   数据模型抖动   宕机

  - 数据量大  千亿级  万亿级别 qps峰值

  - 数仓架构组件强依赖

    全链路  设计5-10个组件  ，计算业务链路复杂，100+实时作业

    20+外部数据源

  - 分层模型

  容灾备份：

  

- 场景问题以及解决方案

- 未来的实时数仓是什么样子，规划

###### 数据仓库的规范设计

1. 设计规范

   逻辑架构

   技术架构

   分层设计

   主题划分

   方法论

2. 命名规范

   各层级的规范

   任务命名

   表命名

   字段级别的命名

   指标命名

   标签命名

3. 模型规范

   建模方法

   建模工具

   血缘关系

   维度退化

   一致性维度

   元数据管理

4. 开发规范

   脚本注释

   字段别名

   编码规范

   脚本格式

   数据类型

   缩写规范

5. 流程规范

   需求流程

   工程流程

   上线流程

   调度流

   调度和生命周期的管理

二、设计规范-指标

1. 面向主题域管理

2. 划分原子指标和派生指标

3. 进行指标的命名规范

   原则：简单易懂+统一

   易懂：直接判断这个指标到底属于那个业务过程

   统一：确保派生指标和它继承的原子指标命名是一致的

4. 分级管理

   指标比较多，很难管理，所以需要按照原则或等级进行划分

   一级指标：核心指标   领导看的

   二级指标：原子指标和业务部分创建的派生指标

三、命名规范-表命名

1. 常规表

   是我们需要固化的表，正常使用的，没有下线的，长时间去维护的表。

   规范：分层前缀[dwd|dws|ads|app].业务域(CMIS/CMS/APP)_主题域(event)__ xxx __更新频率(inc增量/all全量)

   dwd.xxx_xxx_all    每日全量

   dwd.xxx_xxx_inc	每日增量

   dwd.xxx_xxx_m_all    每月全量

   dwd.xxx_xxx_m_inc	每月增量

2. 中间表/临时表

   temp/mid_table_name_20210707_dim/ods

3. 维度表

   基于底层数据，抽象出来具有描述性质的表。也可以手动维护

   dim.xxx

四、开发规范

1. 表和列的注释是否有缺失，复杂的指标计算逻辑是否有注释
2. 任务是否支持多次重跑而输出不变，不能有insert into 这个语句
3. ......

![image-20210719161423924](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719161423924.png)































































