# 			大数据相关整理

### 基础知识

#### 一、Linux

##### 常用操作



#### 二、 Java

#####  基础

###### 数据类型

- 基本数据类型
  - byte，short,int,long    整型
  - char    字符型
  - float,double    浮点型
  - boolean    布尔型
- 引用数据类型
  - String
  - Stringbulider    线程不安全，建议字符串相加操作比较多的情况下使用。
  - StringBuffer    线程安全的,多线程情况下使用

##### 集合

###### ArrayList、LinkedList、Vetor

- ArrayList 

   内部是通过数组实现的，它允许对元素进行快速随机访问.

  优点：查询快，修改快

  缺点：增删慢

  - 数组的缺点是每个元素之间不能有间隔，当数组大小不能满足时需要增加存储能力，会在原始大小上扩容1.5倍，将已有数据复制到新的存储空间中。
  - 当从ArrayList的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。

  适合随机查找和遍历，不适合插入和删除。

- Vetor

  与ArrayList一样，也是通过数组实现的，但是它支持线程的同步，即某一时刻只有一个线程能够写Vector,避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问效率比ArrayList慢。

- LinkedList

  它是用链表结构存储数据的，优缺点和数组正好相反。

  优点：增删快

  每次增加或删除，不会影响其他大量元素，只会影响链表中相关联的前后关系。

  缺点：查询慢，修改慢

  

  每次查询元素，都需要根据链接关系逐个进行匹配
  
  适合做数据的动态插入和删除，随机访问和遍历速度比较慢。

<font color=red>问题拓展</font>

数组和链表的区别

- 数组

  必须事先定义固定的长度，不能适应数据动态地增减的情况。从栈中分配空间, 对于程序方便快速,但是自由度小。

  优点：利用下标定位，随机访问性强，查找速度快。

  缺点：插入和删除的效率低，内存利用率低，内存空间要求高，必须有足够的连续的内存空间。

- 链表

  链表动态地进行存储分配，可以适应数据动态地增减的情况。从堆中分配空间, 自由度大但是申请管理比较麻烦。

  优点：插入和删除的效率高。

  缺点：定位查询速度慢，修改慢。内存利用率高，不会浪费内存。

- 总结

  如果需要快速访问数据，很少或不插入和删除元素，就应该用数组;相反， 如
  果需要经常插入和删除元素就需要用链表数据结构了。

- 结合项目中使用

  - 对于需要快速插入，删除元素，应该使用 LinkedList

  - 对于需要快速随机访问元素，应该使用 ArrayList

  - 对于“单线程环境操作 List” 或者 “多线程环境，但 List 仅仅只会被单个线
    程操作”的情况，此时应该使用非同步的类(如 ArrayList)

    对于“多线程环境，且 List 可能同时被多个线程操作”，此时，应该使用同步的类
    Vector，或 Collections.synchronizedList(new ArrayList())

##### 多线程

##### jvm

##### 类加载机制

##### 垃圾回收



##### 设计模式

###### 单例模式

顾名思义就是只有一个实例，并且它自己负责创建自己的对象，这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。

应用场景：

window桌面的回收站、网站计数器、多线程的线程池等。

- 需要频繁的进行创建和销毁的对象；
- 创建对象时耗时过多或耗费资源过多，但又经常用到的对象；
- 工具类对象；
- 频繁访问数据库或文件的对象。

单例的实现主要是通过以下两个步骤：

1. 将该类的构造方法cxz义为私有方法，这样其他处的代码就无法通过调用该类的构造方法来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例；
2. 在该类内提供一个静态方法，当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用。

实现方式：

1. 懒汉式  (线程不安全，不可用)

   ```java
   // 1、线程不安全（不可用）
   public class Singleton {
    
       // 指向自己实例的私有静态引用
       private static Singleton instance;
    
       // 私有的构造方法
       private Singleton(){}
    
       // 以自己实例为返回值的静态的公有方法，静态工厂方法
       public static Singleton getInstance(){
           // 被动创建，在真正需要使用时才去创建
           if (instance == null) {
               instance = new Singleton();
           }
           return instance;
       }
   }
   // 这种写法起到了Lazy Loading的效果，但是只能在单线程下使用。如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式。
   
   // 2、同步方法，线程安全（不推荐使用）
   public class Singleton {
   
       private static Singleton singleton;
   
       private Singleton() {}
   
       public static synchronized Singleton getInstance() {
           if (singleton == null) {
               singleton = new Singleton();
           }
           return singleton;
       }
   }
   
   //解决上面第一种实现方式的线程不安全问题，做个线程同步就可以了，于是就对getInstance()方法进行了线程同步。
   // 缺点：效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return就行了。方法进行同步效率太低要改进。
   
   // 3、同步代码块，线程安全（不可用）
   public class Singleton {
   
       private static Singleton singleton;
   
       private Singleton() {}
   
       public static Singleton getInstance() {
           if (singleton == null) {
               synchronized (Singleton.class) {
                   singleton = new Singleton();
               }
           }
           return singleton;
       }
   }
   // 由于第二种实现方式同步效率太低，所以摒弃同步方法，改为同步产生实例化的的代码块。但是这种同步并不能起到线程同步的作用。跟第一种实现方式遇到的情形一致，假如一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例
   
   ```

   

2. 饿汉式

   ```java
   // 1、静态常量 （可用）
   public class Singleton {
   
       private final static Singleton INSTANCE = new Singleton();
   
       private Singleton(){}
   
       public static Singleton getInstance(){
           return INSTANCE;
       }
   }
   
   //优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。
   
   //缺点：在类装载的时候就完成实例化，没有达到Lazy Loading的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费。
   
   // 2、静态代码块 （可用）
   public class Singleton {
   
       private static Singleton instance;
   
       static {
           instance = new Singleton();
       }
   
       private Singleton() {}
   
       public static Singleton getInstance() {
           return instance;
       }
   }
   
   "这种方式和上面的方式其实类似，只不过将类实例化的过程放在了静态代码块中，也是在类装载的时候，就执行静态代码块中的代码，初始化类的实例。优缺点和上面是一样的。"
   
   ```

   

3. 双检锁 （推荐用）

   ```java
   public class Singleton {
   
       private static volatile Singleton singleton;
   
       private Singleton() {}
   
       public static Singleton getInstance() {
           if (singleton == null) {
               synchronized (Singleton.class) {
                   if (singleton == null) {
                       singleton = new Singleton();
                   }
               }
           }
           return singleton;
       }
   }
   // Double-Check概念对于多线程开发者来说不会陌生，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。
   // 优点：线程安全；延迟加载；效率较高
   ```

   

4. 静态内部类  (推荐用)

   ```java
   public class Singleton {
   
       private Singleton() {}
   
       private static class SingletonInstance {
           private static final Singleton INSTANCE = new Singleton();
       }
   
       public static Singleton getInstance() {
           return SingletonInstance.INSTANCE;
       }
   }
   // 这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。
   //类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。
   //优点：避免了线程不安全，延迟加载，效率高。
   ```

   

5. 枚举 (推荐用)

   ```java
   public enum Singleton {
       INSTANCE;
       public void whateverMethod() {
   		//
       }
   }
   // 直接通过Singleton.INSTANCE.doSomething()的方式调用即可。方便、简洁又安全。
   ```

###### 工厂模式



##### 常用算法



#### 三、Mysql

#### 四、Redis

###### 数据类型

1. string  

   Redis最基本的数据类型，一个Redis中字符串value最多可以是512M 

   - 赋值：set key value
   - 取值：get key/getset key value
   - 删除：del key
   - 数值自增和 自减 ：incr key / decr key
   - 为key增加一个指定数值：incrby key increment
   - 为key减少一个指定数值：decrby key decrement
   - 拼凑字符串：append key value 

2. hash  

   是一个string类型的field和value的映射表，hash特别适合用于存储对象

3. list

   简单的字符串列表，按照插入顺序排序

4. set

   是string类型的无序集合，自动排重

5. zset

   是一个没有重复元素的字符串有序集合

###### Redis的持久化

- RDB

  在指定的时间间隔内将内存中的数据集快照写入磁盘， 也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里

  优势：

  - 适合大规模的数据恢复
  - 对数据完整性和一致性要求不高更适合使用
  - 节省磁盘空间
  - 恢复速度快

  劣势

  - Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑
  - 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。

  ![image-20210723110536032](C:\Users\Hua\AppData\Roaming\Typora\typora-user-images\image-20210723110536032.png)

- AOF

  以日志的形式来记录每个写操作（增量保存），将Redis执行过的所有写指令记录下来(读操作不记录)， 只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作

  默认不开启，可以在redis.conf中配置文件名称

  AOF和RDB同时开启，系统默认取AOF的数据

  **AOF同步频率设置**

  - appendfsync always

    始终同步，每次Redis的写入都会立刻记入日志；性能较差但数据完整性比较好

  - appendfsync everysec

    每秒同步，每秒记入日志一次，如果宕机，本秒的数据可能丢失

  - appendfsync no

    redis不主动进行同步，把同步时机交给操作系统

  优势：

  - 备份机制更稳健，丢失数据概率更低。
  - 可读的日志文本，通过操作AOF文件，可以处理误操作。

  劣势：

  - 比起RDB占用更多的磁盘空间。
  - 恢复备份速度要慢。
  - 每次读写都同步的话，有一定的性能压力。

  总结：

  ![image-20210723111256919](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210723111256919.png)



###### 缓存穿透

参考：https://www.imooc.com/article/283986?from=timeline

**缓存流程**

前台请求，后台先从缓存中取数据，取到直接返回结果，取不到时从数据库中取，数据库取到更新缓存，并返回结果，数据库也没取到，那直接返回空结果。

![img](https://gitee.com/zhengqianhua0314/image-store/raw/master/20180919143214712)



缓存穿透是指查询一个一定不存在的数据，由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查，给数据库造成了很大的压力，失去了缓存的意义。

**这里需要注意和缓存击穿的区别，缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞**

**如何解决缓存穿透：**

- 布隆过滤器

  将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。

- 缓存空对象

  （简单粗暴）如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。

  但是这种方法会存在两个问题：

  - 如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多的空值的键；

  - 即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。

###### 缓存击穿

key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。

**如何解决缓存击穿**

- 互斥锁的方式来实现
- edis、tair、zookeeper 等提供的分布式锁来实现

###### 缓存雪崩

当缓存服务器重启、挂掉或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力

**雪崩场景**

- Redis挂掉了，请求全部走数据库.
- 对缓存数据设置相同的过期时间，导致某段时间缓存失效，请求全部走数据库.

这种情况下，很可能就把我们的数据库搞垮，导致整个服务器瘫痪。

**如何解决缓存雪崩**

- Redis挂掉了，请求全部走数据库的情况

  - 事发前：实现Redis的高可用（主从架构+Sentinel(哨兵) 或者Redis Cluster (集群)），尽量避免Redis挂掉
  - 事发中：万一Redis真的挂了，我们可以设置本地缓存（ehcache）+ 限流（hystrix），尽量避免我们的数据库被干掉
  - 事发后：Redis持久化，重启后自动从磁盘上加载数据，快速回复缓存数据

- 缓存数据设置相同的过期时间，导致某段时间缓存失效，请求全部走数据库

  在缓存的时候给过期时间加上一个随机值，这样就会大幅减少缓存在同一时间过期。



### 1 大数据组件

#### 1.1 Hadoop

- 广义上说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。
- 狭义上说，Hadoop指Apache这款开源框架，是分布式系统基础架构。它的核心组件有：
  - HDFS(分布式文件系统)：解决海量数据存储
  - YARN（作业调度和集群资源管理）：解决资源任务调度
  - MapReduce（分布式运算编程框架）：解决海量数据计算

**Hadoop特性优点**

- 扩展能力：Hadoop是在可用的计算机集群间分配数据并完成计算任务的，这些集群可以方便的扩展到数以千计的节点中。
- 成本低：Hadoop通过普通廉价的机器组成服务器集群来分发以及处理数据，以至于成本很低。
- 高效率：通过并发数据，Hadoop可以在节点之间动态并行的移动数据，使得速度非常快。
- 可靠性：能自动维护数据的多份复制，并且在任务失败后能自动地重新部署计算任务。所以Hadoop的按位存储和处理数据的能力值得人们信赖。

Hadoop集群中hadoop都需要启动哪些进程，他们的作用分别是什么

- namenode

  HDFS的守护进程，负责维护整个文件系统，存储着整个文件系统的元素据信息如文件名，文件目录结构，文件属性以及每个文件的块列表和块所在的额datanode等，image+editlog

- datanode

  是文件系统的工作节点，存储文件块数据，以及块数据的校验和。

- secondarynamenode

  守护进程，相当于一个namenode的元数据的备份机制，定期的更新，和namenode进行通信，将namenode上的image和edits进行合并，可以作为namenode的备份使用

- resourcemanager

  是YARN的守护进程，

  - 负责所有资源的分配与调度
  - client的请求由它负责
  - 监控nodemanager
  - 启动或监控ApplicationMaster

- nodemanager

  - 单个节点的资源管理，
  - 执行来自resourcemanager的具体任务和命令
  - 处ApplicationMaster的命令

- DFSZKFailoverController

  高可用是他负责监控NN的状态，并及时的把状态信息写入ZK。它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为active NN的权利，因为最多只有2个节点，目前选择策略还比较简单（先到先得，轮换）。

- JournalNode

  高可用情况下存放namenode的editlog文件

##### 1、 HDFS  

<font color = red>**端口：(9870)**</font>

分布式文件管理系统，适合一次写入，多次读出的场景，且不支持文件的修改。

###### HDFS的架构

![image-20210721142755430](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721142755430.png)

- NameNode（nn）：也就是Master
  - 管理HDFS的名称空间
  - 配置副本策略
  - 管理数据块（Block）映射信息
  - 处理客户端读写请求
- DataNode（dn）：也就是slave
  - 存储实际的数据块
  - 执行数据块的读写操作
- Client
  - 文件切分，文件上传HDFS的时候，Client将文件切分成一个个的Block，然后进行上传
  - 与NameNode交互，获取文件的位置信息
  - 与DataNode交互，读取或写入数据
  - Client提供一些命令来管理HDFS，比如NameNode格式化
  - Client可以通过一些命令来访问HFDS，比如对HDFS增删改查操作
- SecondaryNameNode
  - 辅助NameNode，分担其工作量，比如定期合并Fsiimage和Edits，并推送给NameNode
  - 在紧急情况下，可辅助恢复NameNode

###### HDFS读写流程

**写流程**

![image-20210721151358219](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721151358219.png)

1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查该用户是否有上传权限，以及目标文件是否已存在，父目录是否存在，如果这两者有任意一个不满足，则直接报错，如果两者都满足，
   则返回给客户端一个可以上传的信息。
2. client 根据文件的大小进行切分，默认 128M 一块，切分完成之后给
   namenode 发送请求第一个 block 块上传到哪些服务器上
3. namenode 收到请求之后，根据网络拓扑和机架感知以及副本机制进行文
   件分配，返回可用的 DataNode 的地址
4. 客户端收到地址之后与服务器地址列表中的一个节点如 A 进行通信，本质
   上就是 RPC 调用，建立 pipeline，A 收到请求后会继续调用 B，B 在调用 C，
   将整个 pipeline 建立完成，逐级返回 client
5. client 开始向 A 上发送第一个 block（先从磁盘读取数据然后放到本地内存
   缓存），以 packet（数据包，64kb）为单位，A 收到一个 packet 就会发送给
   B，然后 B 发送给 C，A 每传完一个 packet 就会放入一个应答队列等待应答
6. 数据被分割成一个个的 packet 数据包在 pipeline 上依次传输，在
   pipeline 反向传输中，逐个发送 ack（命令正确应答），最终由 pipeline 中第
   一个 DataNode 节点 A 将 pipelineack 发送给 Client
7. 当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个
   block ，namenode 重新选择三台 DataNode 给 client

**读流程**

![image-20210721152415020](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721152415020.png)

1. 客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。



###### HDFS文件块大小

HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）设置，默认大小Hadoop2.x版本中是128M，老版本中是64M。

为什么块的大小不能设置太小，太大

- HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置
- 如果块设置太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间，导致程序在处理这块数据时，非常慢
- 总结：HDFS块的大小设置主要取决于磁盘传输速率

###### 熟悉HDFS的配置



###### NN和DN的作用

DataNode工作机制

![image-20210721154141383](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721154141383.png)

- 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。
- DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。
- 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。
- 集群运行中可以安全加入和退出一些机器。

DataNode掉线时限参数设置

![image-20210721154451072](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721154451072.png)

需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。

```xml
<property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>300000</value>
</property>
<property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
</property>
```



###### NameNode和SecondaryNameNode

NameNode工作机制

![image-20210721153014978](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721153014978.png)

1. NameNode启动
   - 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
   - 客户端对元数据进行增删改的请求。
   - NameNode记录操作日志，更新滚动日志。
   - NameNode在内存中对元数据进行增删改。
2. Secondary NameNode工作
   - SecondaryNameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。
   - Secondary NameNode请求执行CheckPoint。
   - NameNode滚动正在写的Edits日志。
   - 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。
   - Secondary NameNode加载编辑日志和镜像文件到内存，并合并。
   - 生成新的镜像文件fsimage.chkpoint。
   - 拷贝fsimage.chkpoint到NameNode。
   - NameNode将fsimage.chkpoint重新命名成fsimage。

**NN和2NN工作机制详解：**

```

Fsimage：NameNode内存中元数据序列化后形成的文件。
Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。
NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。
由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。
SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。
```

**CheckPoint时间设置**

- 通常情况下，SecondaryNameNode每隔一小时执行一次。

  ​	hdfs-default.xml

  ```xml
  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>3600s</value>
  </property>
  ```

- 一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次

  ```xml
  <property>
    <name>dfs.namenode.checkpoint.txns</name>
    <value>1000000</value>
  <description>操作动作次数</description>
  </property>
  
  <property>
    <name>dfs.namenode.checkpoint.check.period</name>
    <value>60s</value>
  <description> 1分钟检查一次操作次数</description>
  </property >
  ```

  

###### 机架感知 （副本存储节点选择）

![image-20210721152259789](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721152259789.png)

###### HDFS安全模式

安全模式是HDFS所处的一种特俗状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求，是一种保护机制。用于保证集群中的数据块的安全性。

在NameNode主节点启动时，HDFS首先进入安全模式，集群会开始检查数据块的完整性，DataNode在启动的时候会向NameNode汇报可用的Block信息，当整个系统达到安全标准时就，HDFS自动离开安全模式。

###### HDFS操作文件的常用命令

1. mkdir创建目录

   ```shell
   # hadoop fs -mkdir <paths>
   
   hadoop fs -mkdir /bigdata/dir1
   
   hadooop fs -mkdir -p /bigdata/dir2/dir3
   ```

2. put上传文件

   ```shell
   # hadoop fs -put <localsrc> ... <dst>
   
   hadoop fs -put localfile /bigdata/dir1
   ```

3. ls列出文件

   ```shell
   # hadoop fs -ls <args>
   
   hadoop fs -ls /bigdata/dir1
   ```

4. lsr    递归列出子目录中的文件及目录信息

   ```shell
   # hadoop fs -lsr <args>
   
   hadoop fs -lsr /bigdata
   ```

5. cat    将路径指定文件的内容输出到stdout

   ```shell
   # hadoop fs -cat URI [URI...]
   
   hadoop fs -cat /bigdata/*
   ```

6. get    复制文件到本地文件系统

   ```shell
   # hadoop fs -get [-ignorecrc] [-crc] <src> <localdst>
   
   hadoop fs -get /bigdata/test.txt localfile
   ```

7. rm    删除指定文件,只删除非空目录和文件

   rmr 递归删除

   ```shell
   # hadoop fs -rm URI [URI...]
   
   hadoop fs -rm /bigdata/test.txt
   
   # hadoop fs -rm URI [URI...]
   
   hadoop fs -rmr /bigdata
   ```

8. chgrp   改变文件所属的组,命令的使用者必须是文件的所有者或超级用户

   ```shell
   # hadoop fs -chgrp [-R] GROUP URI [URI...]
   
   hadoop fs -chgrp root /bigdata/test.txt
   ```

9. chmod    改变文件的权限，命令的使用者必须是文件的所有者或超级用户

   ```shell
   hadoop fs -chmod 744 /bigdata/test.txt
   ```

10. copyFromLocal    上传文件，除了限定资源路径是一个本地文件外，与put命令相似

    ```shell
    # hadoop fs -copyFromLocal <localsrc> URI
    
    hadoop fs -copyFromLocal word.txt /bigdata/dir1
    # 再上传一次就会报错，如果想要覆盖文件要加-f
    
    hadoop fs -copyFromLocal -f word.txt /bigdata/dir1
    ```

11. copyToLocal    复制文件到本地文件系统 ,与get命令相似

    ```shell
    # hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>
    
    hadoop fs -copyToLocal /bigdata/dir1/word.txt 
    ```

12. cp    复制文件

    ```shell
    # hadoop fs -cp URI[URI...] <dest>
    
    hadoop fs -cp /bigdata/test.txt /bigdata/dir1
    ```

13. du    显示目录中所有文件的大小

    ```shell
    # hadoop fs -du URI [URI...]
    
    hadoop fs -du /bigdata
    ```

14. expunge    清空回收站

    ```shell
    hadoop fs -expunge
    ```

15. mv    移动文件

    ```shell
    # hadoop fs -mv URI [URI...] <dest>
    
    hadoop fs -mv /bigdata/dir1/word.txt /bigdata
    ```

16. setrep    改变一个文件的副本系数

    ```shell
    # hadoop fs -setrep [-R] <path>
    # -R 用于递归改变目录下所有文件的副本系数
    
    hadoop fs -setrep -w 3 -R /bigdata
    ```

17. stat    返回指定路径的统计信息

    ```shell
    # hadoop fs -stat URI [URI...]
    
    hadoop fs -stat /bigdata
    ```

18. tail    将文件尾部的内容输出到stdout

    ```shell
    # hadoop fs -tail [-f] URI
    
    hadoop fs -tail /bigdata/test.txt
    ```

19. touchz    创建一个空文件

    ```shell
    # hadoop fs -touchz URI [URI...]
    
    hadoop fs -touchz /bigdata/new.txt
    ```

###### hadoop系统管理命令

1. 查看hadoop 版本

   ```shell
   hadoop version
   ```

   

2. 启动hadoop所有进程

   ```shell
   sbin/start-all.sh
   ```

   

3. 停止hadoop所有进程

   ```shell
   sbin/stop-all.sh
   ```

   

4. 格式化一个新的分布式文件系统

   ```shell
   bin/hadoop namenode -format
   ```

   

5. 启动HDFS

   ```shell
   bin/start-dfs.sh
   ```

   

6. 停止HDFS

   ```shell
   bin/stop-dfs.sh
   ```

   

7. 启动Yarn

   ```shell
   bin/start-yarn.sh
   ```

   

8. 停止Yarn

   ```shell
   bin/stop-yarn.sh
   ```

9. HDFS 安全模式

   - 进入安全模式

     ```shell
     bin/hadoop dfsadmin -safemode enter
     ```

     

   - 推出安全模式

     ```shell
     bin/hadoop dfsadmin -safemode leave
     ```

     

   - 查看集群是否处于安全模式

     ```shell
     bin/hadoop dfsadmin -safemode get
     ```

##### 2、YARN

<font color=red>**端口：（8088）**</font>

###### YARN架构图

YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。

![image-20210721135231860](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721135231860.png)

###### yarn工作机制

![image-20210722100512097](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722100512097.png)

1. MR程序提交到客户端所在的节点。
2. YarnRunner向ResourceManager申请一个Application。
3. RM将该应用程序的资源路径返回给YarnRunner。
4. 该程序将运行所需资源提交到HDFS上。
5. 程序资源提交完毕后，申请运行mrAppMaster。
6. RM将用户的请求初始化成一个Task。
7. 其中一个NodeManager领取到Task任务。
8. 该NodeManager创建容器Container，并产生MRAppmaster。
9. Container从HDFS上拷贝资源到本地。
10. MRAppmaster向RM 申请运行MapTask资源。
11. RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。
12. MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。
13. MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。
14. ReduceTask向MapTask获取相应分区的数据。
15. 程序运行完毕后，MR会向RM申请注销自己。

###### yarn资源调度策略

Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。

具体设置详见：yarn-default.xml文件

```xml
<property>
    <description>The class to use as the resource scheduler.</description>
    <name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>
```

- 先进先出调度器（FIFO）

  ![image-20210722101146158](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722101146158.png)

  即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。

- 容量调度器（Capacity Scheduler）

  ![image-20210722101432248](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722101432248.png)

  Capacity Scheduler 主要有以下几个特点：

  - 容量保证。管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。
  - 灵活性，如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。
  - 多重租赁。支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。
  - 安全保证。每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。
  - 动态更新配置文件。管理员可根据需要动态修改各种配置参数，以实现在线集群管理。

- 公平调度器（Fair Scheduler）

  ![image-20210722101741550](C:\Users\Hua\AppData\Roaming\Typora\typora-user-images\image-20210722101741550.png)

  公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源。当有作业提交上来，系统会将空闲的资源分配给新的作业，每个任务大致上会获取平等数量的资源。和传统的调度策略不同的是它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的任务挨饿！

###### yarn的配置

###### yarn任务调度的整体过程

##### 3、MapReduce

MapReduce是一个分布式运算程序的编程框架，将计算过程分为两个阶段：Map和Reduce

- Map阶段并行处理输入数据

- Reduce阶段对Map结果进行汇总

**优点**

- MapReduce易于编程
- 良好的扩展性
- 高容错性
- 适合PB级以上海量数据的离线处理

**缺点**

- 不擅长实时计算
- 不擅长流式计算
- 不擅长DAG(有向无环图)计算

###### MR的工作原理

![image-20210721161706253](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721161706253.png)

MapReduce执行流程

![image-20210722095400570](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722095400570.png)

![image-20210722095425768](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722095425768.png)

上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：

1. MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中
2. 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序
5. ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据
6. ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）
7. 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）

**注意：**

- Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。
- 缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。



###### MR手写代码实现简单的wc或topN

- wordcount

  1. 编写Mapper类

     ```java
     package com.atguigu.mapreduce;
     import java.io.IOException;
     import org.apache.hadoop.io.IntWritable;
     import org.apache.hadoop.io.LongWritable;
     import org.apache.hadoop.io.Text;
     import org.apache.hadoop.mapreduce.Mapper;
     
     public class WordcountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
     	
     	Text k = new Text();
     	IntWritable v = new IntWritable(1);
     	
     	@Override
     	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {
     		
     		// 1 获取一行
     		String line = value.toString();
     		
     		// 2 切割
     		String[] words = line.split(" ");
     		
     		// 3 输出
     		for (String word : words) {
     			
     			k.set(word);
     			context.write(k, v);
     		}
     	}
     }
     ```

     

  2. 编写Reducer类

     ```java
     package com.atguigu.mapreduce.wordcount;
     import java.io.IOException;
     import org.apache.hadoop.io.IntWritable;
     import org.apache.hadoop.io.Text;
     import org.apache.hadoop.mapreduce.Reducer;
     
     public class WordcountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
     
     int sum;
     IntWritable v = new IntWritable();
     
     	@Override
     	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
     		
     		// 1 累加求和
     		sum = 0;
     		for (IntWritable count : values) {
     			sum += count.get();
     		}
     		
     		// 2 输出
              v.set(sum);
     		context.write(key,v);
     	}
     }
     ```

     

  3. 编写Driver驱动类

     ```java
     package com.atguigu.mapreduce.wordcount;
     import java.io.IOException;
     import org.apache.hadoop.conf.Configuration;
     import org.apache.hadoop.fs.Path;
     import org.apache.hadoop.io.IntWritable;
     import org.apache.hadoop.io.Text;
     import org.apache.hadoop.mapreduce.Job;
     import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
     import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
     
     public class WordcountDriver {
     
     	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
     
     		// 1 获取配置信息以及获取job对象
     		Configuration configuration = new Configuration();
     		Job job = Job.getInstance(configuration);
     
     		// 2 关联本Driver程序的jar
     		job.setJarByClass(WordcountDriver.class);
     
     		// 3 关联Mapper和Reducer的jar
     		job.setMapperClass(WordcountMapper.class);
     		job.setReducerClass(WordcountReducer.class);
     
     		// 4 设置Mapper输出的kv类型
     		job.setMapOutputKeyClass(Text.class);
     		job.setMapOutputValueClass(IntWritable.class);
     
     		// 5 设置最终输出kv类型
     		job.setOutputKeyClass(Text.class);
     		job.setOutputValueClass(IntWritable.class);
     		
     		// 6 设置输入和输出路径
     		FileInputFormat.setInputPaths(job, new Path(args[0]));
     		FileOutputFormat.setOutputPath(job, new Path(args[1]));
     
     		// 7 提交job
     		boolean result = job.waitForCompletion(true);
     		System.exit(result ? 0 : 1);
     	}
     }
     ```

     

###### combinner

![image-20210721165534051](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721165534051.png)

自定义Combiner步骤

1. 自定义一个Combiner继承Reducer，重写Reduce方法

   ```java
   public class WordcountCombiner extends Reducer<Text, IntWritable, Text,IntWritable>{
   
   	@Override
   	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
   
           // 1 汇总操作
   		int count = 0;
   		for(IntWritable v :values){
   			count += v.get();
   		}
   
           // 2 写出
   		context.write(key, new IntWritable(count));
   	}
   }
   ```

2. 在Job驱动类中设置：  

   ```java
   job.setCombinerClass(WordcountCombiner.class);
   ```

   

###### partitioner

如果要求将统计结果按照条件输出到不同文件中（分区）。

默认Partitioner分区

![image-20210721164643608](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721164643608.png)

**自定义Partitioner步骤**

1. 自定义类继承Partitioner，重写个体Partition()方法

   ```java
   public class ProvincePartitioner extends Partitioner<Text, FlowBean> {
   
   	@Override
   	public int getPartition(Text key, FlowBean value, int numPartitions) {
   
   		// 1 获取电话号码的前三位
   		String preNum = key.toString().substring(0, 3);
   		
   		int partition = 4;
   		
   		// 2 判断是哪个省
   		if ("136".equals(preNum)) {
   			partition = 0;
   		}else if ("137".equals(preNum)) {
   			partition = 1;
   		}else if ("138".equals(preNum)) {
   			partition = 2;
   		}else if ("139".equals(preNum)) {
   			partition = 3;
   		}
   
   		return partition;
   	}
   }
   ```

2. 在job驱动类中设置自定义Partitioner

   ```java
   job.setPartitionerClass(ProvincePartitioner.class);
   ```

   

3. 自定义Partition后，要根据自定义Partition的逻辑设置相应数量的ReduceTask

   ```
   job.setNumReduceTasks(5);
   ```

![image-20210721165241640](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721165241640.png)

###### MR数据倾斜问题

###### shuffle的原理，减少shuffle的方法

Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。

![image-20210722100105428](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722100105428.png)

###### Mapreduce优化

执行Mapreduce常见的问题

1. client对集群中HDSF的操作没有权限

2. 提交集群运行，运行失败

   ```java
   job.setJar("");
   ```

   

##### 4、其他

###### hadoop集群搭建过程及常见的bug

- DataNode和NameNode进程同时只能工作一个

  ![image-20210721140326154](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210721140326154.png)

###### hadoop集群的扩容

###### 

##### 





#### 1.2 Spark





###### Shuffle



#### 1.3 Flink

#### 1.4 Kafka

###### kafka介绍

Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域，使用scala语言编写。

kafka相比其他消息队列的优势

- 可靠性：分布式，分区，复制和容错
- 可扩展性：kafka消息传递系统轻松缩放，无需停机
- 耐用性：kafka使用分布式提交日志，这意味着消息会尽可能快速的保存在磁盘上，因此它是持久的
- 性能：kafka对于发布和订阅消息都具有高吞吐量，即使存储了许多TB的消息，他也保持稳定的性能
- 速度快：保证零停机和零丢失

###### 为什么使用Kafka

- 缓冲和削峰，上游数据时有突发流量，下游可能扛不住，或者下游没有足够
  多的机器来保证冗余，kafka 在中间可以起到一个缓冲的作用，把消息暂存
  在 kafka 中，下游服务就可以按照自己的节奏进行慢慢处理。

- 解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为
  一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获
  取扩展能力。

- 冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅 topic
  的服务消费到，供多个毫无关联的业务使用。

- 健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会
  影响主要业务的正常进行。

- 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异
  步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列
  中放入多少消息就放多少，然后在需要的时候再去处理它们。

  

###### kafka架构

![image-20210723144904892](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210723144904892.png)

- **Producer**

  消息生产者，就是向kafka broker发消息的客户端

- **Consumer**

  消息消费者，向kafka broker取消息的客户端

- **Consumer Group**

  消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。

- **Broker**

  一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic

- **Topic**

  可以理解为一个队列，生产者和消费者面向的都是一个topic

- **Partition**

  为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列

- **Replica**

  副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower

- **Leader**

  每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader

- **Follower**

  每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader

###### kafka消费的有序性

单分区内有序，多分区，分区间无序。

如果想保证消费者有序，可以从业务上把需要有序的打到同⼀个 partition。

###### 分区与消费者组间的关系

每个分区只能由同一个消费组内的一个消费者(consumer)来消费，可以由不同的
消费组的消费者来消费，同组的消费者则起到并发的效果。

###### 生产者分区策略

分区的原因：

- 方便在集群中扩展
- 可以提高并发

分区的原则：

- 指明 partition 的情况下，直接将指明的值直接作为 partiton 值
- 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值
- 既没有 partition 值又没有 key 值的情况下，所有发往指定话题的records，会积攒成一个batch（达到一定大小或者两条消息间隔过长）一起发送到一个分区 。当形成新的batch，我们会随机选择一个新的分区发送

###### 消费者分区分配策略

参考：https://zhuanlan.zhihu.com/p/127349064

一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。

Kafka有三种分配策略，roundrobin，range，Sticky(0.11.0才被提出来的)。

- range（默认）

  对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。

- roundRobin

  策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。

- sticky

  Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的：

  1. 分区的分配要尽可能的均匀，分配给消费者者的主题分区数最多相差一个；
  2. 分区的分配尽可能的与上次分配的保持相同。

**触发分区重平衡的时机**

- 消费者的数量增加或减少
- 分区数量增加，目前只能增加，不能减少
- 订阅的主题发生变化

```properties
应尽量减少分区重平衡的时机，因为重平衡过程中，消费者无法从kafka消费消息，这对kafka的TPS影响极大，而如果kafka集内节点较多，比如数百个，那重平衡可能会耗时极多。数分钟到数小时都有可能，而这段时间kafka基本处于不可用状态
```

**如何减少分区重平衡的次数**

- session.timout.ms :  控制心跳超时时间
- heartbeat.interval.ms :控制心跳发送频率
- max.poll.interval.ms  :控制消费者poll的间隔

推荐设置

```properties
session.timout.ms：设置为6s
heartbeat.interval.ms：设置2s
max.poll.interval.ms：推荐为消费者处理消息最长耗时再加1分钟
```



###### 数据不丢失

可以从三方面保证：producer,consumer,broker

- **生产者**

  **ack 机制**：在 kafka 发送数据的时候，每次发送消息都会有一个确认反馈
  机制，确保消息正常的能够被收到，其中状态有 0，1，-1。

  - 同步模式

    ack 设置为 0，风险很大，一般不建议设置为 0。即使设置为 1，也会随着 leader宕机丢失数据。所以如果要严格保证生产端数据不丢失，可设置为-1。

  - 异步模式

    也会考虑 ack 的状态，除此之外，异步模式下的有个 buffer，通过 buffer 来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果 buffer满了数据还没有发送出去，有个选项是配置是否立即清空 buffer。可以设置为-1，永久阻塞，也就数据不再生产。异步模式下，即使设置为-1。也可能因为程序员的不科学操作，操作数据丢失，比如 kill -9，但这是特别的例外情况。

  ```properties
  注：
  ack=0：producer 不等待 broker 同步完成的确认，继续发送下一条(批)信息。
  ack=1（默认）：producer 要等待leader成功收到数据并得到确认，才发送下一条message。
  ack=-1：producer 得到 follwer 确认，才发送下一条数据。
  ```

- **消费者**

  通过 offset commit 来保证数据的不丢失，kafka 自己记录了每次消费的 offset 数
  值，下次继续消费的时候，会接着上次的 offset 进行消费。

  而 offset 的信息在 kafka0.8 版本之前保存在 zookeeper 中，在 0.8 版本之后保存
  到 topic 中，即使消费者在运行过程中挂掉了，再次启动的时候会找到 offset 的
  值，找到之前消费消息的位置，接着消费，由于 offset 的信息写入的时候并不
  是每条消息消费完成后都写入的，所以这种情况有可能会造成重复消费，但是不
  会丢失消息。

  唯一例外的情况是，我们在程序中给原本做不同功能的两个 consumer 组设置
  KafkaSpoutConfig.bulider.setGroupid 的时候设置成了一样的 groupid，这种情况会导致这两个组共享同一份数据，就会产生组 A 消费 partition1，partition2 中的消息，组 B 消费 partition3 的消息，这样每个组消费的消息都会丢失，都是不完整
  的。 为了保证每个组都独享一份消息数据，groupid 一定不要重复才行。

- **集群的Broker**

  每个 broker 中的 partition 我们一般都会设置有 replication（副本）的个数，生产
  者写入的时候首先根据分发策略（有 partition 按 partition，有 key 按 key，都没
  有轮询）写入到 leader 中，follower（副本）再跟 leader 同步数据，这样有了备
  份，也可以保证消息数据的不丢失。

数据重复

###### ISR机制

Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。

如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由**replica.lag.time.max.ms**参数设定。Leader发生故障之后，就会从ISR中选举新的leader。

###### 故障处理细节（HW、LEO）

```properties
AR：  Assigned Replicas的缩写，是每个partition下所有副本（replicas）的统称；
ISR： In-Sync Replicas的缩写，是指副本同步队列，ISR是AR中的一个子集；
LEO：LogEndOffset的缩写，表示每个partition的log最后一条Message的位置。
HW： HighWatermark的缩写，是指consumer能够看到的此partition的位置。 取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。
```

![image-20210724092841819](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210724092841819.png)

- follower故障

  follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了

- leader故障

  leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。

  注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。

###### kafka Exactly Once 语义

- At Least Once

  将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，但是不能保证数据不重复。

- At Most Once

  将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次

- Exactly Once

  数据既不重复也不丢失。0.11版本的Kafka，引入了一项重大特性：

  幂等性。所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：

  At Least Once + 幂等性 = Exactly Once

  要启用幂等性，只需要设置Producer的参数 enable.idompotence=true 即可。



###### kafka auto.offset.reset值详解

- earliest

  当各分区下有已提交的offest时，从提交的offest开始消费，无提交的offest时，从头消费。

- latest

  当各分区下有已提交的offest时，从提交的offest开始消费；无提交的offest时，消费新产生的该分区下的数据。

- none

  当各分区下有已提交的offest时，从提交的offest开始消费；只要有一个分区不存在已提交的offest，则抛出异常。

#### 1.5 Hive

Hive是基于Hadoop的一个数据仓库工具，可以将一个结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。

###### Hivesql执行原理

###### Hive底层如何存储null的

###### Hive支持的几种排序

###### Hive的动态分区

###### Hive内部表、外部表

###### 条件函数

###### 日期函数

###### 爆炸函数

###### 窗口函数

###### Hive数据倾斜问题

###### Hive优化

Fetch抓取



#### 1.6 HBase

<font color = red>**端口：(16010)**</font>

HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。

###### HBase的特点

1. 大：一个表可以有数十亿行，上百万列；
2. 无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态
   的增加，同一张表中不同的行可以有截然不同的列；
3. 面向列：面向列（族）的存储和权限控制，列（族）独立检索；
4. 稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；
5. 数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动
   分配，是单元格插入时的时间戳；
6. 数据类型单一：Hbase 中的数据都是字符串，没有类型。

###### 数据模型

- 逻辑结构

  ![image-20210722150208944](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722150208944.png)

- 物理存储结构

  ![image-20210722150239148](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722150239148.png)

1. Name Space

   命名空间，类似于关系型数据库的database概念，每个命名空间下有多个表。HBase两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间。

2. Table

   类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。

3. Row

   HBase表中的每行数据都由一个***\*RowKey\****和多个***\*Column\****（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。

4. Column

   HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。

5. Time Stamp

   用于标识数据的不同版本（version），每条数据写入时，系统会自动为其加上该字段，其值为写入HBase的时间。

6. Cell

   由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据全部是字节码形式存贮。



###### 架构原理

![image-20210722150407608](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722150407608.png)

架构角色：

- Region Server

  Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:

  - 对于数据的操作：get, put, delete
  - 对于Region的操作：splitRegion、compactRegion

- Master

  Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：

  - 对于表的操作：create, delete, alter
  - 对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。

- Zookeeper

  HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。

- HDFS

  HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。



###### RegionServer架构

![image-20210610102843088](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610102843088.png)

1. **StoreFile**

   保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。

2. **MemStore**

   写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。

3. **WAL**

   由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。

4. **BlockCache**

   读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。

###### 写流程

![image-20210610105020064](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610105020064.png)

**流程**

- 1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。

- 2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。

- 3）与目标Region Server进行通讯；

- 4）将数据顺序写入（追加）到WAL；

- 5）将数据写入对应的MemStore，数据会在MemStore进行排序；

- 6）向客户端发送ack；

- 7）等达到MemStore的刷写时机后，将数据刷写到HFile

###### MemStore Flush

![image-20210610110258703](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610110258703.png)

**MemStore刷写时机：**

触发MemStore刷写的机制大概分为：人为手动触发、HBase定时触发、HLog数量限制触发，其他事件触发（Compact、Split、Truncate等）、内存限制触发。其中内存限制触发细分为：MemStore级别限制触发、Region级别限制触发、RegionServer级别限制触发。

1. **Region 中所有 MemStore 占用的内存超过相关阈值**

   当某个memstore的大小达到了**hbase.hregion.memstore.flush.size（默认值128M）**，其所在region的所有memstore都会刷写。

   但是如果我们的数据增加得很快，达到了 **hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier** 的大小，**hbase.hregion.memstore.block.multiplier 默认值为4**，也就是128*4=512MB的时候，那么除了触发 MemStore 刷写之外，HBase 还会在刷写的时候同时阻塞所有写入该 Store 的写请求！这时候如果你往对应的 Store 写数据，会出现 RegionTooBusyException 异常。

2. **整个 RegionServer 的 MemStore 占用内存总和大于相关阈值**

   当region server中memstore的总大小达到

   <font color=red>**java_heapsize  * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit(默认值0.95)**</font>,region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。

   如果此时数据写入吞吐量依然很大，导致该RegionServer种所有MemStore的大小加和超过该RegionServer全局水位阈值<font color=red>java_heapsize * hbase.regionserver.global.memstore.size </font>值大小，RegionServer会阻塞写请求，直到MemStore刷写大小将到低水位阈值。

3. **定期自动刷写**

   到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置<font color=red>**hbase.regionserver.optionalcacheflushinterval**（默认1小时）</font>。

4. **WAL数量大于相关阈值**

   WAL（Write-ahead log，预写日志）用来解决宕机之后的操作恢复问题的。数据到达 Region 的时候是先写入 WAL，然后再被写到 Memstore 的。如果 WAL 的数量越来越大，这就意味着 MemStore 中未持久化到磁盘的数据越来越多。当 RS 挂掉的时候，恢复时间将会变成，所以有必要在 WAL 到达一定的数量时进行一次刷写操作。

   当WAL文件的数量超过<font color=red>**hbase.regionserver.maxlogs**</font>，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到该参数值以下(<font color=red>该属性名已经废弃，现无需手动设置，最大值为32</font>.

5. **人为手动触发**

   通过 shell 命令`flush 'tablename'`或者`flush ‘regionname’`分别对整表所有region和具体一个Region进行flush。

6. **其他事件触发**

   在执行Region的合并、分裂、快照以及HFile的Compact等前会执行刷写

###### 读流程

**整体流程**

![image-20210610120054869](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610120054869.png)

**Merge细节**

![](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610120148944.png)

具体流程：

1. Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。
2. 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。
3. 与目标Region Server进行通讯.
4. 分别在MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。
5. 将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。
6. 将合并后的最终结果返回给客户端。

###### StoreFile Compaction

由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。

Compaction分为两种，分别是**Minor Compaction**和**Major Compaction**。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，并不清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且**会**清理掉所有过期和删除的数据。

![image-20210610151009698](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610151009698.png)



###### Region Split

默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。

**Region Split时机：**

当1个region中的某个Store下所有StoreFile的总大小超过

<font color=red>**Min(initialSize * R^3 ,hbase.hregion.max.filesize)**</font>，该Region就会进行拆分。其中initialSize的默认值为<font color=red>**2*hbase.hregion.memstore.flush.size**</font>，R为当前Region Server中属于该Table的Region个数（0.94版本之后）。

具体的切分策略为：

第一次split：1^3 * 256 = 256MB 

第二次split：2^3 * 256 = 2048MB 

第三次split：3^3 * 256 = 6912MB 

第四次split：4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB 

后面每次split的size都是10GB了。

**Hbase 2.0引入了新的split策略**：如果当前RegionServer上该表只有一个Region，按照2 * hbase.hregion.memstore.flush.size分裂，否则按照hbase.hregion.max.filesize分裂。

![image-20210610152309076](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210610152309076.png)

###### HBase有没有并发问题，底层如何实现MVVC的

###### HBase预分区

- 预分区作用

  - 增加数据读写效率
  - 负载均衡，防止数据倾斜
  - 方便集群容灾调度
  - 优化map数量

- 如何如分区

  每一个redion维护着startRowkey和endRowKey，如果加入的数据符合某个region维护的rowkey范围，则该数据交给这个region维护

  - 手动设定预分区

    ```shell
    create 'staff','info','pattition1',SPLITS => ['1000','2000','3000','4000']
    ```

  - 使用16进制算法生成预分区

    ```shell
    create 'staff2','info',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}
    ```

###### HBase RowKey设计

在HBase中RowKey在数据检索和数据存储方面都有重要的作用，一个好的RowKey设计会影响到数据在HBase中的分布，还会影响我们查询效率，所以一个好的RowKey的设计方案是多么重要。

**RowKey设计原则**

- **长度原则**

  RowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，**一般设计成定长。建议越短越好，不要超过16个字节**，原因如下：

  - 数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。
  - MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。
  - 目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。

- **散列原则**

  设计的RowKey应均匀的分布在各个HBase节点上。

- **唯一原则**

  必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。

**总结**

在HBase的使用过程，设计RowKey是一个很重要的一个环节。我们在进行RowKey设计的时候可参照如下步骤：

1. 结合业务场景特点，选择合适的字段来做为RowKey，并且按照查询频次来放置字段顺序
2. 通过设计的RowKey能尽可能的将数据打散到整个集群中，均衡负载，避免热点问题
3. 设计的RowKey应尽量简短

###### HBase数据热点问题

HBase 中的行是以 rowkey 的字典序排序的，这种设计优化了scan 操作，可以将相关的 行 以及会被一起读取的行 存取在临近位置，便于 scan 。 然而，糟糕的 rowkey 设计是 热点 的源头。 热点发生在大量的客户端直接访问集群的一个或极少数节点。访问可以是读，写，或者其他操作。大量访问会使 热点region 所在的单个机器超出自身承受能力，引起性能下降甚至是 region 不可用。这也会影响同一个 regionserver 的其他 regions，由于主机无法服务其他region 的请求。设计良好的数据访问模式以使集群被充分，均衡的利用。

解决方案：

1. 加盐

   Salting（加盐）的原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之前的RowKey的开头不同。随机数能保障数据在所有Regions间的负载均衡。

2. 哈希

   哈希会使同一行永远用同一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完成的 rowkey，使用Get 操作获取正常的获取某一行数据。

3. 反转

   翻转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没意义的部分）放在前面。这样可以有效的随机 rowkey,但是牺牲了 rowkey 的有序性。

###### HBase列簇设计

原则：

在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享
region 的，每个列簇数据相差太大导致查询效率低下。
最优：

将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查询
效率最高，也能保持尽可能少的访问不同的磁盘文件。以用户信息为例，可以将
必须的基本信息存放在一个列族，而一些附加的额外信息可以放在另一列族。

###### 场景题

- 每天百亿甚至千亿数据存入HBase,如何保证数据的存储正确，在规定时间里全部写入完毕、不存留数据

  需求分析：
   1）百亿数据：证明数据量非常大；
   2）存入 HBase：证明是跟 HBase 的写入数据有关；
   3）保证数据的正确：要设计正确的数据结构保证正确性；
   4）在规定时间内完成：对存入速度是有要求的。
  解决思路：
   1）数据量百亿条，什么概念呢？假设一整天 60x60x24 = 86400 秒都在写入
  数据，那么每秒的写入条数高达 100 万条，HBase 当然是支持不了每秒百万条
  数据的， 所以这百亿条数据可能不是通过实时地写入，而是批量地导入。批量
  导入推荐使用 BulkLoad 方式（推荐阅读：Spark 之读写 HBase），性能是普
  通写入方式几倍以上；
   2）存入 HBase：普通写入是用 JavaAPI put 来实现，批量导入推荐使用
  BulkLoad；
   3）保证数据的正确：这里需要考虑 RowKey 的设计、预建分区和列族设计等
  问题；
   4）在规定时间内完成也就是存入速度不能过慢，并且当然是越快越好，使用
  BulkLoad。

  

- HBase的regionserver发生故障之后的处理方法

  1）ZooKeeper 会监控 HRegionServer 的上下线情况，当 ZK 发现某个
  HRegionServer 宕机之后会通知 HMaster 进行失效备援；
  2）该 HRegionServer 会停止对外提供服务，就是它所负责的 region 暂时停
  止对外提供服务；
  3）HMaster 会将该 HRegionServer 所负责的 region 转移到其他
  HRegionServer 上，并且会对 HRegionServer 上存在 memstore 中还未持久
  化到磁盘中的数据进行恢复；
  4）这个恢复的工作是由 WAL 重播来完成，这个过程如下：
   ① wal 实际上就是一个文件，存在/hbase/WAL/对应 RegionServer 路径下。

   ② 宕机发生时，读取该 RegionServer 所对应的路径下的 wal 文件，然后根
  据不同的 region 切分成不同的临时文件 recover.edits。
   ③ 当 region 被分配到新的 RegionServer 中，RegionServer 读取 region
  时会进行是否存在 recover.edits，如果有则进行恢复。



#### 1.7 Sqoop

###### sqoop介绍

![image-20210722103356927](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210722103356927.png)

Sqoop 是一款开源的工具，主要用于在 Hadoop(Hive)与传统的数据库(mysql、postgresql...)
间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres 等）中的
数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。

将导入或导出命令翻译成 mapreduce 程序来实现。
在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制。



###### 1、sqoop中文乱码问题

乱码问题由于两个平台数据编码不一致造成的。或者远程连接平台编码问题以及sqoop命令中编码问题

```shell
sqoop export --connect "jdbc:mysql://192.168.200.40:3306/otherdb?useUnicode=true&characterEncoding=utf-8" 
```

mysql建库建表时指定编码：

```mysql
 mysql> CREATE DATABASE `otherdb` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 

mysql> create table  use_02 (remark varchar(20),groupName varchar(225)) charset utf8 collate utf8_general_ci;
```



###### 2、sqoop导入到Hive表中文注释乱码问题

Hive在MySQL中的元数据出现乱码,用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：



1. 进入Hive数据库Metastore执行以下SQL

   ```sql
   # 修改表字段注解和表注解
   alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8；
   alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8；
   # 修改分区字段注解
   alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;
   alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;
   # 修改索引注解
   alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
   
   ```

2. 修改hive-site.xml配置文件

   ```xml
   <property>
       <name>javax.jdo.option.ConnectionURL</name>
       <value>jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;useUnicode=true&characterEncoding=UTF-8</value>
       <description>JDBC connect string for a JDBC metastore</description>
   </property>
   ```

###### 3、Sqoop导入导出Null存储一致性问题

Hive 中的 Null 在底层是以“\N”来存储，而 MySQL 中的 Null 在底层就是 Null，为了
保证数据两端的一致性。

在导出数据时采用

```properties
--input-null-string  '\\N' 和--input-null-non-string  '\\N' 
```

导入Hive数据时采用

```properties
--null-string '\\N' 和--null-non-string '\\N'
```

###### 4、Sqoop 在导入数据的时候数据倾斜

sqoop 抽数的并行化主要涉及到两个参数：

- num-mappers：启动N个map来并行导入数据，默认4个；

- split-by：按照某一列来切分表的工作单元。

split-by根据不同的参数类型有不同的切分方法，如比较简单的int型，Sqoop会取最大和最小split-by字段值，然后根据传入的num-mappers来确定划分几个区域。

sqoop import 抽数查询愿数据的时候主要有两者方式：--table  方式：全量数据抽取  --query 方式：增加检索条件部分数据抽数，当然也可以where 1=1 进行全量操作，有很多人应该注意到 --query 语句后面必须要加一个 "and $CONDITIONS" 字符串，不加还报错。那$CONDITIONS 到底是干什么用的呢？其实它的作用是数据分割条件的占位符，也就是说最终数据查询语句中的$CONDITIONS 会被split-by>=501 and split-by<=1000 这样的分割条件替换。

###### 5、Sqoop 数据导出 Parquet（项目中遇到的问题）

Ads 层数据用 Sqoop 往 MySql 中导入数据的时候，如果用了 orc（Parquet）不能导入，需转化成 text 格式

- 创建临时表，把 Parquet 中表数据导入到临时表，把临时表导出到目标表用于可视化
- Sqoop 里面有参数，可以直接把 Parquet 转换为 text

总结：ads 层建表的时候就不要建 Parquet 表

#### 1.8 Flume

Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。

![image-20210805085914371](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210805085914371.png)

##### 基础架构



![image-20210805090020008](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210805090020008.png)

- Agent

  Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。主要有3个部分组成，Source、Channel、Sink。

- Source

  Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。

- Sink

  Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。

  Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。

- Channel

  Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。

  Flume自带两种Channel：Memory Channel和File Channel。

  Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。

  File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。

- Event

  传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。Event由**Header**和**Body**两部分组成，Header用来存放该event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。

  ![img](https://gitee.com/zhengqianhua0314/image-store/raw/master/wps1.jpg) 









#### 1.9  Kylin

#### 1.10 Presto



#### 1.11 Azkaban

#### 1.12 ClickHouse(后边学习)

### 2 项目

数仓架构图

![image-20210719103018902](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719103018902.png)

- 数据采集层

  把数据从各种数据源中采集和存储到数据库或HDFS，期间有可能会做一些ETL(抽取 extra，转化 transfer，装载 load)操作。

  数据源种类可以有多种：

  日志，所占份额最大，存储在备份服务器上

  业务数据库：如mysql,oracle 

  HTTP/FTP接口的数据：合作伙伴提供的数据接口

  其他数据源：如Excel等需要手工录入的数据

- 数据存储与分析

  HDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。

  离线数据分析与计算，也就是对实时性要求不高的部分，Hive是很不错的选择

  presto操作Hive

- 数据共享

  前面使用Hive、Spark、SparkSQL、Presto分析和计算的结果，还是在HDFS上，但大多业务和应用不能直接从HDFS上获取数据，那么就需要一个数据共享的地方，使得各业务和产品能方便的获取数据。这里的数据共享，其实指的是前面数据分析计算后的结果存放的地方，也就是关系型数据库和NoSql数据库

- 数据应用

  报表：报表所使用的数据，一般也就是已经统计汇总好的，存放于数据共享层

  接口：接口的数据都是直接查询数据共享层即可得到的。

  即席查询：通常是现有的报表和数据共享层数据并不满足需求，需要从数据存储层直接查询。一般都是通过直接操作SQL得到。

数据仓库的要求

- 高效率

  数据仓库的分析数据一般分为日、周、月、季、年等，可以看出，以日为周期的数据要求效率最高，要求24小时甚至12小少内，客户能看到昨天的数据分析。由于有的企业每日的数据量很大，如果数据仓库设计的不好，需要延时1到2天才能显示数据，这显然是不能出现这总事情的。

- 高质量

  数据仓库所提供的各种信息，肯定要准确的数据。数据仓库通常要经过数据清洗，装载，查询，展现等多个流程得到。如果复杂的架构会有风多层次，那么由于数据源有脏数据或者代码不严谨，都可能导致数据不准确或错误，如果客户看到错误的信息就可能导致分析出错误的决策，造成经济的损失。

- 高拓展性

  之所以有的大型数据仓库系统架构设计复杂，是因为考虑到了未来3-5年的扩展性，如果在未来需要扩展一些新的功能了，就可以不用重建数据仓库系统，就能很稳定运行。

数据仓库分层的原因

- 用空间换时间，通过数据预处理提高效率，通过大量的预处理可以提升应用系统的用户体验（效率），但是数据仓库会存在大量冗余的数据。
- 增强可扩展性，方便以后业务的变更。如果不分层的话，当源业务系统的业务规则发生变化，整个数据仓库需要重建，会影响整个数据清洗过程，工作量巨大。
- 通过分层管理来实现分布完成工作，简化数据清洗过程，使每一层处理逻辑变得更简单。因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，每一层的处理逻辑都相对简单和容易理解，比较容易保证每一个步骤的正确性，但数据发生错误的时候，往往我们只需要局部调整某个步骤即可。

数据仓库的分层

标准的数据仓库分层分为：stg(数据缓冲层)，ODS(贴源层)，DW: DWD,DWS,DWT(数据仓库层)，ADS(数据集市层),APP(应用层)















#### 离线项目——数仓

##### 数仓概念

数据仓库（Data Warehouse）是为企业所有决策制定过程，提供所有系统数据支持的战略集合。通过数据仓库中数据的分析，可以帮助企业改进业务流程。控制成本、提高产品质量等。

是一个面向主题的、集成的、相对稳定的、反应历史变化的数据集合，用于支持管理、运营决策等。

![image-20210718101740349](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718101740349.png)

##### 数仓建模

- 确认业务流程
- 声明粒度
- 确认维度
- 确认事实

![微信图片_20210718102005](https://gitee.com/zhengqianhua0314/image-store/raw/master/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210718102005.png)

#### 1  数据源

```
数据源：业务系统的埋点日志，业务系统数据库、FTP文件服务器。
分别采取Flume,sqoop和datax的方式进行抽取。抽取的方式有全量、增量。
```

#### 2 ODS层

```

```

#### 3 DWD层

```

快照是一天一个的全量数据，一般适用于数据量比较小、对历史数据进行分析的场景。
拉链表：在增量数据量比较大的时候，但是数据量变化比较缓慢的场景可以使用拉链表。比如新增用户信息表。
```

- 

#### 4 Dim层



#### 项目逻辑梳理

- 拿到需求，指标如何建模

  ```
  根据指标确定计算指标所需的数据所在的业务流，自上而下地根据业务流程进行数据探查，找到计算指标所需数据的表，确定在哪一层建设模型。比如运营分析系统的项目在DWD层建了一个埋点日志行为拓展表的明细大宽表模型，可以支持运营分析系统所有的指标。以event事件表作为中心事实表，以用户维度表、产品信息、活动信息等作为关联维度。
  ```

#### 数据治理

1. 数据质量管理

   离线数据

   **线上准确性监控**：分为指标、表、APP层数据数据应用层级别的监控。

   指标准确性：

   ​	跨表对比：不同表相同指标之间等值判断

   ​	同一张表的逻辑性判断：相同表不同指标之间的逻辑判断（放贷人数<= 贷款申请人数）

   ​	自身判断：指标本身的规则判断，枚举值、唯一性、非空判断

   表维度：

   ​	行数级别的判断：全量表或分区表行数基于过去某个时间同比或环比的变化判断，也支持取值范围的判断

   ​	大小判断：全量表或分区表大小基于过去某个时间同比或环比的变化判断，也支持取值范围的判断

   数据应用层维度：

   ​	接口返回历史的数据不变判断：指过去某天、周、月，在指标定义不变的前提下。

   **线上及时性规则**

   ​	开始调度的时间：job任务从开始进入队列的时间，不是执行时间。

   ​	执行时长：作业开始执行到执行结束的时间，通常由作业的优先级、执行引擎、	SQL执行的效率。

   ​	deadline时间：从开始调度到最长的可执行时间

   ​	规则校验时间：针对表编写的检验规则（阈值、重试次数、邮件钉钉告警），数	据更新，达到阈值，重试次数超过多少次？触发，开始下一个调度执行。



### 3 优化

##### Spark

###### 内存溢出

Spark中的OOM问题主要有以下三种：

- map执行中内存溢出
- shuffle后内存溢出
- driver内存溢出

**Driver heap OOM**

场景一：用户在Driver端生成大对象，比如创建了一个大的集合数据结构

解决思路：

1. 考虑将该大对象转化成Executor端加载。例如调用sc.textFile、sc.hadoopFile等
2. 如果无法避免，相应增加driver-memory的值

场景二：从Executor端收集数据回Driver端

解决思路：

1. 本身不建议将大的数据从Executor端，collect回来，建议将Driver端对collect回来的数据所做的操作，转化成Executor端RDD操作
2. 若无法避免，相应增加driver-memory的值

**Map过程中OOM**

**Shuffle后OOM**

shuffle 后，单个文件过大导致的。在 Spark 中，join，reduceByKey 这一类型的过程，都会有 shuffle 的过程，在 shuffle 的使用，需要传入一个 partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是HashPatitioner，默认值是父 RDD 中最大的分区数，这个参数通过Spark.default.parallelism 控 制 ( 在 Spark-sql 中 用 Spark.sql.shuffle.partitions) ，Spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的Partitioner 或者自己实现的 Partitioner 就不能使用Spark.default.parallelism 这个参数来控制shuffle 的并发量了。如果是别的 partitioner 导致的 shuffle 内存溢出，就需要从partitioner 的代码增加 partitions 的数量。



### 4 踩坑记录

#### 4.1 Hive

##### 1、[Hive分区表新增字段，查询为Null](https://www.cnblogs.com/wuning/p/11867733.html)

描述：在开发过程中，向hive分区表新增字段，发现查询新增字段的值为NULL。

```sql
-- 分区在增加字段前存在，会出现查询新增字段值为NULL的情况
-- 分区在增加字段前不存在，正常

-- 解决办法
-- 对于在增加字段前已经存在的分区，必须再执行
alter table student paritition(dt = '2019-11-14') add columns(sex string);
```

##### 2、hive 分区字段为空

在hive里面表可以创建成分区表，但是当分区字段的值是`''` 或者 `null`时，hive会自动将分区命名为默认分区名称。默认情况下，默认分区的名称为`_HIVE_DEFAULT_PARTITION_`，默认分区名称是可配置的

配置参数是

```sql
hive.exec.default.partition.name
```

### 5 面试问题

![image-20210719224635057](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719224635057.png)

![image-20210719232621163](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719232621163.png)

##### 1 Hive

1. 如下数据为蚂蚁森林中用户领取的减少碳排放量,找出连续 3 天及以上减少碳排放量在 100 以上的用户.

   ```sql
   id      dt     lowcarbon
   1001 2021-12-12 123
   1002 2021-12-12 45
   1001 2021-12-13 43
   1001 2021-12-13 45
   1001 2021-12-13 23
   1002 2021-12-14 45
   1001 2021-12-14 230
   1002 2021-12-15 45
   1001 2021-12-15 23
   … …
   ```

   解题思路：

   1. 按照用户ID及日期字段分组，计算每个用户单日减少的碳排放量

   2. 等差数列法：两个等差数列如果等差相同，则相同位置的数据相减得到的结果相同。

      按照用户分组，同时按照日期排序，计算每条数据的Rank值

   3. 将每行数据中的日期减去Rank值

   4. 按照用户及Flag分组，求每个组有多少条数据，并找出大于等于3条的用户

      ```sql
      select
          id,
          flag,
          count(*) ct
      from 
      (select
          id,
          dt,
          lowcarbon,
          date_sub(dt,rk) flag
      from 
      (select
          id,
          dt,
          lowcarbon,
          rank() over(partition by id order by dt) rk
      from 
      (select
          id,
          dt,
          sum(lowcarbon) lowcarbon
      from test1
      group by id,dt
      having lowcarbon>100)t1
      )t2
      )t3
      group by id,flag
      having ct>=3;
      ```

2. 分组问题

   如下为电商公司用户访问时间数据：

   ```sql
   id   ts(秒)
   1001 17523641234
   1001 17523641256
   1002 17523641278
   1001 17523641334
   1002 17523641434
   1001 17523641534
   1001 17523641544
   1002 17523641634
   1001 17523641638
   1001 17523641654
   ```

   某个用户连续的访问记录如果时间间隔小于 60 秒，则分为同一个组，结果为：

   ```sql
   id   ts(秒)      group
   1001 17523641234 1
   1001 17523641256 1
   1001 17523641334 2
   1001 17523641534 3
   1001 17523641544 3
   1001 17523641638 4
   1001 17523641654 4
   1002 17523641278 1
   1002 17523641434 2
   1002 17523641634 3
   ```

   解题思路：

   1. 将上一行时间数据下移
   2. 将当前行时间数据减去上一行时间数据
   3. 计算每个用户范围内从第一行到当前行tsdiff大于等于60的总个数(分组号)

   HQL:

   ```sql
   select
       id,
       ts,
       sum(if(tsdiff>=60,1,0)) over(partition by id order by ts) groupid
   from
       (select
       id,
       ts,
       ts-lagts tsdiff
   from
       (select
       id,
       ts,
       lag(ts,1,0) over(partition by id order by ts) lagts
   from
       test2)t1)t2;
   ```

3. 间隔连续问题

   某游戏公司记录的用户每日登录数据

   ```sql
   id   dt
   1001 2021-12-12
   1002 2021-12-12
   1001 2021-12-13
   1001 2021-12-14
   1001 2021-12-16
   1002 2021-12-16
   1001 2021-12-19
   1002 2021-12-17
   1001 2021-12-20
   ```

   计算每个用户最大的连续登录天数，可以间隔一天。解释：如果一个用户在 1,3,5,6 登录游戏，则视为连续 6 天登录。

   解题思路：

   1. 将上一行日期数据下移
   2. 将当前行日期减去上一行日期数据（datediff(t1,t2)）
   3. 按照用户分组,同时按照时间排序,计算从第一行到当前行大于2的数据的总条数(sum(if(flag>2,1,0)))
   4. 按照用户和flag分组,求最大时间减去最小时间并加上1
   5. 取连续登录天数的最大值

   HQL

   ```sql
   select
       id,
       max(days)+1
   from
       (select
       id,
       flag,
       datediff(max(dt),min(dt)) days
   from
       (select
       id,
       dt,
       sum(if(flag>2,1,0)) over(partition by id order by dt) flag
   from
       (select
       id,
       dt,
       datediff(dt,lagdt) flag
   from
       (select
       id,
       dt,
       lag(dt,1,'1970-01-01') over(partition by id order by dt) lagdt
   from
       test3)t1)t2)t3
   group by id,flag)t4
   group by id;
   ```

4. 打折日期交叉问题

   如下为平台商品促销数据：字段为品牌，打折开始日期，打折结束日期

   ```sql
   id     stt        edt
   oppo   2021-06-05 2021-06-09
   oppo   2021-06-11 2021-06-21
   vivo   2021-06-05 2021-06-15
   vivo   2021-06-09 2021-06-21
   redmi  2021-06-05 2021-06-21
   redmi  2021-06-09 2021-06-15
   redmi  2021-06-17 2021-06-26
   huawei 2021-06-05 2021-06-26
   huawei 2021-06-09 2021-06-15
   huawei 2021-06-17 2021-06-21
   ```

   计算每个品牌总的打折销售天数，注意其中的交叉日期，比如 vivo 品牌，第一次活动时
   间为 2021-06-05 到 2021-06-15，第二次活动时间为 2021-06-09 到 2021-06-21 其中 9 号到 15
   号为重复天数，只统计一次，即 vivo 总打折天数为 2021-06-05 到 2021-06-21 共计 17 天。

   解题思路

   1. 将edt列当前行以前的数据中最大的edt放置当前行
   2. 比较开始时间与移动下来的数据,如果开始时间大,则不需要操作,反之则需要将移动下来的数据加一替换当前行的开始时间，如果是第一行数据,maxEDT为null,则不需要操作
   3. 将每行数据中的结束日期减去开始日期
   4. 按照品牌分组,计算每条数据加一的总和

   ```sql
   select
       id,
       sum(if(days>=0,days+1,0)) days
   from
       (select
       id,
       datediff(edt,stt) days
   from
       (select
       id,
       if(maxEdt is null,stt,if(stt>maxEdt,stt,date_add(maxEdt,1))) stt,
       edt
   from 
       (select
       id,
       stt,
       edt,
       max(edt) over(partition by id order by stt rows between UNBOUNDED PRECEDING and 1 PRECEDING) maxEdt
   from test4)t1)t2)t3
   group by id;
   ```

5. 同时在线问题

   如下为某直播平台主播开播及关播时间，根据该数据计算出平台最高峰同时在线的主播人数

   ```sql
   id   stt                 edt
   1001 2021-06-14 12:12:12 2021-06-14 18:12:12
   1003 2021-06-14 13:12:12 2021-06-14 16:12:12
   1004 2021-06-14 13:15:12 2021-06-14 20:12:12
   1002 2021-06-14 15:12:12 2021-06-14 16:12:12
   1005 2021-06-14 15:18:12 2021-06-14 20:12:12
   1001 2021-06-14 20:12:12 2021-06-14 23:12:12
   1006 2021-06-14 21:12:12 2021-06-14 23:15:12
   1007 2021-06-14 22:12:12 2021-06-14 23:10:12
   
   ```

   解题思路

   1. 对数据分类,在开始数据后添加正1,表示有主播上线,同时在关播数据后添加-1,表示有主播下线
   2. 按照时间排序,计算累加人数
   3. 找出同时在线人数最大值

   HQL

   ```sql
   select
       max(sum_p)
   from
       (select
       id,
       dt,
       sum(p) over(order by dt) sum_p
   from
       (select id,stt dt,1 p from test5
   union
   select id,edt dt,-1 p from test5)t1)t2;
   ```


##### 2 Spark

1 在日常任务sparkstreaming任务出现堆积怎么办

出现原因分析：

1. 由于修改业务逻辑代码，模型任务停掉，kafka数据不断增加
2. 平时正常运行的模型，在业务高峰期，有高于正常情况几倍甚至几十倍的数据打入kafka，资源是固定的。

任务堆积的影响：

任务会有延迟，堆积，处理延迟变大。

优化：反压

修改参数：

spark.streaming.concurrentJobs=1

spark.streaming.backpressure.enabled=true 开启spark的反压机制，会减少一次从kafka拉取的数据量

spark.streaming.kafka.maxRatePerPartition=2000   设置每个分区每秒拉取的最大数据量，当模型重启的时候，限制拉取的数据量，防止一下拉取过多，OOM,卡死。

spark.streaming.backpressure.initialRate = 1000    设置任务启动后第一个任务处理的数据量，一般是跟maxRatePerPartition参数配合使用。



##### 3 Kafka

###### Kafka机器数量

kafka机器数量 = 2 * （峰值生产速度*副本数/100）+ 1

###### 副本数

一般设置成2个或3个，很多企业设置为2个。

副本优势：提高可靠性

副本劣势：增加了网络IO传输，增加了磁盘存储压力

###### Kafka压测

Kafka 官方自带压力测试脚本（kafka-consumer-perf-test.sh、kafka-producer-perf-test.sh）。Kafka 压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络 IO）。一般都是网络 IO达到瓶颈。

###### Kafka日志保存时间

生产环境建议3天,kafka默认保存7天

###### Kafka消息数据积压，Kafka消费能力不足怎么处理

- 如果是 Kafka 消费能力不足，则可以考虑增加 Topic 的分区数，并且同时提升消费
  组的消费者数量，消费者数=分区数。（两者缺一不可）
- 如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取
  数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

###### Kafka 参数优化

- Broker参数配置（server.properties）

  日志保留策略配置

  ```properties
  # 保留三天，也可以更短 （log.cleaner.delete.retention.ms）
  log.retention.hours=72
  ```

  副本相关配置

  ```properties
  default.replication.factor:2 默认副本 1 个
  ```

  网络通信延时

  ```properties
  replica.socket.timeout.ms:30000 #当集群之间网络不稳定时,调大该参数
  replica.lag.time.max.ms= 600000# 如果网络不好,或者 kafka 集群压力较
  大,会出现副本丢失,然后会频繁复制副本,导致集群压力更大,此时可以调大该参数
  ```

- Producer优化（producer.properties）

  ```properties
  compression.type:none
  #默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和
  Broker 的存储压力。
  ```

- Kafka内存调整（kafka-server-start.sh）

  默认内存 1 个 G，生产环境尽量不要超过 6 个 G。

  ```sh
  export KAFKA_HEAP_OPTS="-Xms4g -Xmx4g"
  ```

###### Kafka 单条日志传输大小

kafka 对于消息体的大小默认为单条最大值是 1M 但是在我们应用场景中, 常常会出现
一条消息大于 1M，如果不对 kafka 进行配置。则会出现生产者无法将消息推送到 kafka 或消费者无法去消费 kafka 里面的数据, 这时我们就要对 kafka 的server.properties进行以下配置：

```properties
replica.fetch.max.bytes: 1048576 broker 可复制的消息的最大字节数, 默认为
1M
message.max.bytes: 1000012 kafka 会接收单个消息 size 的最大限制， 默认为
1M 左右
```

注意：message.max.bytes 必须小于等于 replica.fetch.max.bytes，否则就会导致 replica 之间数据同步失败。



##### 9 其他应用场景题

###### 风控部署策略所需数据的埋点及获取

埋点意义就是在应用中特定的流程收集一些信息，应用使用的状况。

![image-20210718152337918](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718152337918.png)

步骤如下：

- 认识金融借贷产品环节
- 数据存表公共字段解析
- 用户基本信息的获取/价值
- 用户安装列表信息获取/价值
- 用户设备信息获取/价值
- 创建埋点的整体思路

2. 数据存表公共字段解析

   用户每次登录，每步操作都需要进行数据采集存储，数据的公共字段可以帮助我们尽快匹配特征数据与用户之间的联系以及特征与特征之间的联系。

   ![image-20210718152751417](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718152751417.png)

3. 用户基本信息的获取/价值

   ![image-20210718152957379](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718152957379.png)

   ![image-20210718153341494](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718153341494.png)

4. 用户安装列表信息获取/价值

   ![image-20210718153630640](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718153630640.png)

5. 用户设备信息获取/价值

   ![image-20210718154013957](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718154013957.png)

   ![image-20210718154038103](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210718154038103.png)

6. 创建埋点的整体思路

   公共字段/详细字段

   满足要求的数据格式/数据再加工

   满足要求的获得率（90%）

   具体措施

   1. 明确埋点的目的，根据需求进行埋点

      埋点前要先想清楚需求是什么，目的是什么，要达到这个目的，需要统计哪些数据，要统计这些数据，需要在哪些页面埋点，埋在页面哪些位置，通过什么样的形式埋点，是统计按钮点击数，还是进入页面的数量。

   2. 与前端开发、数据产品经理沟通讨论

      梳理好要埋点的数据后，要多跟前端开发沟通，讨论埋点合理性与可行性，把埋点的目的跟开发描述清楚，一方面前端开发可以帮忙进行梳理，查漏补缺甚至提出更好的埋点思路；另一方面开发了解清楚后埋起点来更加胸有成竹，效率更快，防止出错。

   3. 开始进行埋点

      使用我司数据分析云平台，在APP/WX/H5里埋点后，还需要在云平台上传相应的事件ID与事件名称，一定要与代码中的ID与名称一致。ID与名称一般是产品这边整理命名，IOS与Android统一。

   4. 漏斗模型

      数据埋点完成后，如果要统计分析事件转化率，则需要提前添加漏斗模型，添加漏斗模型后第二天才会开始统计数据

###### 金融场景大数据云分析平台架构设计深度剖析

结合面试点来讲 ，数据仓库体系：

**如何判断一个模型的好坏**

数仓模型：

1 **模型的完整度**：

app、ads、dwt层直接引用ODS层的比例过高，跨层引用率过高

2 **复用率**

dwd dws层的产出的数量很少

3 **规范度**

数据可回滚，冲泡数据结果不变

###### 元素据管理是怎么做的

元素据的分类：技术元素据，业务元素据，管理元素据

过程：元数据范围探查，数据接入，定标准，维护，分析报告产出，冷热数据分析，关联分析，数据资产数据地图

###### 大宽表的优缺点

**什么是宽表：**

通常是指业务主题相关的指标、维度、属性关联在一起的一张数据库表

在数据仓库建设中，组织相关和相似数据，采用明细宽表，复用关联计算，减少数据扫描，提高明细数据表的易用性

在汇总数据层，加群指标的维度退化，采取更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工。

**优点**

- 提高查询性能
- 快速响应
- 方便使用，降低使用成本
- 提高用户满意度

**缺点**

- 由于把不同的内容都放在同一张表存储，宽表已经不符合三范式的模型设计规范，随之带来的主要坏处就是数据的大量冗余
-  另外就是灵活性差，就比如说线上业务表结构变更，宽表模式改造量也比较大
- 开发宽表为了避免宽表重复迭代，我们应该去了解业务全流程，得需要知道需扩展哪些维度，沉淀哪些指标，这样流程就会比较长，特别是有些业务快速迭代的话，就有点捉襟见肘

###### 互联网大厂实时数仓企业最佳实践

- 实时计算场景

  - 公司的核心数据大盘，大屏
    - 核心经营情况大盘
    - 实时核心日报表
    - 领导老板 运营App
  - 大型节假日 活动的核心数据
  - 运营数据体系
  - 实时特征 ：风险算法，推荐，广告

- 实时数仓架构以及容灾备份保障措施

  - 数据准确性  和离线对比 

  - 数据延迟  核心大屏数据<2min

  - 数据稳定性   数据模型抖动   宕机

  - 数据量大  千亿级  万亿级别 qps峰值

  - 数仓架构组件强依赖

    全链路  设计5-10个组件  ，计算业务链路复杂，100+实时作业

    20+外部数据源

  - 分层模型

  容灾备份：

  

- 场景问题以及解决方案

- 未来的实时数仓是什么样子，规划

###### 数据仓库的规范设计

1. 设计规范

   逻辑架构

   技术架构

   分层设计

   主题划分

   方法论

2. 命名规范

   各层级的规范

   任务命名

   表命名

   字段级别的命名

   指标命名

   标签命名

3. 模型规范

   建模方法

   建模工具

   血缘关系

   维度退化

   一致性维度

   元数据管理

4. 开发规范

   脚本注释

   字段别名

   编码规范

   脚本格式

   数据类型

   缩写规范

5. 流程规范

   需求流程

   工程流程

   上线流程

   调度流

   调度和生命周期的管理

二、设计规范-指标

1. 面向主题域管理

2. 划分原子指标和派生指标

3. 进行指标的命名规范

   原则：简单易懂+统一

   易懂：直接判断这个指标到底属于那个业务过程

   统一：确保派生指标和它继承的原子指标命名是一致的

4. 分级管理

   指标比较多，很难管理，所以需要按照原则或等级进行划分

   一级指标：核心指标   领导看的

   二级指标：原子指标和业务部分创建的派生指标

三、命名规范-表命名

1. 常规表

   是我们需要固化的表，正常使用的，没有下线的，长时间去维护的表。

   规范：分层前缀[dwd|dws|ads|app].业务域(CMIS/CMS/APP)_主题域(event)__ xxx __更新频率(inc增量/all全量)

   dwd.xxx_xxx_all    每日全量

   dwd.xxx_xxx_inc	每日增量

   dwd.xxx_xxx_m_all    每月全量

   dwd.xxx_xxx_m_inc	每月增量

2. 中间表/临时表

   temp/mid_table_name_20210707_dim/ods

3. 维度表

   基于底层数据，抽象出来具有描述性质的表。也可以手动维护

   dim.xxx

四、开发规范

1. 表和列的注释是否有缺失，复杂的指标计算逻辑是否有注释
2. 任务是否支持多次重跑而输出不变，不能有insert into 这个语句
3. ......

![image-20210719161423924](https://gitee.com/zhengqianhua0314/image-store/raw/master/image-20210719161423924.png)































































